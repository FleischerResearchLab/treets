{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp food_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56492e",
   "metadata": {},
   "source": [
    "# Food Parser\n",
    "A class that reads in food loggings from the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2589f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# export \n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FoodParser():\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.__version__ = '0.1.9'\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        # self.spell = SpellChecker()\n",
    "        return\n",
    "\n",
    "    ################# Read in Annotations #################\n",
    "    def process_parser_keys_df(self, parser_keys_df):\n",
    "        parser_keys_df = parser_keys_df.query('food_type in [\"f\", \"b\", \"m\", \"w\", \"modifier\", \"general\", \"stopword\", \"selfcare\"]').reset_index(drop = True)\n",
    "\n",
    "        # 1. all_gram_sets\n",
    "        all_gram_set = []\n",
    "        for i in range(1, 6):\n",
    "            all_gram_set.append(set(parser_keys_df.query('gram_type == ' + str(i)).gram_key.values))\n",
    "\n",
    "        # 2. food_type_dict\n",
    "        food_type_dict = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.food_type.values))\n",
    "\n",
    "        # 3. food2tag\n",
    "        def find_all_tags(s):\n",
    "            tags = []\n",
    "            for i in range(1, 8):\n",
    "                if not isinstance(s['tag' + str(i)], str) and np.isnan(s['tag' + str(i)]):\n",
    "                    continue\n",
    "                tags.append(s['tag' + str(i)])\n",
    "            return tags\n",
    "        food2tags = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.apply(find_all_tags, axis = 1)))\n",
    "        return all_gram_set, food_type_dict, food2tags\n",
    "\n",
    "    # def run_setup(self):\n",
    "    #     setup_dict = {}\n",
    "    #\n",
    "    #     # Read parser_keys\n",
    "    #     parser_keys_df = self.read_parser_keys()\n",
    "    #     all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n",
    "    #     # my_stop_words = combined_gram_set_df.query('food_type == \"stopwords\"').gram_key.values\n",
    "    #\n",
    "    #     # Read the correction history for fire-fighters\n",
    "    #     # correction_dic = pickle.load(open(pkg_resources.resource_stream(__name__, \"data/correction_dic.pickle\"), 'rb'))\n",
    "    #     correction_dic = pickle.load(pkg_resources.resource_stream(__name__, \"data/correction_dic.pickle\"))\n",
    "    #\n",
    "    #     setup_dict['all_gram_set'] = all_gram_set\n",
    "    #     setup_dict['food_type_dict'] = food_type_dict\n",
    "    #     setup_dict['my_stop_words'] = my_stop_words\n",
    "    #     setup_dict['correction_dic'] = correction_dic\n",
    "    #     return setup_dict\n",
    "\n",
    "    def initialization(self):\n",
    "        # 1. read in manually annotated file and bind it to the object\n",
    "        parser_keys_df = pd.read_csv(pkg_resources.resource_stream(__name__, \"../data/parser_keys.csv\"))\n",
    "        all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n",
    "        correction_dic = pickle.load(pkg_resources.resource_stream(__name__, \"../data/correction_dic.pickle\"))\n",
    "        self.all_gram_set = all_gram_set\n",
    "        self.food_type_dict = food_type_dict\n",
    "        self.correction_dic = correction_dic\n",
    "        self.tmp_correction = {}\n",
    "        # setup_dict = self.run_setup()\n",
    "        # self.my_stop_words = setup_dict['my_stop_words']\n",
    "        # self.gram_mask = gram_mask\n",
    "        # self.final_measurement = final_measurement\n",
    "\n",
    "        # 2. Load common stop words and bind it to the object\n",
    "        stop_words = stopwords.words('english')\n",
    "        # Updated by Katherine August 23rd 2020\n",
    "        stop_words.remove('out') # since pre work out is a valid beverage name\n",
    "        stop_words.remove('no')\n",
    "        stop_words.remove('not')\n",
    "        self.stop_words = stop_words\n",
    "        return\n",
    "\n",
    "    ########## Pre-Processing ##########\n",
    "    # Function for removing numbers\n",
    "    def handle_numbers(self, text):\n",
    "        clean_text = text\n",
    "        clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n",
    "        clean_text = re.sub('[0-9]+', '' , clean_text)\n",
    "        clean_text = re.sub('\\s\\s+', ' ', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "    # Function for removing punctuation\n",
    "    def drop_punc(self, text):\n",
    "        clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        return clean_text\n",
    "\n",
    "    # Remove normal stopwords\n",
    "    def remove_stop(self, my_text):\n",
    "        text_list = my_text.split()\n",
    "        return ' '.join([word for word in text_list if word not in self.stop_words])\n",
    "\n",
    "    # def remove_my_stop(self, text):\n",
    "    #     text_list = text.split()\n",
    "    #     return ' '.join([word for word in text_list if word not in self.my_stop_words])\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        text = text.lower()\n",
    "        # return self.remove_my_stop(self.remove_stop(self.handle_numbers(self.drop_punc(text)))).strip()\n",
    "        return self.remove_stop(self.handle_numbers(self.drop_punc(text))).strip()\n",
    "\n",
    "    ########## Handle Format ##########\n",
    "    def handle_front_mixing(self, sent, front_mixed_tokens):\n",
    "        # print(sent)\n",
    "        cleaned_tokens = []\n",
    "        for t in sent.split():\n",
    "            if t in front_mixed_tokens:\n",
    "                number = re.findall('\\d+[xX]*', t)[0]\n",
    "                for t_0 in t.replace(number, number + ' ').split():\n",
    "                    cleaned_tokens.append(t_0)\n",
    "            else:\n",
    "                cleaned_tokens.append(t)\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    def handle_x2(self, sent, times_x_tokens):\n",
    "        # print(sent)\n",
    "        for t in times_x_tokens:\n",
    "            sent = sent.replace(t, ' ' + t.replace(' ', '').lower() + ' ')\n",
    "        return ' '.join([s for s in sent.split() if s != '']).strip()\n",
    "\n",
    "    def clean_format(self, sent):\n",
    "        return_sent = sent\n",
    "\n",
    "        # Problem 1: 'front mixing'\n",
    "        front_mixed_tokens = re.findall('\\d+[^\\sxX]+', sent)\n",
    "        if len(front_mixed_tokens) != 0:\n",
    "            return_sent = self.handle_front_mixing(return_sent, front_mixed_tokens)\n",
    "\n",
    "        # Problem 2: 'x2', 'X2', 'X 2', 'x 2'\n",
    "        times_x_tokens = re.findall('[xX]\\s*?\\d', return_sent)\n",
    "        if len(times_x_tokens) != 0:\n",
    "            return_sent = self.handle_x2(return_sent, times_x_tokens)\n",
    "        return return_sent\n",
    "\n",
    "    ########## Handle Typo ##########\n",
    "    def fix_spelling(self, entry, speller_check = False):\n",
    "        result = []\n",
    "        for token in entry.split():\n",
    "            # Check known corrections\n",
    "            if token in self.correction_dic.keys():\n",
    "                token_alt = self.wnl.lemmatize(self.correction_dic[token])\n",
    "            else:\n",
    "                if speller_check and token in self.tmp_correction.keys():\n",
    "                    token_alt = self.wnl.lemmatize(self.tmp_correction[token])\n",
    "                elif speller_check and token not in self.food_type_dict.keys():\n",
    "                    token_alt = self.wnl.lemmatize(token)\n",
    "                    # token_alt = self.wnl.lemmatize((self.spell.correction(token)))\n",
    "                    self.tmp_correction[token] = token_alt\n",
    "                else:\n",
    "                    token_alt = self.wnl.lemmatize(token)\n",
    "            result.append(token_alt)\n",
    "        return ' '.join(result)\n",
    "\n",
    "    ########### Combine all cleaning ##########\n",
    "    def handle_all_cleaning(self, entry):\n",
    "        cleaned = self.pre_processing(entry)\n",
    "        cleaned = self.clean_format(cleaned)\n",
    "        cleaned = self.fix_spelling(cleaned)\n",
    "        return cleaned\n",
    "\n",
    "    ########## Handle Gram Matching ##########\n",
    "    def parse_single_gram(self, gram_length, gram_set, gram_lst, sentence_tag):\n",
    "        food_lst = []\n",
    "        # print(gram_length, gram_lst, sentence_tag)\n",
    "        for i in range(len(gram_lst)):\n",
    "            if gram_length > 1:\n",
    "                curr_word = ' '.join(gram_lst[i])\n",
    "            else:\n",
    "                curr_word = gram_lst[i]\n",
    "            # print(curr_word, len(curr_word))\n",
    "            if curr_word in gram_set and sum([t != 'Unknown' for t in sentence_tag[i: i+gram_length]]) == 0:\n",
    "                # Add this founded food to the food list\n",
    "                # food_counter += 1\n",
    "                sentence_tag[i: i+gram_length] = str(gram_length)\n",
    "                # tmp_dic['food_'+ str(food_counter)] = curr_word\n",
    "                food_lst.append(curr_word)\n",
    "                # print('Found:', curr_word)\n",
    "        return food_lst\n",
    "\n",
    "    def parse_single_entry(self, entry, return_sentence_tag = False):\n",
    "        # Pre-processing and Cleaning\n",
    "        cleaned = self.handle_all_cleaning(entry)\n",
    "        # print(cleaned)\n",
    "\n",
    "        # Create tokens and n-grams\n",
    "        tokens = nltk.word_tokenize(cleaned)\n",
    "        bigram = list(nltk.ngrams(tokens, 2)) if len(tokens) > 1 else None\n",
    "        trigram = list(nltk.ngrams(tokens, 3)) if len(tokens) > 2 else None\n",
    "        quadgram = list(nltk.ngrams(tokens, 4)) if len(tokens) > 3 else None\n",
    "        pentagram = list(nltk.ngrams(tokens, 5)) if len(tokens) > 4 else None\n",
    "        all_gram_lst = [tokens, bigram, trigram, quadgram, pentagram]\n",
    "\n",
    "        # Create an array of tags\n",
    "        sentence_tag = np.array(['Unknown'] * len(tokens))\n",
    "        # for i in range(len(sentence_tag)):\n",
    "        #     if tokens[i] in self.final_measurement:\n",
    "        #         sentence_tag[i] = 'MS'\n",
    "\n",
    "        all_food = []\n",
    "        food_counter = 0\n",
    "        for gram_length in [5, 4, 3, 2, 1]:\n",
    "            if len(tokens) < gram_length:\n",
    "                continue\n",
    "            tmp_food_lst = self.parse_single_gram(gram_length,\n",
    "                                                self.all_gram_set[gram_length - 1],\n",
    "                                                all_gram_lst[gram_length - 1],\n",
    "                                                sentence_tag)\n",
    "            all_food += tmp_food_lst\n",
    "        # print(sentence_tag)\n",
    "        if return_sentence_tag:\n",
    "            return all_food, sentence_tag\n",
    "        else:\n",
    "            return all_food\n",
    "\n",
    "    def parse_food(self, entry, return_sentence_tag = False):\n",
    "        result = []\n",
    "        unknown_tokens = []\n",
    "        num_unknown = 0\n",
    "        num_token = 0\n",
    "        for w in entry.split(','):\n",
    "            all_food, sentence_tag = self.parse_single_entry(w, return_sentence_tag)\n",
    "            result += all_food\n",
    "            if len(sentence_tag) > 0:\n",
    "                num_unknown += sum(np.array(sentence_tag) == 'Unknown')\n",
    "                num_token += len(sentence_tag)\n",
    "                cleaned = nltk.word_tokenize(self.handle_all_cleaning(w))\n",
    "\n",
    "                # Return un-catched tokens, groupped into sub-sections\n",
    "                tmp_unknown = ''\n",
    "                for i in range(len(sentence_tag)):\n",
    "                    if sentence_tag[i] == 'Unknown':\n",
    "                        # unknown_tokens.append(cleaned[i])\n",
    "                        tmp_unknown += (' ' + cleaned[i])\n",
    "                        if i == len(sentence_tag) - 1:\n",
    "                            unknown_tokens.append(tmp_unknown)\n",
    "                    elif tmp_unknown != '':\n",
    "                        unknown_tokens.append(tmp_unknown)\n",
    "                        tmp_unknown = ''\n",
    "        # for i in range(len(result)):\n",
    "        #     if result[i] in self.gram_mask.keys():\n",
    "        #          result[i] = self.gram_mask[result[i]]\n",
    "        if return_sentence_tag:\n",
    "            return result, num_token, num_unknown, unknown_tokens\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def find_food_type(self, food):\n",
    "        if food in self.food_type_dict.keys():\n",
    "            return self.food_type_dict[food]\n",
    "        else:\n",
    "            return 'u'\n",
    "\n",
    "    ################# DataFrame Functions #################\n",
    "    def expand_entries(self, df):\n",
    "        assert 'desc_text' in df.columns, '[ERROR!] Required a column of \"desc_text\"!!'\n",
    "\n",
    "        all_entries = []\n",
    "        for idx in range(df.shape[0]):\n",
    "            curr_row = df.iloc[idx]\n",
    "            logging = curr_row.desc_text\n",
    "            tmp_dict = dict(curr_row)\n",
    "            if ',' in logging:\n",
    "                for entry in logging.split(','):\n",
    "                    tmp_dict = tmp_dict.copy()\n",
    "                    tmp_dict['desc_text'] = entry\n",
    "                    all_entries.append(tmp_dict)\n",
    "            else:\n",
    "                tmp_dict['desc_text'] = logging\n",
    "                all_entries.append(tmp_dict)\n",
    "\n",
    "        return pd.DataFrame(all_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953f630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
