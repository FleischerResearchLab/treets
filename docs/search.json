[
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "",
    "text": "#hide\nfrom nbdev.showdoc import *\n\nModuleNotFoundError: No module named 'nbdev'\n\n\n\n#export \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport os\nimport matplotlib.pyplot as plt\nimport pickle\nfrom datetime import date \nfrom datetime import datetime\nimport datetime\nfrom collections import defaultdict \nimport nltk\nnltk.download('words', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('punkt', quiet=True)\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, words\nimport glob\n\nimport re\nimport string\nimport pkg_resources\n\n\nutils\n\n#export\ndef file_loader(data_source):\n    \"\"\"\n    Description:\\n\n        This is a helper function that 1. if data_scource is a single file path in pickle/csv format, it reads it into a pd dataframe. 2. if it is a folder path, it reads csv and pickle files that match the pattern parameter in the data_scource folder into one dataframe. \\n\n    \n    Input:\\n\n        - data_source(str, pandas df): input path, csv or pickle file or a pattern to be matched when searching csv/pickle files that will be read into one dataframe.\\n\n    Output:\\n\n        - a pandas dataframe.\\n\n    \"\"\"\n    \n    if isinstance(data_source, str):\n        data_lst = glob.glob(data_source)\n        dfs = []\n        for x in data_lst:\n            if x[-4:] == '.csv':\n                dfs.append(pd.read_csv(x))\n            elif x[-7:] == '.pickle':\n                pickle_file = open(x, 'rb')\n                pickle_file = pickle.load(pickle_file)\n                if not isinstance(pickle_file, pd.DataFrame):\n                    return pickle_file\n                dfs.append(pickle_file)\n        df = pd.concat(dfs).reset_index(drop=True)\n    else:\n        df = data_source\n\n        \n    return df\n\nThis reads a specific csv file into a pd dataframe.\n\nfile_loader(\"data/col_test_data/toy_data_2000.csv\").head(2)\n\nThis reads in all files that matches yrt*_food_data*.csv pattern in data/col_test_data folder\n\nfile_loader('data/col_test_data/yrt*_food_data*.csv').head(2)\n\nThis reads all pickle and csv files in ./data/output folder.\n\nfile_loader('data/output/*').head(2)\n\n\n#export\ndef find_date(data_source, col, h=0):\n    \"\"\"\n    Description:\\n\n        Extract date information from a column and shift each date in the column by h hours. (Day starts h hours early if h is negative and h hours late if h is positive)\\n\n        \n    Input:\\n\n        - data_scource(str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n        - col(str) : column that contains the information of date and time, which is 24 hours system.\n        - h(int) : hours to shift the date. For example, when h = 4, everyday starts and ends 4 hours later than normal.\n        \n    Return:\\n\n        - a pandas series represents the date extracted from col.\\n\n\n    Requirements:\\n\n        Elements in col should be pd.datetime objects\n            \n    \"\"\"\n    df = file_loader(data_source)\n    \n    if df[col].dtype == 'O':\n        raise TypeError(\"'{}' column must be converted to datetime object\".format(col))\n        \n    def find_date(d, h):\n        if h > 0:\n            if d.hour < h:\n                return d.date() - pd.Timedelta('1 day')\n        if h < 0:\n            if d.hour+1 > (24+h):\n                return d.date() + pd.Timedelta('1 day')\n        return d.date()\n    return df[col].apply(find_date, args=([h]))\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\ndf['date'] = find_date(df, 'original_logtime')\ndf[['original_logtime', 'date']].head(3)\n\n\ndf['date'] = find_date(df, 'original_logtime', 4)\ndf[['original_logtime', 'date']].head(3)\n# last two rows shift one day earlier due to that a day starts at 4\n\n\ndf['date'] = find_date(df, 'original_logtime', -4)\ndf[['original_logtime', 'date']].head(5)\n\nnote that last two rows shift one day later due to that a day ends at 20\n\n#export\ndef find_float_time(data_scource, col, h=0):\n    \"\"\"\n    Description:\\n\n        Extract time information from a column and shift each time by h hours. (Day starts h hours early if h is negative and h hours late if h is positive)\\n\n        \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n        - col(str) : column that contains the information of date and time that's 24 hours system.\n        - h(int) : hours to shift the date. For example, when h = 4, everyday starts at 4 and ends at 28. When h = -4, everyday starts at -4 and ends at 20.\n        \n    Return:\\n\n        - a pandas series represents the date extracted from col.\\n\n\n    Requirements:\\n\n        Elements in col should be pd.datetime objects\n            \n    \"\"\"\n    df = file_loader(data_scource)\n    if df[col].dtype == 'O':\n        raise TypeError(\"'{}' column must be converted to datetime object firsly\".format(col))\n    local_time = df[col].apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.)\n    if h > 0:\n        local_time = np.where(local_time < h, 24+ local_time, local_time)\n        return pd.Series(local_time)\n    if h < 0:\n        local_time = np.where(local_time > (24+h), local_time-24., local_time)\n        return pd.Series(local_time)\n    return local_time\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\n\ndf['float_time'] = find_float_time(df, 'original_logtime')\ndf[['original_logtime', 'float_time']].head(3)\n\n\ndf['float_time'] = find_float_time(df, 'original_logtime', 4)\ndf['date'] = find_date(df, 'original_logtime', 4)\ndf[['original_logtime','date', 'float_time']].head(3)\n# last two rows shift one day earlier due to that a day starts at 4 and ends at 28\n\n\ndf['float_time'] = find_float_time(df, 'original_logtime', -4)\ndf['date'] = find_date(df, 'original_logtime', -4)\ndf[['original_logtime','date', 'float_time']].head(5)\n# last two rows shift one day later and local_time starts at -4 and ends at 20\n\n\n#export\ndef week_from_start(data_scource, col, identifier):\n        \"\"\"\n        Description:\\n\n            Calculate the number of weeks for each logging since the first day of the logging for each participant(identifier). The returned values for loggings from the first week are 1. \n        Input:\\n\n            - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n            - col (str): column name that contains date information from the data_scource dataframe.\n            - identifier (str): unique_id or ID, or name that identifies people.\n\n        Return:\\n\n            - a numpy array represents the date extracted from col.\\n\n        \"\"\"\n        \n        df = file_loader(data_scource)\n        if 'date' not in df.columns:\n            raise NameError(\"There must exist a 'date' column.\")\n        # Handle week from start\n        df_dic = dict(df.groupby(identifier).agg(np.min)[col])\n\n        def count_week_public(s):\n            return (s.date - df_dic[s[identifier]]).days // 7 + 1\n    \n        return df.apply(count_week_public, axis = 1)\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\n\n# date column is a pre-requisite\ndf['date'] = find_date(df, 'original_logtime')\ndf['week_from_start'] = week_from_start(df, 'date', 'unique_code')\ndf[['unique_code','original_logtime','week_from_start']][2:4]\n\n\n#export\ndef find_phase_duration(df):\n    \"\"\"\n    Description:\\n\n        This is a function that calculates how many days each phase in the study took. Result includes the start and end date for that phase.\n    \n    Input:\\n\n        - df(pandas df) : information dataframe that contains columns: Start_Day, End_day\n    \n    Output:\\n\n        - a dataframe contains the phase_duration column.\n        \n    Requirement:\\n\n        - 'Start_day' and 'End_day' column exist in the df.\n        - 'phase_duration' column exists in the df.\n    \"\"\"\n    df['phase_duration'] = df['End_day'] - df['Start_Day']+ pd.Timedelta(\"1 days\")\n    return df\n\n\nfind_phase_duration(pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))[['phase_duration']]\n\n\n#export\ndef load_public_data(data_scource, identifier, datetime_col, h):\n    \"\"\"\n    Description:\\n\n        Load original public data and output processed data in a dataframe.\\n\n    \n        Process includes:\\n\n        1. Dropping 'foodimage_file_name' column.\\n\n        2. Handling the format of time by deleting am/pm by generating a new column, 'original_logtime_notz'\\n\n        3. Generating the date column with possible hour shifts, 'date'\\n\n        4. Converting time into float number into a new column with possible hour shifts, 'float_time'\\n\n        5. Converting time to a format of HH:MM:SS, 'time'\\n\n        6. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\\n\n        7. Generating 'year' column based on the input data.\\n\n\n    Input:\\n\n        - data_scource (str or pandas df): input path, csv file\\n\n        - identifier(str): id-like column that's used to identify a subject.\\n\n        - datetime_col(str): column that contains date and time in string format.\\n\n        - h(int) : hours to shift the date. For example, when h = 4, everyday starts and ends 4 hours later than normal.\n        \n    Output:\\n\n        - the processed dataframe in pandas df format.\\n\n\n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - foodimage_file_name\\n\n            - original_logtime\\n\n            - date\\n\n            - unique_code\\n\n    \"\"\"\n    public_all = file_loader(data_scource)\n    \n    try:\n        public_all = public_all.drop(columns = ['foodimage_file_name'])\n    except KeyError:\n        pass\n    \n    def handle_public_time(s):\n        \"\"\"\n        helper function to get rid of am/pm in the end of each time string\n        \"\"\"\n        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n        try:\n            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n        except:\n            try:\n                if int(tmp_s.split()[1][:2]) > 12:\n                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n            except:\n                return np.nan\n\n    public_all[datetime_col] = public_all[datetime_col].apply(handle_public_time)\n    \n    public_all = public_all.dropna().reset_index(drop = True)\n    \n\n    public_all['date'] = find_date(public_all, datetime_col, h)\n    \n    \n    # Handle the time - Time in floating point format\n    \n    public_all['float_time'] = find_float_time(public_all, datetime_col, h)\n    \n    # Handle the time - Time in Datetime object format\n    public_all['time'] = pd.DatetimeIndex(public_all[datetime_col]).time\n    \n    # Handle week from start\n    public_all['week_from_start'] = week_from_start(public_all,'date',identifier)\n    \n    public_all['year'] = public_all.date.apply(lambda d: d.year)\n    \n    return public_all\n\n\nload_public_data('data/test_food_details.csv','unique_code', 'original_logtime', h=4).head(2)\n\n\n#export\ndef in_good_logging_day(data_scource, identifier, date_col, time_col, min_log_num = 2, min_seperation = 4):\n    \"\"\"\n    Description:\\n\n        A logging's in a good logging day if the there are more than min_log_num loggings in one day w/ more than min_seperation hoursx apart from the earliest logging and the latest logging and False otherwise.\\n\n        \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\\n\n        - identifier (str): id-like column that's used to identify a subject.\\n\n        - time_col (str): column that contains time in float format.\\n\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\\n\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\\n\n        \n    Return:\\n\n        - A boolean numpy array indicating whether the corresponding row is in a good logging day. Details on the criteria is in the description.\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n\n    \"\"\"\n    def adherent(s):\n        if len(s.values) >= min_log_num and (max(s.values) - min(s.values)) >= min_seperation:\n            return True\n        else:\n            return False\n        \n    df = file_loader(data_scource)\n    \n    adherent_dict = dict(df.groupby([identifier, date_col])[time_col].agg(adherent))\n\n    \n    return df.apply(lambda x: adherent_dict[(x[identifier], x.date)], axis = 1)\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code','original_logtime', 4)\ndf['in_good_logging_day'] = in_good_logging_day(df, 'unique_code', 'date', 'float_time')\ndf.head(2)\n\n\n# export \ndef clean_loggings(data_scource, text_col, identifier):\n    \"\"\"\n    Description:\\n\n       This function convert all the loggings in the data_scource file into a list of typo-corrected items based on the text_col column. This function is based on a built-in vocabulary dictionary and an n-gram searcher.\\n\n       \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - text_col(str): column that will be converted to individual items.\n        - identifier(str): participants' unique identifier such as id, name, etc.\n        \n    Return:\\n\n        - A dataframe contains the cleaned version of the text_col.\\n\n\n    \"\"\"\n\n    public_all = file_loader(data_scource)\n    # initialize food parser instance\n    fp = FoodParser()\n    fp.initialization()\n    \n    # parse food\n    parsed = [fp.parse_food(i, return_sentence_tag = True) for i in public_all.desc_text.values]\n    \n    public_all_parsed = pd.DataFrame({\n    identifier: public_all[identifier],\n    text_col: public_all[text_col],\n    'cleaned': parsed\n    })\n    \n    public_all_parsed['cleaned'] = public_all_parsed['cleaned'].apply(lambda x: x[0])\n    \n    \n    return public_all_parsed\n\n\nclean_loggings('data/output/public.pickle', 'desc_text', 'unique_code').head(3)\n\n\n# export\ndef get_types(data_scource, food_type):\n    \"\"\"\n    Description:\\n\n       This function filters with the expected food types and return a cleaner version of data_scource file.\\n \n       \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n        - food_type (str): expected types of the loggings for filtering, in format of list. Available types:  \\n\n            1. 'w' : water \\n\n            2. 'b' : beverage \\n\n            3. 'f' : food \\n\n            4. 'm' : medicine \\n\n        \n    Return:\\n\n        - A filtered dataframe with expected food type/types with five columns: 'unique_code','food_type', 'desc_text', 'date', 'local_time'.\\n\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n            \n    \"\"\"\n    \n    df = file_loader(data_scource)\n        \n    if len(food_type) == 0:\n        return df\n    \n    if len(food_type) == 1:\n        if food_type[0] not in ['w', 'b', 'f', 'm']:\n            raise Exception(\"not a valid logging type\")\n        filtered = df[df['food_type']==food_type[0]]\n    else:  \n        filtered = df[df['food_type'].isin(food_type)]\n        \n    \n    return filtered\n\n\nget_types('data/output/public_basline_usable_expanded.pickle',['w', 'f'])[['unique_code','desc_text','food_type']].head(3)\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nget_types(df, ['m'])[['unique_code','desc_text','food_type']].head(3)\n\n\n#export\ndef count_caloric_entries(df):\n    \"\"\"\n    Description:\\n\n        This is a function that counts the number of food and beverage loggings.\n    \n    Input:\\n\n        - df(pandas df) : food_logging data.\n    \n    Output:\\n\n        - a float representing the number of caloric entries.\n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \"\"\"\n    if 'food_type' not in df.columns:\n        raise Exception(\"'food_type' column must exist in the dataframe.\")\n        \n    \n    return df[df['food_type'].isin(['f','b'])].shape[0]\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\ncount_caloric_entries(df)\n\n\n#export\ndef mean_daily_eating_duration(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This is a function that calculates the mean daily eating window, which is defined as the duration of first and last caloric intake.\n    \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains the float time data for each logging.\n    \n    Output:\\n\n        - a float representing the mean daily eating window.\n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n        - float time is calculated.\n        \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n\n    df = df[df['food_type'].isin(['f','b'])]\n    breakfast_time = df.groupby(date_col).agg(min)\n    dinner_time = df.groupby(date_col).agg(max)\n    return (dinner_time[time_col]-breakfast_time[time_col]).mean()\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nmean_daily_eating_duration(df, 'date', 'float_time')\n\n\n#export\ndef std_daily_eating_duration(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the standard deviation of daily eating window, which is defined as the duration between the first and last caloric intake.\n    \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains the float time data for each logging.\n    \n    Output:\\n\n        - a float representing the standard deviation of daily eating window.\n        \n    Requirement:\\n\n        - 'food_type' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    df = df[df['food_type'].isin(['f','b'])]\n    breakfast_time = df.groupby(date_col).agg(min)\n    dinner_time = df.groupby(date_col).agg(max)\n\n    return (dinner_time[time_col]-breakfast_time[time_col]).std()\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nstd_daily_eating_duration(df, 'date', 'float_time' )\n\n\n#export\ndef earliest_entry(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the earliest first calorie on any day in the study period. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains information of logging time in float.\n    \n    Output:\\n\n        - the earliest caloric time in float on any day in the study period. \n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    df = get_types(df, ['f', 'b'])\n    \n    return df[time_col].min()\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\nearliest_entry(df, 'date', 'float_time')\n\n\n#export\ndef mean_first_cal(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the average time of first calory intake. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains information of logging time in float.\n    \n    Output:\\n\n        - the mean first caloric intake time in float in the study period. \n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    \n    return df.groupby([date_col])[time_col].min().mean()\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\nmean_first_cal(df, 'date', 'float_time')\n\n\n# find the average mean first cal time for each participant\ndf.groupby('unique_code').agg(mean_first_cal, 'date', 'float_time').iloc[:,0]\n\n\n#export\ndef std_first_cal(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the average time of first calory intake. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains information of logging time in float.\n    \n    Output:\\n\n        - the mean first caloric intake time in float in the study period. \n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    \n    return df.groupby([date_col])[time_col].min().std()\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\nstd_first_cal(df, 'date', 'float_time')\n\n\n#export\ndef mean_last_cal(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the average time of last calory intake. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains information of logging time in float.\n    \n    Output:\\n\n        - the mean last caloric intake time in float in the study period. \n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    \n    return df.groupby([date_col])[time_col].max().mean()\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\nmean_last_cal(df, 'date', 'float_time')\n\n\n#export\ndef std_last_cal(df, date_col, time_col):\n    \"\"\"\n    Description:\\n\n        This function calculates the average time of last calory intake. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n        - time_col(column existed in df, string) : contains information of logging time in float.\n    \n    Output:\\n\n        - the mean last caloric intake time in float in the study period. \n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    \n    return df.groupby([date_col])[time_col].max().std()\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\nstd_last_cal(df, 'date', 'float_time')\n\n\n#export\ndef logging_day_counts(df):\n    \"\"\"\n    Description:\\n\n        This function calculates the number of days that contains any loggings. \n    Input:\\n\n        - df(pandas df) : food_logging data.\n    \n    Output:\\n\n        - an integer that represents the number of logging days.\n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \"\"\"\n    \n    return df.date.nunique()\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nlogging_day_counts(df)\n\n\n#export\ndef find_missing_logging_days(df, start_date='not_defined', end_date='not_defined'):\n    \"\"\"\n    Description:\\n\n        This function finds the days during which there's no logging within the period from start_date to end_date. \n    Input:\\n\n        - df(panda df): food_logging data.\n        - start_date(datetime.date object): start date of the period of calculation. If not defined, it will be automatically set to be the earliest date in df. \n        - end_date(datetime.date object): end date of the period of calculation. If not defined, it will be automatically set to be the latest date in df.\n    \n    Output:\\n\n        - a list that contains all of the dates that don't contain loggings.\n        \n    Requirement:\\n\n        - 'date' column existed in the df.\n    \"\"\"\n    # if start_date or end_date is missing, return nan\n    if pd.isnull(start_date) or pd.isnull(end_date):\n        return np.nan\n    # if there is no input on start_date or end_date, use earliest date and latest date\n    if start_date=='not_defined':\n        start_date = df['date'].min()\n    if end_date=='not_defined':\n        end_date = df['date'].max()\n        \n    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n    \n    # get all the dates between two dates\n    lst = []\n    for x in pd.date_range(start_date, end_date, freq='d'):\n         if x not in df['date'].unique():\n                lst.append(x.date())\n    \n    return lst\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nfind_missing_logging_days(df, datetime.date(2017, 12, 7), datetime.date(2017, 12, 10))\n\n\n#export\ndef good_lwa_day_counts(df, window_start, window_end, time_col, min_log_num=2, min_seperation=5, buffer_time= '15 minutes',h=4, start_date='not_defined', end_date='not_defined'):\n    \"\"\"\n    Description:\\n\n        This function calculates the number of good logging days, good window days, outside window days and adherent days. Good logging day is defined as a day that the person makes at least min_log_num number of loggings and the time separation between the earliest and the latest logging are greater than min_seperation.\\n\n        A good window day is defined as a date that all the food loggings are within the assigned restricted window. An adherent day is defined as a date that is both a good logging day and a good window day.\n    Input:\\n\n        - df(pandas df): food_logging data.\n        - window_start(datetime.time object): start time of the restriction window.\n        - window_end(datetime.time object): end time of the restriction window.\n        - time_col(str) : the column that represents the eating time.\n        - min_log_num(count, int): minimum number of loggings to qualify a day as a good logging day\n        - min_seperation(hours, int): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day\n        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.\n        - h(hours, int): hours to be pushed back\n        - start_date(datetime.date object): start date of the period for calculation. If not defined, it will be automatically set to be the earliest date in df.\n        - end_date(datetime.date object): end date of the period for calculation. If not defined, it will be automatically set to be the latest date in df.\n    Output:\\n\n        - Two lists.\n            - First list contains 4 integers that represent the number of good logging days, good window days, outside window days and adherent_days\n            - Second list contains lists consisted of specific dates that are not good logging days, window days and adherent days within the range of start date and end date(both inclusive).\n    Requirement:\\n\n        - 'date' column existed in the df.\n        - float time is calculated as time_col.\n\n    \"\"\"\n    # if start_date or end_date is missing, return nan\n    if pd.isnull(start_date) or pd.isnull(end_date):\n        return [np.nan,np.nan,np.nan,np.nan], [[],[],[]]\n    \n    # if window start or window end are nan, make the windows the same as control's window time.\n    if pd.isnull(window_start):\n        window_start = datetime.time(0,0)\n    if pd.isnull(window_end):\n        window_end = datetime.time(23,59)\n        \n    # if there is no input on start_date or end_date, use earliest date and latest date\n    if start_date=='not_defined':\n        start_date = df['date'].min()\n    if end_date=='not_defined':\n        end_date = df['date'].max()\n\n    # helper function to determine a good logging\n    def good_logging(local_time_series):\n        if len(local_time_series.values) >= min_log_num and (max(local_time_series.values) - min(local_time_series.values)) >= min_seperation:\n            return True\n        else:\n            return False\n\n    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n    df = df[df['food_type'].isin(['f','b'])]\n    df['original_logtime'] = pd.to_datetime(df['original_logtime']).dt.tz_localize(None)\n\n    buffer_time = pd.Timedelta(buffer_time).total_seconds()/3600.\n\n    in_window_count = []\n    daily_count = []\n    good_logging_count = []\n    cur_dates = df['date'].sort_values(ascending = True).unique()\n    for aday in cur_dates:\n        window_start_daily = window_start.hour+window_start.minute/60- buffer_time\n        window_end_daily = window_end.hour+window_end.minute/60 + buffer_time\n        tmp = df[df['date']==aday]\n        if (window_start == datetime.time(0,0)) and (window_end == datetime.time(23,59)):\n            in_window_count.append(tmp[(tmp[time_col]>=window_start_daily+h) & (tmp[time_col]<=window_end_daily+h)].shape[0])\n        else:\n            in_window_count.append(tmp[(tmp[time_col]>=window_start_daily) & (tmp[time_col]<=window_end_daily)].shape[0])\n        daily_count.append(df[df['date']==aday].shape[0])\n        good_logging_count.append(good_logging(df[df['date']==aday][time_col]))\n\n    in_window_count = np.array(in_window_count)\n    daily_count = np.array(daily_count)\n    good_logging_count = np.array(good_logging_count)\n    good_logging_by_date = [cur_dates[i] for i, x in enumerate(good_logging_count) if x == False]\n\n    good_window_days = (in_window_count==daily_count)\n    good_window_day_counts = good_window_days.sum()\n    good_window_by_date = [cur_dates[i] for i, x in enumerate(good_window_days) if x == False]\n    \n    outside_window_days = in_window_count.size - good_window_days.sum()\n    good_logging_days = good_logging_count.sum()\n    if good_logging_count.size == 0:\n        adherent_day_counts = 0\n        adherent_days_by_date = []\n    else:\n        adherent_days = (good_logging_count & (in_window_count==daily_count))\n        adherent_days_by_date = [cur_dates[i] for i, x in enumerate(adherent_days) if x == False]\n        adherent_day_counts = adherent_days.sum()\n    \n    rows = [good_logging_days, good_window_day_counts, outside_window_days, adherent_day_counts]\n    bad_dates = [good_logging_by_date, good_window_by_date, adherent_days_by_date]\n\n    return rows, bad_dates\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime',4)\ndates, bad_dates = good_lwa_day_counts(df, datetime.time(8,0,0), datetime.time(23,59,59), 'float_time')\ndates\n\n\nbad_dates[0][:5]\n\n\n\nexperiment design\n\n# export\ndef filtering_usable_data(df, identifier, date_col, num_items, num_days):\n    '''\n    Description:\\n\n        This function filters the cleaned app data so the users who satisfies the criteria are left. The criteria is that the person is left if the total loggings for that person are more than num_items and at the same time, the total days of loggings are more than num_days.\\n\n    Input:\\n\n        - df (pd.DataFrame): the dataframe to be filtered\n        - identifier (str): unique_id or ID, or name that identifies people.\n        - date_col (str): column name that contains date information from the df dataframe.\n        - num_items (int):   number of items to be used as cut-off\n        - num_days (int):    number of days to be used as cut-off\n    Output:\\n\n        - df_usable:  a panda DataFrame with filtered rows\n        - set_usable: a set of unique_code to be included as \"usable\"\n    Requirements:\\n\n        df should have the following columns:\n            - desc_text\n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n    '''\n    print(' => filtering_usable_data()')\n    print('  => using the following criteria:', num_items, 'items and', num_days, 'days.')\n\n    # Item logged\n    log_item_count = df.groupby(identifier).agg('count')[['desc_text']].rename(columns = {'desc_text': 'Total Logged'})\n\n    # Day counts\n    log_days_count = df[[identifier, date_col]]\\\n        .drop_duplicates().groupby(identifier).agg('count').rename(columns = {date_col: 'Day Count'})\n\n    item_count_passed = set(log_item_count[log_item_count['Total Logged'] >= num_items].index)\n    day_count_passed = set(log_days_count[log_days_count['Day Count'] >= num_days].index)\n\n    print('  => # of public users pass the criteria:', end = ' ')\n    print(len(item_count_passed & day_count_passed))\n    passed_participant_set = item_count_passed & day_count_passed\n    df_usable = df.loc[df.unique_code.apply(lambda c: c in passed_participant_set)]\\\n        .copy().reset_index(drop = True)\n    # print('  => Now returning the pd.DataFrame object with the head like the following.')\n    # display(df_usable.head(5))\n    return df_usable, set(df_usable.unique_code.unique())\n\n\ndf = file_loader('data/output/public.pickle')\ndf.shape\n\n\nfiltering_usable_data(df,'unique_code','date', 1000, 14)[0].shape\n\n\n#export\ndef prepare_baseline_and_intervention_usable_data(data_scource, identifier, date_col, baseline_num_items, baseline_num_days, intervention_num_items, intervention_num_days):\n    \"\"\"\n    Description:\\n\n        Filter and create baseline_expanded and intervention groups based on data_scource pickle file. Expanded baseline dataset contains the first two weeks data and 13, 14 weeks data that pass the given criteria. Intervention dataset contains 13, 14 weeks data that pass the given criteria.\\n\n        \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n        - identifier (str): unique_id or ID, or name that identifies people.\n        - date_col (str): column name that contains date information from the df dataframe.\n        - baseline_num_items (int): number of items to be used as cut-off for baseline group. \\n\n        - baseline_num_days (int): number of days to be used as cut-off for baseline group. \\n\n        - intervention_num_items (int): number of items to be used as cut-off for intervention group.\\n\n        - intervention_num_days (int): number of days to be used as cut-off for intervention group. \\n\n        \n    Return:\\n\n        - a list in which index 0 is the baseline expanded dataframe and 1 is the intervention dataframe.\\n\n\n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - week_from_start\\n\n            - desc_text\\n\n    \"\"\"\n    \n    \n    public_all = file_loader(data_scource)\n    \n    # create baseline data\n    df_public_baseline = public_all.query('week_from_start <= 2')\n    df_public_baseline_usable, public_baseline_usable_id_set = \\\n    filtering_usable_data(df_public_baseline,identifier, date_col, num_items = baseline_num_items, num_days = baseline_num_days)\n    \n    # create intervention data\n    df_public_intervention = public_all.query('week_from_start in [13, 14]')\n    df_public_intervention_usable, public_intervention_usable_id_set = \\\n    filtering_usable_data(df_public_intervention,identifier, date_col, num_items = intervention_num_items, num_days = intervention_num_days)\n    \n    # create df that contains both baseline and intervention id_set that contains data for the first two weeks\n    expanded_baseline_usable_id_set = set(list(public_baseline_usable_id_set) + list(public_intervention_usable_id_set))\n    df_public_basline_usable_expanded = public_all.loc[public_all.apply(lambda s: s.week_from_start <= 2 \\\n                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n        \n    return [df_public_basline_usable_expanded, df_public_intervention_usable]\n\n\ndf= prepare_baseline_and_intervention_usable_data('data/output/public.pickle','unique_code','date', 20, 10, 40, 12)[0]\ndf.head(2)\n\n\ndf.shape\n\n\n\ndata analysis/summary\n\n#export\ndef users_sorted_by_activity(data_scource, food_type = [\"f\", \"b\", \"m\", \"w\"]):\n    \"\"\"\n    Description:\\n\n        This function returns a dataframe reports the number of good logging days for each user in the data_scource file. The default order is descending.\\n\n        \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\\n\n        - food_type (str): food types to filter in a list format. Default: [\"f\", \"b\", \"m\", \"w\"]. Available food types:\\n\n            1. 'w' : water \\n\n            2. 'b' : beverage \\n\n            3. 'f' : food \\n\n            4. 'm' : medicine \\n\n        \n    Return:\\n\n        - A dataframe contains the number of good logging days for each user.\\n\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n            - unique_code\\n\n            - date\\n\n    \n    \"\"\"\n\n    public_all = file_loader(data_scource)\n    \n    # filter the dataframe so it only contains input food type\n    \n    filtered_users = public_all.query('food_type in @food_type')\n    \n    filtered_users['in_good_logging_day'] = in_good_logging_day(filtered_users,'unique_code','date','local_time')\n    \n    public_top_users_day_counts = pd.DataFrame(filtered_users.query('in_good_logging_day == True')\\\n                            [['date', 'unique_code']].groupby('unique_code')['date'].nunique())\\\n                            .sort_values(by = 'date', ascending = False).rename(columns = {'date': 'day_count'})\n\n    \n    return public_top_users_day_counts\n\n\nusers_sorted_by_activity('data/output/public.pickle', ['f','b']).head(2)\n\n\n#export\ndef eating_intervals_percentile(data_scource, time_col, identifier):\n    \"\"\"\n    Description:\n       This function calculates the .025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975 quantile of eating time and mid 95%, mid 90%, mid 80%, mid 75% and mid 50% duration for each user.\\n \n    \n    Input:\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - time_col(str) : the column that represents the eating time.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        \n    Return:\\n\n        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_scource file.\n    \n    Optional functions to use to have proper inputs:\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n    \n    if df.shape[0] == 0:\n        return pd.DataFrame(np.array([[np.nan]*21]), columns = ['count', 'mean', 'std', 'min', '2.5%', '5%', '10%', '12.5%', '25%',\n       '50%', '75%', '87.5%', '90%', '95%', '97.5%', 'max', 'duration mid 95%',\n       'duration mid 90%', 'duration mid 80%', 'duration mid 75%',\n       'duration mid 50%'])\n    \n    ptile = df.groupby(identifier)[time_col].describe(percentiles=[.025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975])\n    ll = ['2.5%','5%','10%','12.5%','25%']\n    ul = ['97.5%','95%', '90%','87.5%', '75%']\n    mp = ['duration mid 95%', 'duration mid 90%', 'duration mid 80%', 'duration mid 75%','duration mid 50%']\n    for low, upp, midp in zip(ll,ul,mp):\n        ptile[midp] = ptile[upp] - ptile[low]\n        \n    return ptile\n\n\ndf = load_public_data('data/test_food_details.csv','unique_code', 'original_logtime',4)\neating_intervals_percentile(df, 'float_time', 'unique_code').iloc[:2]\n\n\n# export\ndef first_cal_analysis_summary(data_scource, identifier, date_col, time_col,min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for each user.\\n \n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        \n        \n    Return:\\n\n        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for all subjects from the data_scource file.\\n\n        \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n\n    df = file_loader(data_scource)\n        \n    # leave only the loggings in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n    \n    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n    first_cal_df = pd.DataFrame(first_cal_series)\n    all_rows = []\n    for index in first_cal_df.index:\n        tmp_dict = dict(first_cal_series[index[0]])\n        tmp_dict['id'] = index[0]\n        all_rows.append(tmp_dict)\n    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n        .drop_duplicates().reset_index(drop = True)\n    \n    return first_cal_summary_df\n\n\nfirst_cal_analysis_summary('data/output/public_basline_usable_expanded.pickle', 'unique_code', 'date', 'local_time').head(3)\n\n\n# export\ndef last_cal_analysis_summary(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for each user.\\n \n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        \n    Return:\\n\n        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for all subjects from the data_scource file.\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    df = file_loader(data_scource)\n        \n    # leave only the loggings that are in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier,date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n    \n    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n    last_cal_df = pd.DataFrame(last_cal_series)\n    all_rows = []\n    for index in last_cal_df.index:\n        tmp_dict = dict(last_cal_series[index[0]])\n        tmp_dict['id'] = index[0]\n        all_rows.append(tmp_dict)\n    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n        .drop_duplicates().reset_index(drop = True)\n\n    \n    return last_cal_summary_df\n\n\nlast_cal_analysis_summary('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time').head(3)\n\n\n#export\ndef summarize_data(data_scource, identifier, float_time_col, date_col, min_log_num = 2, min_seperation = 4):\n    \"\"\"\n    Description:\\n\n       This function calculates num_days, num_total_items, num_f_n_b, num_medications, num_water, duration_mid_95, start_95, end_95, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, adherent_count, first_cal variation (90%-10%), last_cal variation (90%-10%).\\n \n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - float_time_col(str) : the column that represents the eating time.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\\n\n        \n    Return:\\n\n        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_scource file.\\n\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n  \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n    \n    #num_days\n    num_days = df.groupby(identifier).date.nunique()\n\n    #num_total_items\n    num_total_items = df.groupby(identifier).count().iloc[:,0]\n\n    #num_f_n_b\n    num_f_n_b = get_types(df, ['f','b']).groupby(identifier).count().iloc[:,0]\n\n    #num_medications\n    num_medications = get_types(df, ['m']).groupby(identifier).count().iloc[:,0]\n\n    #num_water\n    num_water = get_types(df, ['w']).groupby(identifier).count().iloc[:,0]\n\n    #duration_mid_95, start_95, end_95\n    eating_intervals = eating_intervals_percentile(df, float_time_col, identifier)[['2.5%','95%','duration mid 95%']]\n\n    #first_cal_avg\n    first_cal_avg = df.groupby([identifier, date_col])[float_time_col].min().groupby(identifier).mean()\n\n    #first_cal_std\n    first_cal_std = df.groupby([identifier, date_col])[float_time_col].min().groupby(identifier).std()\n\n    #last_cal_avg\n    last_cal_avg = df.groupby([identifier, date_col])[float_time_col].max().groupby(identifier).mean()\n\n    #last_cal_std\n    last_cal_std = df.groupby([identifier, date_col])[float_time_col].max().groupby(identifier).std()\n\n    #eating_win_avg\n    eating_win_avg = last_cal_avg - first_cal_avg\n\n    #eating_win_std\n    eating_win_std = (df.groupby([identifier, date_col])[float_time_col].max()-\n                          df.groupby([identifier, date_col])[float_time_col].min()).groupby(identifier).std()\n    \n    #good_logging_count\n    df['in_good_logging_day'] = in_good_logging_day(df,identifier,date_col, float_time_col, min_log_num=min_log_num, min_seperation=min_seperation)\n    good_logging_count = df.groupby(identifier)['in_good_logging_day'].sum()\n\n    #first_cal variation (90%-10%)\n    first_cal_variability = first_cal_analysis_summary(df,identifier,date_col,float_time_col).set_index('id')\n    for col in first_cal_variability.columns:\n        if col == 'id' or col == '50%':\n            continue\n        first_cal_variability[col] = first_cal_variability[col] - first_cal_variability['50%']\n    first_cal_ser = first_cal_variability['90%'] - first_cal_variability['10%']\n\n    #last_cal variation (90%-10%)\n    last_cal_variability = last_cal_analysis_summary(df,identifier,date_col,float_time_col).set_index('id')\n    for col in last_cal_variability.columns:\n        if col == 'id' or col == '50%':\n            continue\n        last_cal_variability[col] = last_cal_variability[col] - last_cal_variability['50%']\n    last_cal_ser = last_cal_variability['90%'] - last_cal_variability['10%']\n\n    returned = pd.concat([num_days, num_total_items, num_f_n_b, num_medications, num_water, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, good_logging_count, first_cal_ser, last_cal_ser], axis=1).reset_index()\n    returned.columns = [identifier,'num_days', 'num_total_items', 'num_f_n_b', 'num_medications', 'num_water', 'first_cal_avg', 'first_cal_std', 'last_cal_avg', 'last_cal_std', 'eating_win_avg', 'eating_win_std', 'good_logging_count', 'first_cal variation (90%-10%)', 'last_cal variation (90%-10%)']\n    returned = returned.merge(eating_intervals, on = identifier, how='left').fillna(0)\n    \n    returned['num_medications'] = returned['num_medications'].astype('int')\n    returned['num_water'] = returned['num_water'].astype('int')\n    \n    return returned\n\n\ndf = load_public_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\nsummarize_data(df,'unique_code', 'float_time', 'date')\n\n\n#export\ndef summarize_data_with_experiment_phases(food_data, ref_tbl, min_log_num=2, min_seperation=5, buffer_time= '15 minutes', h=4,report_level=2, txt = False):\n    \"\"\"\n    Description:\\n\n        This is a comprehensive function that performs all of the functionalities needed.\n    \n    Input:\\n\n        - food_data(panda df): food_logging data.\n        - ref_tbl(panda df): table that contains window information and study phase information for each participant.\n        - min_log_num(counts): minimum number of loggings to qualify a day as a good logging day.\n        - min_seperation(hours): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day\n        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.\n        - h(hours): hours to be pushed back.\n        - report_level(int): whether to print out the dates of no logging days, bad logging days, bad window days and non-adherent days for each participant. 0 - no report. 1 - report no logging days. 2 - report no logging days, bad logging days, bad window days and non adherent days.\n        - txt(boolean): if True, a txt format report will be saved in the current directory named \"treets_warning_dates.txt\".\n        \n\n    Output:\\n\n        - df : dataframe that has all the variables needed and has the same row number as the ref_tbl.\n        \n    Requirement:\\n\n        - food logging data is already read from all files in the directories into a dataframe, which will be passed in as the variable food_data.\n        - Columns 'Start_Day', 'End_day', 'mCC_ID', 'Eating_Window_Start', 'Eating_Window_End' existed in the ref_tbl.\n    \n    Sidenote: \n        - For eating window without restriction(HABIT or TRE not in intervention period), Eating_Window_Start is 0:00, Eating_Window_End is 23:59.\n    \n    \"\"\"\n    df = food_data.copy()\n    # preprocess to get the date and float_time column\n    df['original_logtime'] = pd.to_datetime(df['original_logtime'])\n    df['date'] =  find_date(df, 'original_logtime', h)\n    df['float_time'] =  find_float_time(df, 'original_logtime', h)\n    \n    # get study phase duration\n    result = find_phase_duration(ref_tbl)\n    \n    # reset the index of ref_tbl to avoid issues during concatenation\n    ref_tbl = ref_tbl.reset_index(drop=True)\n    \n    # loop through each row and get 'caloric_entries', 'mean_daily_eating_window', 'std_daily_eating_window', 'eariliest_entry', 'logging_day_counts',\n    # and 'good_logging_days', 'good_window_days', 'outside_window_days' and 'adherent_days' and find missing dates\n    matrix = []\n    missing_dates = {}\n    bad_dates_dic = {}\n   \n    for index, row in ref_tbl.iterrows():\n        id_ = row['mCC_ID']\n        rows = []\n        temp_df = df[df['PID']==id_]\n        temp_df = temp_df[(temp_df['date']>=row['Start_Day']) & (temp_df['date']<=row['End_day'])]\n        # num of caloric entries\n        rows.append(count_caloric_entries(temp_df))\n        # num of medication\n        rows.append(get_types(temp_df, ['m']).shape[0])\n        # num of water\n        rows.append(get_types(temp_df, ['w']).shape[0])\n        # first cal average\n        rows.append(mean_first_cal(temp_df,'date', 'float_time'))\n        #first cal std\n        rows.append(std_first_cal(temp_df, 'date', 'float_time'))\n        # last cal average\n        rows.append(mean_last_cal(temp_df,'date', 'float_time'))\n        # last cal std\n        rows.append(std_last_cal(temp_df, 'date', 'float_time'))\n        # mean eating window\n        rows.append(mean_daily_eating_duration(temp_df,'date','float_time'))\n        \n        rows.append(std_daily_eating_duration(temp_df,'date','float_time'))\n        rows.append(earliest_entry(temp_df, 'date','float_time'))\n\n        rows.append(logging_day_counts(temp_df))\n        row_day_num, bad_dates = good_lwa_day_counts(df[df['PID']==id_]\n                                           , window_start=row['Eating_Window_Start']\n                                           , window_end = row['Eating_Window_End']\n                                           , time_col = 'float_time'\n                                           , min_log_num=min_log_num\n                                           , min_seperation=min_seperation\n                                           , buffer_time= buffer_time\n                                           , start_date=row['Start_Day']\n                                           , end_date=row['End_day']\n                                            , h=h)\n        for x in row_day_num:\n            rows.append(x)\n        bad_logging = bad_dates[0]\n        bad_window = bad_dates[1]\n        non_adherent = bad_dates[2]\n\n        if '{}_bad_logging'.format(id_) not in bad_dates_dic:\n            bad_dates_dic['{}_bad_logging'.format(id_)]=bad_logging\n            bad_dates_dic['{}_bad_window'.format(id_)]=bad_window\n            bad_dates_dic['{}_non_adherent'.format(id_)]=non_adherent\n        else:\n            bad_dates_dic['{}_bad_logging'.format(id_)]+=bad_logging\n            bad_dates_dic['{}_bad_window'.format(id_)]+=bad_window\n            bad_dates_dic['{}_non_adherent'.format(id_)]+=non_adherent\n                \n        matrix.append(rows)\n        date_lst = find_missing_logging_days(df[df['PID']==id_], row['Start_Day'],row['End_day'])\n        # only consider when the result is not nan\n        if isinstance(date_lst, list)==True:\n            if id_ in missing_dates:\n                missing_dates[id_] += date_lst\n            else:\n                missing_dates[id_] = date_lst\n\n    # create a temp dataframe\n    tmp = pd.DataFrame(matrix, columns = ['caloric_entries_num','medication_num', 'water_num','first_cal_avg','first_cal_std',\n                                          'last_cal_avg', 'last_cal_std', 'mean_daily_eating_window', 'std_daily_eating_window', 'earliest_entry', 'logging_day_counts'\\\n                                         ,'good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days'])\n    \n    # concat these two tables\n    returned = pd.concat([ref_tbl, tmp], axis=1)\n    \n    # loop through each row and get 2.5%, 97.5%, duration mid 95% column\n    column_025 = []\n    column_975 = []\n    for index, row in ref_tbl.iterrows():\n        id_ = row['mCC_ID']\n        temp_df = df[df['PID']==id_]\n        temp_df = temp_df[(temp_df['date']>=row['Start_Day']) & (temp_df['date']<=row['End_day'])]\n        series = eating_intervals_percentile(temp_df, 'float_time', 'PID')\n        try:\n            column_025.append(series.iloc[0]['2.5%'])\n        except:\n            column_025.append(np.nan)\n        try:\n            column_975.append(series.iloc[0]['97.5%'])\n        except:\n            column_975.append(np.nan)\n    returned['2.5%'] = column_025\n    returned['97.5%'] = column_975\n    returned['duration mid 95%'] = returned['97.5%'] - returned['2.5%']\n    \n    def convert_to_percentage(ser, col):\n        if pd.isnull(ser[col]):\n            return ser[col]\n        else:\n            return str(round(ser[col]/ser['phase_duration'].days * 100, 2)) + '%'\n    \n    # calculate percentage for \n    for x in returned.columns:\n        if x in ['logging_day_counts','good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days']:\n            returned['%_'+x] = returned.apply(convert_to_percentage, col = x, axis = 1)\n\n    # reorder the columns\n    returned = returned[['mCC_ID', 'Participant_Study_ID', 'Study Phase',\n       'Intervention group (TRE or HABIT)', 'Start_Day', 'End_day',\n       'Eating_Window_Start','Eating_Window_End', 'phase_duration',\n       'caloric_entries_num','medication_num', 'water_num','first_cal_avg',\n        'first_cal_std','last_cal_avg', 'last_cal_std', \n        'mean_daily_eating_window', 'std_daily_eating_window',\n       'earliest_entry', '2.5%', '97.5%', 'duration mid 95%',\n       'logging_day_counts', '%_logging_day_counts', 'good_logging_days',\n        '%_good_logging_days','good_window_days', '%_good_window_days', \n        'outside_window_days','%_outside_window_days', 'adherent_days',\n       '%_adherent_days']]    \n    \n    if report_level == 0:\n        return returned\n    \n    if txt:\n        with open('treets_warning_dates.txt', 'w') as f:\n            # print out missing dates with participant's id\n            for x in missing_dates:\n                if len(missing_dates[x])>0:\n                    f.write(\"Participant {} didn't log any food items in the following day(s):\\n\".format(x))\n                    print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n                    for date in missing_dates[x]:\n                        f.write(str(date)+'\\n')\n                        print(date)\n    else:\n        for x in missing_dates:\n            if len(missing_dates[x])>0:\n                print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n                for date in missing_dates[x]:\n                    print(date)\n                \n    if report_level == 1:\n        return returned\n    \n    if txt:\n        with open('treets_warning_dates.txt', 'a') as f:\n            # print out bad logging, bad window and non-adherent dates with participant's id\n            for x in bad_dates_dic:\n                if len(bad_dates_dic[x])>0:\n                    strings = x.split('_')\n                    f.write(\"Participant {} have {} day(s) in the following day(s):\\n\".format(strings[0], strings[1]+' '+strings[2]))\n                    print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n                    for date in bad_dates_dic[x]:\n                        f.write(str(date)+'\\n')\n                        print(date)\n    else:\n        for x in bad_dates_dic:\n            if len(bad_dates_dic[x])>0:\n                strings = x.split('_')\n                print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n                for date in bad_dates_dic[x]:\n                    print(date)\n    \n    return returned\n\n\ndf = summarize_data_with_experiment_phases(pd.read_csv('data/col_test_data/toy_data_2000.csv')\\\n                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'), report_level = 2)\ndf.T\n\n\n\nplots\n\n# export\ndef first_cal_mean_with_error_bar(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function takes the loggings in good logging days, calculates the means and standard deviations of first_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\\n \n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        \n    Return:\\n\n        - None.\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n\n    # leave only the loggings that are in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n\n    first_cal_series = df.groupby([identifier, date_col])[time_col].min()\n    \n    \n    # find means and stds for each person\n    means = first_cal_series.groupby('unique_code').mean().to_frame().rename(columns={'local_time':'mean'})\n    stds = first_cal_series.groupby('unique_code').std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n    \n    if means.shape[0] > 50:\n        print(\"More than 50 people are present which might make the graph look messy\")\n    \n    temp = pd.concat([means,stds], axis=1)\n    temp.sort_values('mean', inplace=True)\n    \n    # plot scatter plot with error bars\n    plt.scatter(range(temp.shape[0]),temp['mean'])\n    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n    plt.xticks([])\n    plt.ylabel(\"Hours in a day\")\n    plt.title('first_cal Time per Person in Ascending Order')\n\n\nfirst_cal_mean_with_error_bar('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n# export\ndef last_cal_mean_with_error_bar(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function takes the loggings in good logging days, calculates the means and standard deviations of last_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        \n    Return:\\n\n        - None.\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n\n    # leave only the loggings that are in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n\n    last_cal_series = df.groupby([identifier, date_col])[time_col].max()\n    \n    \n    # find means and stds for each person\n    means = last_cal_series.groupby('unique_code').mean().to_frame().rename(columns={'local_time':'mean'})\n    stds = last_cal_series.groupby('unique_code').std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n    \n    if means.shape[0] > 50:\n        print(\"More than 50 people are present which might make the graph look messy\")\n    \n    temp = pd.concat([means,stds], axis=1)\n    temp.sort_values('mean', inplace=True)\n    \n    # plot scatter plot with error bars\n    plt.scatter(range(temp.shape[0]),temp['mean'])\n    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n    plt.xticks([])\n    plt.ylabel(\"Hours in a day\")\n    plt.title('last_cal Time per Person in Ascending Order')\n\n\nlast_cal_mean_with_error_bar('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n# export\ndef first_cal_analysis_variability_plot(data_scource,identifier, date_col, time_col, min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time from the 50% first_cal time. It can also make a histogram that represents the 90%-10% interval for all subjects.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.\n        \n    Return:\\n\n        - A dataframe that contains 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time minus 50% time for each subjects from the data_scource file.\\n\n        \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n\n    df = file_loader(data_scource)\n        \n    # leave only the loggings in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n    \n    first_cal_series = df.groupby(['unique_code', 'date'])['local_time'].min().groupby('unique_code').quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n    first_cal_df = pd.DataFrame(first_cal_series)\n    all_rows = []\n    for index in first_cal_df.index:\n        tmp_dict = dict(first_cal_series[index[0]])\n        tmp_dict['id'] = index[0]\n        all_rows.append(tmp_dict)\n    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n        .drop_duplicates().reset_index(drop = True)\n    first_cal_variability_df = first_cal_summary_df.copy()\n    \n    for col in first_cal_variability_df.columns:\n        if col == 'id' or col == '50%':\n            continue\n        first_cal_variability_df[col] = first_cal_variability_df[col] - first_cal_variability_df['50%']\n    first_cal_variability_df['50%'] = first_cal_variability_df['50%'] - first_cal_variability_df['50%']\n    \n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    sns_plot = sns.distplot( first_cal_variability_df['90%'] - first_cal_variability_df['10%'] )\n    ax.set(xlabel='Variation Distribution for first_cal (90% - 10%)', ylabel='Kernel Density Estimation')\n\n\nfirst_cal_analysis_variability_plot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n# export\ndef last_cal_analysis_variability_plot(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n    \"\"\"\n    Description:\\n\n       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time from the 50% last_cal time and makes a histogram that represents the 90%-10% interval for all subjects.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.\n    Return:\\n\n        - None\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    df = file_loader(data_scource)\n        \n    # leave only the loggings that are in a good logging day\n    df['in_good_logging_day'] = in_good_logging_day(df, identifier,date_col, time_col, min_log_num, min_separation)\n    df = df[df['in_good_logging_day']==True]\n    \n    last_cal_series = df.groupby(['unique_code', 'date'])['local_time'].max().groupby('unique_code').quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n    last_cal_df = pd.DataFrame(last_cal_series)\n    all_rows = []\n    for index in last_cal_df.index:\n        tmp_dict = dict(last_cal_series[index[0]])\n        tmp_dict['id'] = index[0]\n        all_rows.append(tmp_dict)\n    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n        .drop_duplicates().reset_index(drop = True)\n    last_cal_variability_df = last_cal_summary_df.copy()\n    \n    for col in last_cal_variability_df.columns:\n        if col == 'id' or col == '50%':\n            continue\n        last_cal_variability_df[col] = last_cal_variability_df[col] - last_cal_variability_df['50%']\n    last_cal_variability_df['50%'] = last_cal_variability_df['50%'] - last_cal_variability_df['50%']\n    \n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    sns_plot = sns.distplot( last_cal_variability_df['90%'] - last_cal_variability_df['10%'] )\n    ax.set(xlabel='Variation Distribution for last_cal (90% - 10%)', ylabel='Kernel Density Estimation')\n\n\nlast_cal_analysis_variability_plot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n#export\ndef first_cal_avg_histplot(data_scource, identifier, date_col, time_col):\n    \"\"\"\n    Description:\\n\n       This function take the first caloric event (no water or med) and calculate average event's time for “each participant”.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n\n    Return:\\n\n        - None\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n    df = df.query('food_type in [\"f\", \"b\"]')\n    first_cal_time = df.groupby([identifier, date_col])[time_col].min()\n    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier).mean()\n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    sns.distplot(avg_first_cal_time[time_col], kde = False)\n    ax.set(xlabel='First Meal Time - Averaged by Person', ylabel='Frequency Count')\n\n\nfirst_cal_avg_histplot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n# export\ndef first_cal_sample_distplot(data_scource, n, identifier, date_col, time_col):\n    \"\"\"\n    Description:\\n\n       This function plots the distplot for the first_cal time from n participants that will be randomly selected with replacement.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n        - n (int): the number of distplot in the output figure\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        \n    Return:\\n\n        - None\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n    df = df[df['food_type'].isin(['f','b'])]\n    first_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n                                       [time_col].min())\n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    \n    print('Plotting distplots for the following users:')\n    for i in np.random.choice(np.array(list(set(first_cal_by_person.index.droplevel(date_col)))), n):\n        print(i)\n        sns.distplot(first_cal_by_person[time_col].loc[i])\n\n\nfirst_cal_sample_distplot('data/output/public_intervention_usable.pickle',5, 'unique_code', 'date', 'local_time')\n\n\n#export\ndef last_cal_avg_histplot(data_scource, identifier, date_col, time_col):\n    \"\"\"\n    Description:\\n\n       This function take the last caloric event (no water or med) and calculate average event's time for “each participant”.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n\n    Return:\\n\n        - None\n        \n    Requirements:\\n\n            - food_type\\n\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    df = file_loader(data_scource)\n    df = df.query('food_type in [\"f\", \"b\"]')\n    first_cal_time = df.groupby([identifier, date_col])[time_col].max()\n    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier).mean()\n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    sns.distplot(avg_first_cal_time[time_col], kde = False)\n    ax.set(xlabel='Last Meal Time - Averaged by Person', ylabel='Frequency Count')\n\n\nlast_cal_avg_histplot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')\n\n\n# export\ndef last_cal_sample_distplot(data_scource, n, identifier, date_col, time_col):\n    \"\"\"\n    Description:\\n\n       This function plots the distplot for the last_cal time from n participants that will be randomly selected with replacement.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n        - n (int): the number of participants that will be randomly selected in the output figure\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n        \n    Return:\\n\n        - None\n        \n    Requirements:\\n\n        - food_type\n    \n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n        \n    \"\"\"\n    df = file_loader(data_scource)\n    df = df[df['food_type'].isin(['f','b'])]\n    last_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n                                       [time_col].max())\n    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n    \n    print('Plotting distplots for the following users:')\n    for i in np.random.choice(np.array(list(set(last_cal_by_person.index.droplevel(date_col)))), n):\n        print(i)\n        sns.distplot(last_cal_by_person[time_col].loc[i])\n\n\nlast_cal_sample_distplot('data/output/public_intervention_usable.pickle',5,'unique_code', 'date', 'local_time')\n\n\n# export\ndef swarmplot(data_scource, max_loggings, identifier, date_col, time_col):\n    \"\"\"\n    Description:\\n\n       This function plots the swarmplot the participants from the data_scource file.\\n\n    \n    Input:\\n\n        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n        - max_loggings (int): the max number of loggings to be plotted for each participants, loggings will be randomly selected.\n        - identitfier(str) : participants' unique identifier such as id, name, etc.\n        - date_col(str) : the column that represents the dates.\n        - time_col(str) : the column that represents the float time.\n    Return:\\n\n        - None\n        \n    Requirements:\\n\n        data_scource file must have the following columns:\\n\n            - food_type\\n\n    Optional functions to use to have proper inputs:\n        - find_date() for date_col\n        - find_float_time() for time_col\n    \"\"\"\n    \n    df = file_loader(data_scource)\n    \n    def subsamp_by_cond(alldat):\n        alld = []\n        for apart in alldat[identifier].unique():\n            dat = alldat[alldat[identifier]==apart]\n            f_n_b = dat.query('food_type in [\"f\", \"b\"]')\n            n = min([f_n_b.shape[0], max_loggings])\n            sub = f_n_b.sample(n = n, axis=0)\n            alld.append(sub)\n        return pd.concat(alld)\n\n    sample = subsamp_by_cond(df)\n    fig, ax = plt.subplots(1, 1, figsize = (10, 30), dpi=300)\n\n\n    ax.axvspan(3.5,6, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n    ax.axvspan(18,28.5, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n    # plt.xlabel('Hour of day')\n    plt.xticks([4,8,12,16,20,24,28],[4,8,12,16,20,24,4])\n    plt.title('Food events for TRE group')\n\n    ax = sns.swarmplot(data = sample, \n                  y = identifier, \n                  x = time_col, \n                  dodge = True, \n                  color = sns.xkcd_rgb['golden rod'],\n                 )\n\n    ax.set(\n        facecolor = 'white', \n        title = 'Food events (F & B)',\n        ylabel = 'Participant',\n        xlabel = 'Local time of consumption'\n    )\n\n\nswarmplot('data/output/public.pickle', 20,'unique_code', 'date', 'local_time')\n\n\n# export\nclass FoodParser():\n    \"\"\"\n    A class that reads in food loggings from the app. Used as helper function for the function clean_loggings().\n    \"\"\"\n\n    def __init__(self):\n        # self.__version__ = '0.1.9'\n        self.wnl = WordNetLemmatizer()\n        # self.spell = SpellChecker()\n        return\n\n    ################# Read in Annotations #################\n    def process_parser_keys_df(self, parser_keys_df):\n        parser_keys_df = parser_keys_df.query('food_type in [\"f\", \"b\", \"m\", \"w\", \"modifier\", \"general\", \"stopword\", \"selfcare\"]').reset_index(drop = True)\n\n        # 1. all_gram_sets\n        all_gram_set = []\n        for i in range(1, 6):\n            all_gram_set.append(set(parser_keys_df.query('gram_type == ' + str(i)).gram_key.values))\n\n        # 2. food_type_dict\n        food_type_dict = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.food_type.values))\n\n        # 3. food2tag\n        def find_all_tags(s):\n            tags = []\n            for i in range(1, 8):\n                if not isinstance(s['tag' + str(i)], str) and np.isnan(s['tag' + str(i)]):\n                    continue\n                tags.append(s['tag' + str(i)])\n            return tags\n        food2tags = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.apply(find_all_tags, axis = 1)))\n        return all_gram_set, food_type_dict, food2tags\n\n\n    def initialization(self):\n        # 1. read in manually annotated file and bind it to the object\n        \n        # pypi version\n#         parser_keys_df = pd.read_csv(pkg_resources.resource_stream(__name__, \"/data/parser_keys.csv\"))\n#         all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n#         correction_dic = pickle.load(pkg_resources.resource_stream(__name__, \"/data/correction_dic.pickle\"))\n        \n#         # testing version\n        parser_keys_df = pd.read_csv(\"data/parser_keys.csv\")\n        all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n        correction_dic = file_loader(\"data/correction_dic.pickle\")\n        \n        \n        self.all_gram_set = all_gram_set\n        self.food_type_dict = food_type_dict\n        self.correction_dic = correction_dic\n        self.tmp_correction = {}\n\n\n        # 2. Load common stop words and bind it to the object\n        stop_words = stopwords.words('english')\n        # Updated by Katherine August 23rd 2020\n        stop_words.remove('out') # since pre work out is a valid beverage name\n        stop_words.remove('no')\n        stop_words.remove('not')\n        self.stop_words = stop_words\n        return\n\n    ########## Pre-Processing ##########\n    # Function for removing numbers\n    def handle_numbers(self, text):\n        clean_text = text\n        clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n        clean_text = re.sub('[0-9]+', '' , clean_text)\n        clean_text = re.sub('\\s\\s+', ' ', clean_text)\n        return clean_text\n\n    # Function for removing punctuation\n    def drop_punc(self, text):\n        clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n        return clean_text\n\n    # Remove normal stopwords\n    def remove_stop(self, my_text):\n        text_list = my_text.split()\n        return ' '.join([word for word in text_list if word not in self.stop_words])\n\n\n    def pre_processing(self, text):\n        text = text.lower()\n        return self.remove_stop(self.handle_numbers(self.drop_punc(text))).strip()\n\n    ########## Handle Format ##########\n    def handle_front_mixing(self, sent, front_mixed_tokens):\n        cleaned_tokens = []\n        for t in sent.split():\n            if t in front_mixed_tokens:\n                number = re.findall('\\d+[xX]*', t)[0]\n                for t_0 in t.replace(number, number + ' ').split():\n                    cleaned_tokens.append(t_0)\n            else:\n                cleaned_tokens.append(t)\n        return ' '.join(cleaned_tokens)\n\n    def handle_x2(self, sent, times_x_tokens):\n        for t in times_x_tokens:\n            sent = sent.replace(t, ' ' + t.replace(' ', '').lower() + ' ')\n        return ' '.join([s for s in sent.split() if s != '']).strip()\n\n    def clean_format(self, sent):\n        return_sent = sent\n\n        # Problem 1: 'front mixing'\n        front_mixed_tokens = re.findall('\\d+[^\\sxX]+', sent)\n        if len(front_mixed_tokens) != 0:\n            return_sent = self.handle_front_mixing(return_sent, front_mixed_tokens)\n\n        # Problem 2: 'x2', 'X2', 'X 2', 'x 2'\n        times_x_tokens = re.findall('[xX]\\s*?\\d', return_sent)\n        if len(times_x_tokens) != 0:\n            return_sent = self.handle_x2(return_sent, times_x_tokens)\n        return return_sent\n\n    ########## Handle Typo ##########\n    def fix_spelling(self, entry, speller_check = False):\n        result = []\n        for token in entry.split():\n            # Check known corrections\n            if token in self.correction_dic.keys():\n                token_alt = self.wnl.lemmatize(self.correction_dic[token])\n            else:\n                if speller_check and token in self.tmp_correction.keys():\n                    token_alt = self.wnl.lemmatize(self.tmp_correction[token])\n                elif speller_check and token not in self.food_type_dict.keys():\n                    token_alt = self.wnl.lemmatize(token)\n                    self.tmp_correction[token] = token_alt\n                else:\n                    token_alt = self.wnl.lemmatize(token)\n            result.append(token_alt)\n        return ' '.join(result)\n\n    ########### Combine all cleaning ##########\n    def handle_all_cleaning(self, entry):\n        cleaned = self.pre_processing(entry)\n        cleaned = self.clean_format(cleaned)\n        cleaned = self.fix_spelling(cleaned)\n        return cleaned\n\n    ########## Handle Gram Matching ##########\n    def parse_single_gram(self, gram_length, gram_set, gram_lst, sentence_tag):\n        food_lst = []\n        # print(gram_length, gram_lst, sentence_tag)\n        for i in range(len(gram_lst)):\n            if gram_length > 1:\n                curr_word = ' '.join(gram_lst[i])\n            else:\n                curr_word = gram_lst[i]\n            if curr_word in gram_set and sum([t != 'Unknown' for t in sentence_tag[i: i+gram_length]]) == 0:\n                sentence_tag[i: i+gram_length] = str(gram_length)\n                food_lst.append(curr_word)\n        return food_lst\n\n    def parse_single_entry(self, entry, return_sentence_tag = False):\n        # Pre-processing and Cleaning\n        cleaned = self.handle_all_cleaning(entry)\n\n        # Create tokens and n-grams\n        tokens = nltk.word_tokenize(cleaned)\n        bigram = list(nltk.ngrams(tokens, 2)) if len(tokens) > 1 else None\n        trigram = list(nltk.ngrams(tokens, 3)) if len(tokens) > 2 else None\n        quadgram = list(nltk.ngrams(tokens, 4)) if len(tokens) > 3 else None\n        pentagram = list(nltk.ngrams(tokens, 5)) if len(tokens) > 4 else None\n        all_gram_lst = [tokens, bigram, trigram, quadgram, pentagram]\n\n        # Create an array of tags\n        sentence_tag = np.array(['Unknown'] * len(tokens))\n\n        all_food = []\n        food_counter = 0\n        for gram_length in [5, 4, 3, 2, 1]:\n            if len(tokens) < gram_length:\n                continue\n            tmp_food_lst = self.parse_single_gram(gram_length,\n                                                self.all_gram_set[gram_length - 1],\n                                                all_gram_lst[gram_length - 1],\n                                                sentence_tag)\n            all_food += tmp_food_lst\n        if return_sentence_tag:\n            return all_food, sentence_tag\n        else:\n            return all_food\n\n    def parse_food(self, entry, return_sentence_tag = False):\n        result = []\n        unknown_tokens = []\n        num_unknown = 0\n        num_token = 0\n        for w in entry.split(','):\n            all_food, sentence_tag = self.parse_single_entry(w, return_sentence_tag)\n            result += all_food\n            if len(sentence_tag) > 0:\n                num_unknown += sum(np.array(sentence_tag) == 'Unknown')\n                num_token += len(sentence_tag)\n                cleaned = nltk.word_tokenize(self.handle_all_cleaning(w))\n\n                # Return un-catched tokens, groupped into sub-sections\n                tmp_unknown = ''\n                for i in range(len(sentence_tag)):\n                    if sentence_tag[i] == 'Unknown':\n                        # unknown_tokens.append(cleaned[i])\n                        tmp_unknown += (' ' + cleaned[i])\n                        if i == len(sentence_tag) - 1:\n                            unknown_tokens.append(tmp_unknown)\n                    elif tmp_unknown != '':\n                        unknown_tokens.append(tmp_unknown)\n                        tmp_unknown = ''\n\n        if return_sentence_tag:\n            return result, num_token, num_unknown, unknown_tokens\n        else:\n            return result\n\n    def find_food_type(self, food):\n        if food in self.food_type_dict.keys():\n            return self.food_type_dict[food]\n        else:\n            return 'u'\n\n    ################# DataFrame Functions #################\n    def expand_entries(self, df):\n        assert 'desc_text' in df.columns, '[ERROR!] Required a column of \"desc_text\"!!'\n\n        all_entries = []\n        for idx in range(df.shape[0]):\n            curr_row = df.iloc[idx]\n            logging = curr_row.desc_text\n            tmp_dict = dict(curr_row)\n            if ',' in logging:\n                for entry in logging.split(','):\n                    tmp_dict = tmp_dict.copy()\n                    tmp_dict['desc_text'] = entry\n                    all_entries.append(tmp_dict)\n            else:\n                tmp_dict['desc_text'] = logging\n                all_entries.append(tmp_dict)\n\n        return pd.DataFrame(all_entries)"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "treets",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html",
    "href": "data/col_test_data/make_test_toy_data.html",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport public_data_food_analysis_3.columbia as pdfac\nimport public_data_food_analysis_3.core as pdfa"
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-12",
    "href": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-12",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "make data for 2021-05-12",
    "text": "make data for 2021-05-12\n\n# create the log time\noriginal_logtime_512 = ['2021-05-12 02:30:00 +0000', '2021-05-12 02:45:00 +0000', '2021-05-12 04:45:00 +0000']\n# create the log content\ndesc_text_512 = ['milk', 'some medication', 'cookie']\n# create food type\nfood_type_512 = ['b', 'm', 'f']"
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-13",
    "href": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-13",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "make data for 2021-05-13",
    "text": "make data for 2021-05-13\n\n# create the log time\noriginal_logtime_513 = ['2021-05-13 04:30:00 +0000']\n# create the log content\ndesc_text_513 = ['milk']\n# create food type\nfood_type_513 = ['b']"
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-14",
    "href": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-14",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "make data for 2021-05-14",
    "text": "make data for 2021-05-14\n\n# create the log time\noriginal_logtime_514 = ['2021-05-14 02:30:00 +0000','2021-05-14 08:30:00 +0000','2021-05-14 12:30:00 +0000','2021-05-14 19:30:00 +0000']\n# create the log content\ndesc_text_514 = ['coffee','milk', 'lunch', 'dinner']\n# create food type\nfood_type_514 = ['b','b', 'f', 'f']\n\n\n# form the expected table for s-rem study phase\ndf_srem = pd.DataFrame(data = {'original_logtime':original_logtime_512+original_logtime_513+original_logtime_514,\n                              'desc_text':desc_text_512+desc_text_513+desc_text_514,\n                              'food_type':food_type_512+food_type_513+food_type_514,\n                              })\ndf_srem['PID'] = 'yrt1999'\ndf_srem\n\n\n\n\n\n  \n    \n      \n      original_logtime\n      desc_text\n      food_type\n      PID\n    \n  \n  \n    \n      0\n      2021-05-12 02:30:00 +0000\n      milk\n      b\n      yrt1999\n    \n    \n      1\n      2021-05-12 02:45:00 +0000\n      some medication\n      m\n      yrt1999\n    \n    \n      2\n      2021-05-12 04:45:00 +0000\n      cookie\n      f\n      yrt1999\n    \n    \n      3\n      2021-05-13 04:30:00 +0000\n      milk\n      b\n      yrt1999\n    \n    \n      4\n      2021-05-14 02:30:00 +0000\n      coffee\n      b\n      yrt1999\n    \n    \n      5\n      2021-05-14 08:30:00 +0000\n      milk\n      b\n      yrt1999\n    \n    \n      6\n      2021-05-14 12:30:00 +0000\n      lunch\n      f\n      yrt1999\n    \n    \n      7\n      2021-05-14 19:30:00 +0000\n      dinner\n      f\n      yrt1999"
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-15",
    "href": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-15",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "make data for 2021-05-15",
    "text": "make data for 2021-05-15\n\n# create the log time\noriginal_logtime_515 = ['2021-05-15 07:30:00 +0000', '2021-05-15 08:30:00 +0000', '2021-05-15 03:45:00 +0000']\n# create the log content\ndesc_text_515 = ['milk', 'some medication', 'steak']\n# create food type\nfood_type_515 = ['b', 'm', 'f']"
  },
  {
    "objectID": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-16",
    "href": "data/col_test_data/make_test_toy_data.html#make-data-for-2021-05-16",
    "title": "make data for S-REM phase(no restriction on eating window)",
    "section": "make data for 2021-05-16",
    "text": "make data for 2021-05-16\n\n# create the log time\noriginal_logtime_516 = ['2021-05-16 08:30:00 +0000', '2021-05-16 12:30:00 +0000', '2021-05-16 17:30:00 +0000']\n# create the log content\ndesc_text_516 = ['milk', 'some food', 'food']\n# create food type\nfood_type_516 = ['b', 'f', 'f']"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TREETS",
    "section": "",
    "text": "#hide\nfrom treets import *"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "TREETS",
    "section": "Install",
    "text": "Install\npip install treets"
  },
  {
    "objectID": "index.html#example-for-a-quick-data-analysis-on-phased-studies.",
    "href": "index.html#example-for-a-quick-data-analysis-on-phased-studies.",
    "title": "TREETS",
    "section": "Example for a quick data analysis on phased studies.",
    "text": "Example for a quick data analysis on phased studies.\n\nimport treets.core as treets\nimport pandas as pd\nimport nltk\n\nTake a brief look on the food logging dataset and the reference information sheet\n\ntreets.file_loader('data/col_test_data/yrt*').head(2)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      original_logtime\n      desc_text\n      food_type\n      PID\n    \n  \n  \n    \n      0\n      0\n      2021-05-12 02:30:00 +0000\n      Milk\n      b\n      yrt1999\n    \n    \n      1\n      1\n      2021-05-12 02:45:00 +0000\n      Some Medication\n      m\n      yrt1999\n    \n  \n\n\n\n\n\npd.read_excel('data/col_test_data/toy_data_17May2021.xlsx').head(2)\n\n\n\n\n\n  \n    \n      \n      mCC_ID\n      Participant_Study_ID\n      Study Phase\n      Intervention group (TRE or HABIT)\n      Start_Day\n      End_day\n      Eating_Window_Start\n      Eating_Window_End\n    \n  \n  \n    \n      0\n      yrt1999\n      2\n      S-REM\n      TRE\n      2021-05-12\n      2021-05-14\n      00:00:00\n      23:59:00\n    \n    \n      1\n      yrt1999\n      2\n      T3-INT\n      TRE\n      2021-05-15\n      2021-05-18\n      08:00:00\n      18:00:00\n    \n  \n\n\n\n\nCall summarize_data_with_experiment_phases() function to make the table that contains analytic information that we want.\n\ndf = treets.summarize_data_with_experiment_phases(treets.file_loader('data/col_test_data/yrt*')\\\n                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))\n\nParticipant yrt1999 didn't log any food items in the following day(s):\n2021-05-18\nParticipant yrt2000 didn't log any food items in the following day(s):\n2021-05-12\n2021-05-13\n2021-05-14\n2021-05-15\n2021-05-16\n2021-05-17\n2021-05-18\nParticipant yrt1999 have bad logging day(s) in the following day(s):\n2021-05-12\n2021-05-15\nParticipant yrt1999 have bad window day(s) in the following day(s):\n2021-05-15\n2021-05-17\nParticipant yrt1999 have non adherent day(s) in the following day(s):\n2021-05-12\n2021-05-15\n2021-05-17\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      mCC_ID\n      Participant_Study_ID\n      Study Phase\n      Intervention group (TRE or HABIT)\n      Start_Day\n      End_day\n      Eating_Window_Start\n      Eating_Window_End\n      phase_duration\n      caloric_entries_num\n      ...\n      logging_day_counts\n      %_logging_day_counts\n      good_logging_days\n      %_good_logging_days\n      good_window_days\n      %_good_window_days\n      outside_window_days\n      %_outside_window_days\n      adherent_days\n      %_adherent_days\n    \n  \n  \n    \n      0\n      yrt1999\n      2\n      S-REM\n      TRE\n      2021-05-12\n      2021-05-14\n      00:00:00\n      23:59:00\n      3 days\n      7\n      ...\n      3\n      100.0%\n      2.0\n      66.67%\n      3.0\n      100.0%\n      0.0\n      0.0%\n      2.0\n      66.67%\n    \n    \n      1\n      yrt1999\n      2\n      T3-INT\n      TRE\n      2021-05-15\n      2021-05-18\n      08:00:00\n      18:00:00\n      4 days\n      8\n      ...\n      3\n      75.0%\n      2.0\n      50.0%\n      1.0\n      25.0%\n      2.0\n      50.0%\n      1.0\n      25.0%\n    \n    \n      2\n      yrt2000\n      3\n      T3-INT\n      TRE\n      2021-05-12\n      2021-05-14\n      08:00:00\n      16:00:00\n      3 days\n      0\n      ...\n      0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n    \n    \n      3\n      yrt2000\n      3\n      T3-INT\n      TRE\n      2021-05-15\n      2021-05-18\n      08:00:00\n      16:00:00\n      4 days\n      0\n      ...\n      0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n      0.0\n      0.0%\n    \n    \n      4\n      yrt2001\n      4\n      T12-A\n      TRE\n      NaT\n      NaT\n      NaN\n      NaN\n      NaT\n      0\n      ...\n      0\n      nan%\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 32 columns\n\n\n\nLook at resulting statistical information for the first row in the resulting dataset.\n\ndf.iloc[0]\n\nmCC_ID                                           yrt1999\nParticipant_Study_ID                                   2\nStudy Phase                                        S-REM\nIntervention group (TRE or HABIT)                    TRE\nStart_Day                            2021-05-12 00:00:00\nEnd_day                              2021-05-14 00:00:00\nEating_Window_Start                             00:00:00\nEating_Window_End                               23:59:00\nphase_duration                           3 days 00:00:00\ncaloric_entries_num                                    7\nmedication_num                                         0\nwater_num                                              0\nfirst_cal_avg                                   5.916667\nfirst_cal_std                                   2.240722\nlast_cal_avg                                   19.666667\nlast_cal_std                                   12.933323\nmean_daily_eating_window                           13.75\nstd_daily_eating_window                        11.986972\nearliest_entry                                       4.5\n2.5%                                              4.5375\n97.5%                                            27.5625\nduration mid 95%                                  23.025\nlogging_day_counts                                     3\n%_logging_day_counts                              100.0%\ngood_logging_days                                    2.0\n%_good_logging_days                               66.67%\ngood_window_days                                     3.0\n%_good_window_days                                100.0%\noutside_window_days                                  0.0\n%_outside_window_days                               0.0%\nadherent_days                                        2.0\n%_adherent_days                                   66.67%\nName: 0, dtype: object"
  },
  {
    "objectID": "index.html#example-for-a-quick-data-analysis-on-non-phased-studies.",
    "href": "index.html#example-for-a-quick-data-analysis-on-non-phased-studies.",
    "title": "TREETS",
    "section": "Example for a quick data analysis on non-phased studies.",
    "text": "Example for a quick data analysis on non-phased studies.\ntake a look at the original dataset\n\ndf = treets.file_loader('data/test_food_details.csv')\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      ID\n      unique_code\n      research_info_id\n      desc_text\n      food_type\n      original_logtime\n      foodimage_file_name\n    \n  \n  \n    \n      0\n      1340147\n      7572733\n      alqt14018795225\n      150\n      Water\n      w\n      2017-12-08 17:30:00+00:00\n      NaN\n    \n    \n      1\n      1340148\n      411111\n      alqt14018795225\n      150\n      Coffee White\n      b\n      2017-12-09 00:01:00+00:00\n      NaN\n    \n  \n\n\n\n\npreprocess the data to create features we might need in the furthur analysis such as float time, week count since the first week, etc.\n\ndf = treets.load_public_data(df,'unique_code', 'original_logtime',4)\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      ID\n      unique_code\n      research_info_id\n      desc_text\n      food_type\n      original_logtime\n      date\n      float_time\n      time\n      week_from_start\n      year\n    \n  \n  \n    \n      0\n      1340147\n      7572733\n      alqt14018795225\n      150\n      Water\n      w\n      2017-12-08 17:30:00+00:00\n      2017-12-08\n      17.500000\n      17:30:00\n      1\n      2017\n    \n    \n      1\n      1340148\n      411111\n      alqt14018795225\n      150\n      Coffee White\n      b\n      2017-12-09 00:01:00+00:00\n      2017-12-08\n      24.016667\n      00:01:00\n      1\n      2017\n    \n  \n\n\n\n\nCall summarize_data() function to make the table that contains analytic information that we want.¶\n\ndf = treets.summarize_data(df, 'unique_code', 'float_time', 'date')\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      unique_code\n      num_days\n      num_total_items\n      num_f_n_b\n      num_medications\n      num_water\n      first_cal_avg\n      first_cal_std\n      last_cal_avg\n      last_cal_std\n      eating_win_avg\n      eating_win_std\n      good_logging_count\n      first_cal variation (90%-10%)\n      last_cal variation (90%-10%)\n      2.5%\n      95%\n      duration mid 95%\n    \n  \n  \n    \n      0\n      alqt1148284857\n      13\n      149\n      96\n      19\n      34\n      7.821795\n      6.710717\n      23.485897\n      4.869082\n      15.664103\n      8.231201\n      146\n      2.966667\n      9.666667\n      4.535000\n      26.813333\n      22.636667\n    \n    \n      1\n      alqt14018795225\n      64\n      488\n      484\n      3\n      1\n      7.525781\n      5.434563\n      25.858594\n      3.374839\n      18.332813\n      6.603913\n      484\n      13.450000\n      3.100000\n      4.183333\n      27.438333\n      23.416667\n    \n  \n\n\n\n\nLook at resulting statistical information for the first row in the resulting dataset.\n\ndf.iloc[0]\n\nunique_code                      alqt1148284857\nnum_days                                     13\nnum_total_items                             149\nnum_f_n_b                                    96\nnum_medications                              19\nnum_water                                    34\nfirst_cal_avg                          7.821795\nfirst_cal_std                          6.710717\nlast_cal_avg                          23.485897\nlast_cal_std                           4.869082\neating_win_avg                        15.664103\neating_win_std                         8.231201\ngood_logging_count                          146\nfirst_cal variation (90%-10%)          2.966667\nlast_cal variation (90%-10%)           9.666667\n2.5%                                      4.535\n95%                                   26.813333\nduration mid 95%                      22.636667\nName: 0, dtype: object"
  },
  {
    "objectID": "index.html#clean-text-in-food-loggings",
    "href": "index.html#clean-text-in-food-loggings",
    "title": "TREETS",
    "section": "Clean text in food loggings",
    "text": "Clean text in food loggings\n\n# import the dataset\ndf = treets.file_loader('data/col_test_data/yrt*')\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      original_logtime\n      desc_text\n      food_type\n      PID\n    \n  \n  \n    \n      0\n      0\n      2021-05-12 02:30:00 +0000\n      Milk\n      b\n      yrt1999\n    \n    \n      1\n      1\n      2021-05-12 02:45:00 +0000\n      Some Medication\n      m\n      yrt1999\n    \n    \n      2\n      2\n      2021-05-12 04:45:00 +0000\n      bacon egg\n      f\n      yrt1999\n    \n  \n\n\n\n\n\ntreets.clean_loggings(df, 'desc_text', 'PID').head(3)\n\n\n\n\n\n  \n    \n      \n      PID\n      desc_text\n      cleaned\n    \n  \n  \n    \n      0\n      yrt1999\n      Milk\n      [milk]\n    \n    \n      1\n      yrt1999\n      Some Medication\n      [medication]\n    \n    \n      2\n      yrt1999\n      bacon egg\n      [bacon, egg]\n    \n  \n\n\n\n\nWe can see that words are lower cased, modifiers are removed(2nd row) and items are split into individual items(third row)."
  },
  {
    "objectID": "index.html#visualizations",
    "href": "index.html#visualizations",
    "title": "TREETS",
    "section": "Visualizations",
    "text": "Visualizations\n\n# import the dataset\ndf = treets.file_loader('data/test_food_details.csv')\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      ID\n      unique_code\n      research_info_id\n      desc_text\n      food_type\n      original_logtime\n      foodimage_file_name\n    \n  \n  \n    \n      0\n      1340147\n      7572733\n      alqt14018795225\n      150\n      Water\n      w\n      2017-12-08 17:30:00+00:00\n      NaN\n    \n    \n      1\n      1340148\n      411111\n      alqt14018795225\n      150\n      Coffee White\n      b\n      2017-12-09 00:01:00+00:00\n      NaN\n    \n  \n\n\n\n\nmake a scatter plot for people’s breakfast time\n\n# create required features for function first_cal_mean_with_error_bar()\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\ndf['local_time'] = treets.find_float_time(df, 'original_logtime')\ndf['date'] = treets.find_date(df, 'original_logtime')\n\n# call the function\ntreets.first_cal_mean_with_error_bar(df,'unique_code', 'date', 'local_time')\n\n\n\n\nUse swarmplot to visualize each person’s eating time distribution.\n\ntreets.swarmplot(df, 50, 'unique_code', 'date', 'local_time')"
  }
]