[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TREETS",
    "section": "",
    "text": "pip install treets\n\nimport treets.core as treets\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "TREETS",
    "section": "",
    "text": "pip install treets\n\nimport treets.core as treets\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#data-required-and-used-by-treets",
    "href": "index.html#data-required-and-used-by-treets",
    "title": "TREETS",
    "section": "Data Required and Used By TREETS",
    "text": "Data Required and Used By TREETS\n\nCaloric Entry Logging\nLogging data typically comes from the myCircadianClock application. TREETS functionality relies on columns that are found in these datasets, and therefore it is important that logging data fed into TREETS uses this format. Column names, such as ‘food_type’ and ‘desc_text’, are expected to match what is created by data from this app.\n\nA single logging entry consists of all logged food/beverage/medication items logged by a single participant at a specific moment. The app allows users to retroactively log items, and instructs them to label the entire log with a single ‘food type’ that best fits the contents of the log.\n\ntreets.file_loader('data/col_test_data/yrt*').head(2)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nMilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nSome Medication\nm\nyrt1999\n\n\n\n\n\n\n\n\n\n‘Long Form’ Participant Information Sheets\nTo ensure that analysis is properly matched to specific participants and/or study phases, it is crucial that all particpant data used with TREETS follows the exact format specified in this HOWTO document.\n\nParticipant information data should follow the HOWTO document that accompanies TREETS. A short recap of some of the most important points is provided here:\n\nColumns should be in the exact specified order (exact column names are less important than the column order).\nDate columns must be provided in ISO 8601\nParticipants with rows that are missing data are not included in analysis. See the HOWTO document for more information.\nStudy Phase and Intervention Group Names should be consistent for the same group (e.g. TRE and tre would be treated as two separate study groups)\nThe number of intervention groups and study phases is not important, so long as other information is sensible (e.g. starting and ending periods for these phases do not accidentally overlap)\n\n\npd.read_excel('data/col_test_data/toy_data_17May2021.xlsx').head(2)\n\n\n\n\n\n\n\n\n\nmCC_ID\nParticipant_Study_ID\nStudy Phase\nIntervention group (TRE or HABIT)\nStart_Day\nEnd_day\nEating_Window_Start\nEating_Window_End\n\n\n\n\n0\nyrt1999\n2\nS-REM\nTRE\n2021-05-12 00:00:00\n2021-05-14 00:00:00\n00:00:00\n23:59:00\n\n\n1\nyrt1999\n2\nT3-INT\nTRE\n2021-05-15 00:00:00\n2021-05-18 00:00:00\n08:00:00\n18:00:00",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#single-armtreatment-group-studies",
    "href": "index.html#single-armtreatment-group-studies",
    "title": "TREETS",
    "section": "Single Arm/Treatment Group Studies",
    "text": "Single Arm/Treatment Group Studies\nTREETs assumes studies have multiple treatment groups. For studies where there is a single treatment group, please ensure that all participants still have a labeled group name of some kind within the required Long Form spreadsheet (missing values or values marked as ‘N/A’ are ignored).\n\nexample = pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx').rename(columns={'Intervention group (TRE or HABIT)': 'Intervention Group'})\nexample['Intervention Group'] = np.nan\nexample\n\n\n\n\n\n\n\n\n\nmCC_ID\nParticipant_Study_ID\nStudy Phase\nIntervention Group\nStart_Day\nEnd_day\nEating_Window_Start\nEating_Window_End\n\n\n\n\n0\nyrt1999\n2\nS-REM\nNaN\n2021-05-12 00:00:00\n2021-05-14 00:00:00\n00:00:00\n23:59:00\n\n\n1\nyrt1999\n2\nT3-INT\nNaN\n2021-05-15 00:00:00\n2021-05-18 00:00:00\n08:00:00\n18:00:00\n\n\n2\nyrt2000\n3\nT3-INT\nNaN\n2021-05-12 00:00:00\n2021-05-14 00:00:00\n08:00:00\n16:00:00\n\n\n3\nyrt2000\n3\nT3-INT\nNaN\n2021-05-15 00:00:00\n2021-05-18 00:00:00\n08:00:00\n16:00:00\n\n\n4\nyrt2001\n4\nT12-A\nNaN\nNaT\nNaT\nNaN\nNaN\n\n\n\n\n\n\n\n\nIn this example, all participants are from the same Intervention Group. Missing, null, or N/A values are typically ignored by TREETs under the assumption that all variables with existing values are critical to analysis. If your study has only a single arm, please still note the Intervention Group as a value of some kind (such as ‘None’) as shown below.\n\nexample['Intervention Group'] = 'None'\nexample\n\n\n\n\n\n\n\n\n\nmCC_ID\nParticipant_Study_ID\nStudy Phase\nIntervention Group\nStart_Day\nEnd_day\nEating_Window_Start\nEating_Window_End\n\n\n\n\n0\nyrt1999\n2\nS-REM\nNone\n2021-05-12 00:00:00\n2021-05-14 00:00:00\n00:00:00\n23:59:00\n\n\n1\nyrt1999\n2\nT3-INT\nNone\n2021-05-15 00:00:00\n2021-05-18 00:00:00\n08:00:00\n18:00:00\n\n\n2\nyrt2000\n3\nT3-INT\nNone\n2021-05-12 00:00:00\n2021-05-14 00:00:00\n08:00:00\n16:00:00\n\n\n3\nyrt2000\n3\nT3-INT\nNone\n2021-05-15 00:00:00\n2021-05-18 00:00:00\n08:00:00\n16:00:00\n\n\n4\nyrt2001\n4\nT12-A\nNone\nNaT\nNaT\nNaN\nNaN",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#phased-study-example",
    "href": "index.html#phased-study-example",
    "title": "TREETS",
    "section": "Phased Study Example",
    "text": "Phased Study Example\nTREETS comes with a summary analysis function built specifically for phased studies. By default, it prints a report outlining dates where participants were missing logs or where non-adherent to their current assigned eating window for that date.\n\ndf = treets.summarize_data_with_experiment_phases(treets.file_loader('data/col_test_data/yrt*')\\\n                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))\n\nParticipant yrt1999 didn't log any food items in the following day(s):\n2021-05-18\nParticipant yrt2000 didn't log any food items in the following day(s):\n2021-05-12\n2021-05-13\n2021-05-14\n2021-05-15\n2021-05-16\n2021-05-17\n2021-05-18\nParticipant yrt1999 have bad logging day(s) in the following day(s):\n2021-05-12\n2021-05-15\nParticipant yrt1999 have bad window day(s) in the following day(s):\n2021-05-15\nParticipant yrt1999 have non adherent day(s) in the following day(s):\n2021-05-12\n2021-05-15\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nmCC_ID\nParticipant_Study_ID\nStudy Phase\nIntervention group (TRE or HABIT)\nStart_Day\nEnd_day\nEating_Window_Start\nEating_Window_End\nphase_duration\ncaloric_entries_num\n...\nlogging_day_counts\n%_logging_day_counts\ngood_logging_days\n%_good_logging_days\ngood_window_days\n%_good_window_days\noutside_window_days\n%_outside_window_days\nadherent_days\n%_adherent_days\n\n\n\n\n0\nyrt1999\n2\nS-REM\nTRE\n2021-05-12\n2021-05-14\n00:00:00\n23:59:00\n3 days 00:00:00\n7\n...\n3\n100.0\n0.0\n0.0\n1.0\n33.33\n0.0\n0.0\n0.0\n0.0\n\n\n1\nyrt1999\n2\nT3-INT\nTRE\n2021-05-15\n2021-05-18\n08:00:00\n18:00:00\n4 days 00:00:00\n8\n...\n3\n75.0\n0.0\n0.0\n0.0\n0.00\n1.0\n25.0\n0.0\n0.0\n\n\n2\nyrt2000\n3\nT3-INT\nTRE\n2021-05-12\n2021-05-14\n08:00:00\n16:00:00\n3 days 00:00:00\n0\n...\n0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n\n\n3\nyrt2000\n3\nT3-INT\nTRE\n2021-05-15\n2021-05-18\n08:00:00\n16:00:00\n4 days 00:00:00\n0\n...\n0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n\n\n4\nyrt2001\n4\nT12-A\nTRE\nNaT\nNaT\nNaN\nNaN\nNaT\n0\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 36 columns\n\n\n\n\nEach row in the resulting dataframe represents summary data for a participant during a specific study phase and assigned eating window combination. If a participant has had more than one assigned eating window during a study phase, each eating window will receive a summary row that describes the data for the duration of the assignment.\nAdditionally, the majority of statistical information here is contingent on the participant having enough data to generate valid statistics. Daily averages and standard deviation measurements require participants to have a minimum number of logs on each day and a minimum number of days logged (e.g. standard deviation of first caloric entry would be incalculable if the participant only has a single caloric entry for that study phase in the dataset). Similarly, eating window calculations and percentiles are only available if a participant has a valid eating window (e.g. days with only a single caloric log do not produce a valid eating window).\n\ndf.iloc[0]\n\nmCC_ID                                       yrt1999\nParticipant_Study_ID                               2\nStudy Phase                                    S-REM\nIntervention group (TRE or HABIT)                TRE\nStart_Day                                 2021-05-12\nEnd_day                                   2021-05-14\nEating_Window_Start                         00:00:00\nEating_Window_End                           23:59:00\nphase_duration                       3 days 00:00:00\ncaloric_entries_num                                7\nmedication_num                                     0\nwater_num                                          0\nfirst_cal_avg                               5.916667\nfirst_cal_std                               2.240722\nlast_cal_avg                               19.666667\nlast_cal_std                               12.933323\nmean_daily_eating_window                       13.75\nstd_daily_eating_window                    11.986972\nearliest_entry                                   4.5\nmean_daily_eating_occasions                 2.333333\nstd_daily_eating_occasions                  1.527525\nmean_daily_eating_midpoint                 12.083333\nstd_daily_eating_midpoint                   6.355772\n2.5%                                            4.75\n97.5%                                           4.75\nduration mid 95%                                 0.0\nlogging_day_counts                                 3\n%_logging_day_counts                           100.0\ngood_logging_days                                0.0\n%_good_logging_days                              0.0\ngood_window_days                                 1.0\n%_good_window_days                             33.33\noutside_window_days                              0.0\n%_outside_window_days                            0.0\nadherent_days                                    0.0\n%_adherent_days                                  0.0\nName: 0, dtype: object",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#non-phased-summaries",
    "href": "index.html#non-phased-summaries",
    "title": "TREETS",
    "section": "Non-Phased Summaries",
    "text": "Non-Phased Summaries\nTREETs also houses a function made to summarize participant data for an entire study.\n\ndf = treets.file_loader('data/test_food_details.csv')\ndf.head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\nfoodimage_file_name\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\nNaN\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\nNaN\n\n\n\n\n\n\n\n\nTREETS makes use of more specific time-based identifiers to help group data. For their specific use cases, please refer to the API.\n\ndf = treets.load_food_data(df, h = 4)\ndf.head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\n\n\n\n\n\n\n\n\n\ndf = treets.summarize_data(df)\ndf.head(2)\n\n\n\n\n\n\n\n\n\nunique_code\nnum_days\nnum_total_items\nnum_f_n_b\nnum_medications\nnum_water\nfirst_cal_avg\nfirst_cal_std\nlast_cal_avg\nlast_cal_std\neating_win_avg\neating_win_std\ngood_logging_count\nfirst_cal variation (90%-10%)\nlast_cal variation (90%-10%)\n2.5%\n95%\nduration mid 95%\n\n\n\n\n0\nalqt1148284857\n13\n149\n96\n19\n34\n7.821795\n6.710717\n23.485897\n4.869082\n15.664103\n8.231201\n146\n2.750\n10.050\n4.593750\n27.129167\n22.589583\n\n\n1\nalqt14018795225\n64\n488\n484\n3\n1\n7.525781\n5.434563\n25.858594\n3.374839\n18.332813\n6.603913\n484\n13.195\n3.105\n4.183333\n27.445000\n23.416667\n\n\n\n\n\n\n\n\n\ndf.iloc[0]\n\nunique_code                      alqt1148284857\nnum_days                                     13\nnum_total_items                             149\nnum_f_n_b                                    96\nnum_medications                              19\nnum_water                                    34\nfirst_cal_avg                          7.821795\nfirst_cal_std                          6.710717\nlast_cal_avg                          23.485897\nlast_cal_std                           4.869082\neating_win_avg                        15.664103\neating_win_std                         8.231201\ngood_logging_count                          146\nfirst_cal variation (90%-10%)              2.75\nlast_cal variation (90%-10%)              10.05\n2.5%                                    4.59375\n95%                                   27.129167\nduration mid 95%                      22.589583\nName: 0, dtype: object",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#log-cleaning-and-parsing",
    "href": "index.html#log-cleaning-and-parsing",
    "title": "TREETS",
    "section": "Log Cleaning and Parsing",
    "text": "Log Cleaning and Parsing\nAn additional feature that comes with TREETS is ‘food log’ cleaning and parsing. Using a dictionary of common mispellings (and their corrections), TREETS attempts to spell correct typos made in food logs. Food logs are then n-gram matched through a dictionary of n-gram food and medication related phrases to create a list of individual items.\n\n# import the dataset\ndf = treets.file_loader('data/col_test_data/yrt*')\ndf.head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nMilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nSome Medication\nm\nyrt1999\n\n\n2\n2021-05-12 04:45:00 +0000\nbacon egg\nf\nyrt1999\n\n\n\n\n\n\n\n\n\ntreets.clean_loggings(df).head(3)\n\n\n\n\n\n\n\n\n\ndesc_text\ncleaned\n\n\n\n\n0\nMilk\n[milk]\n\n\n1\nSome Medication\n[medication]\n\n\n2\nbacon egg\n[bacon, egg]",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "index.html#visualizations",
    "href": "index.html#visualizations",
    "title": "TREETS",
    "section": "Visualizations",
    "text": "Visualizations\nTREETS includes a small suite of visualizations. Some examples are included below.\n\n# import the dataset\ndf = treets.file_loader('data/test_food_details.csv')\ndf.head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\nfoodimage_file_name\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\nNaN\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\nNaN\n\n\n\n\n\n\n\n\nA scatter plot for people’s breakfast time\n\n# create required features for function first_cal_mean_with_error_bar()\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\ndf['local_time'] = treets.find_float_time(df, h = 4)\ndf['date'] = treets.find_date(df, h = 4)\n\n# call the function\ntreets.first_cal_mean_with_error_bar(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse swarmplot to visualize each person’s eating time distribution.\n\ntreets.swarmplot(df, max_loggings = 50)",
    "crumbs": [
      "TREETS"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "",
    "text": "These functions primarily serve as parts of other functions, but are provided here for utility.\n\nsource\n\n\n\n file_loader (data_source:Union[str,pandas.core.frame.DataFrame])\n\nFlexible file loader able to read a single file path or folder path. Accepts .csv and .json file format loading.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame.Existing dataframes are read as is.\n\n\nReturns\npd.DataFrame\nA single dataframe consisting of all data matching the provided file or folder path.\n\n\n\nProviding the file loader with a specific file path outputs a single Pandas dataframe generated from that data source.\n\nfile_loader(\"data/col_test_data/toy_data_2000.csv\").head(2)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nmilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nsome medication\nm\nyrt1999\n\n\n\n\n\n\n\n\nThe file loader can also accept string patterns to read in multiple files at once. Providing a patterened path such as yrt*_food_data*.csv would load all data matching this pattern.\n\nfile_loader('data/col_test_data/yrt*_food_data*.csv').head(2)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nMilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nSome Medication\nm\nyrt1999\n\n\n\n\n\n\n\n\nIt can also handle reading mixed file types. The below dataframe consists of data read from all .json and .csv files in the data/output/ folder.\n\nfile_loader('data/output/*').head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nlocal_time\ntime\nweek_from_start\nyear\ncleaned\nday_count\n\n\n\n\n0\n7572733.0\nalqt14018795225\n150.0\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1.0\n2017.0\nNaN\nNaN\n\n\n1\n411111.0\nalqt14018795225\n150.0\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1.0\n2017.0\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_date (data_source:Union[str,pandas.core.frame.DataFrame], h:int=4,\n            date_col:int=5)\n\nExtracts date from a datetime column after shifting datetime by ‘h’ hours. A day starts ‘h’ hours early if ‘h’ is negative, or ‘h’ hours later if ‘h’ is positive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n4\nNumber of hours to shift the definition for ‘date’ by. h = 4 would shift days so that time membershipto each date starts at 4:00 AM and ends at 3:59:59 AM the next calendar day.\n\n\ndate_col\nint\n5\nColumn number for existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.Series\n\nSeries of dates in ISO 8601 format.\n\n\n\nBy default, find_date expects log dates for studies to begin at 4:00 AM. To use regular calendar dates, remember to set h = 0.\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\ndf['date'] = find_date(df, h = 0)\ndf[['original_logtime', 'date']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n\n\n\n\n\n\n\n\nIn this example, with log dates starting at the default value of 4 (4:00 AM), we see that two logs from very early morning on 2017-12-09 are counted as being logged on 2017-12-08 instead.\n\ndf['date'] = find_date(df, h = 4)\ndf[['original_logtime', 'date']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-08\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-08\n\n\n\n\n\n\n\n\nSimilarly, in an example where we start log days four hours earlier, the last two rows have dates that are shifted so their log date is one day later than their exact calendar datetime.\n\ndf['date'] = find_date(df, h = -4)\ndf[['original_logtime', 'date']].head(5)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n\n\n3\n2018-02-22 21:52:00+00:00\n2018-02-23\n\n\n4\n2018-02-22 22:53:00+00:00\n2018-02-23\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_float_time (data_source:Union[str,pandas.core.frame.DataFrame],\n                  h:int=4, date_col:int=5)\n\nExtracts time from a datetime column after shifting datetime by ‘h’ hours. A day starts ‘h’ hours early if ‘h’ is negative, or ‘h’ hours later if ‘h’ is positive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n4\nNumber of hours to shift the definition for ‘time’ by. h = 4 would allow float representations of timebetween 4 (inclusive) and 28 (exclusive), representing time that goes from 4:00 AM to 3:59:59 AM the nextcalendar day. NOTE: h value for this function should match the h value used for generating dates.\n\n\ndate_col\nint\n5\nColumn number for existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.Series\n\nSeries of times in float format (e.g. 4:36 AM -&gt; 4.6).\n\n\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\nBy default, find_float_time expects studies to begin at 4:00 AM. To preserve regular calendar dates use h = 0.\n\ndf['float_time'] = find_float_time(df, h = 0)\ndf[['original_logtime', 'float_time']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n0.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n0.966667\n\n\n\n\n\n\n\n\nUsing positive values for h for both date and float time functions changes date ownership for a row based on its original logtime. Float time should be shifted by the same h value as date membership so that times belonging to a different calendar date can be differentiated when necessary (e.g. 2:00 AM –&gt; 2.0, whereas 2:00 AM the next calendar day –&gt; 26.0, for cases where these rows should still be grouped together on the same logging date).\n\ndf['float_time'] = find_float_time(df, h = 4)\ndf['date'] = find_date(df, h = 4)\ndf[['original_logtime','date', 'float_time']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-08\n24.966667\n\n\n\n\n\n\n\n\nIn rare cases, it may be valuable to shift date and time by negative values. In this example where a log date starts at 8:00 PM the previous calendar day and ends at 8:00 PM the current calendar day, note that the last two rows have negative float times and their date membership is shifted one date further than their original calendar datetime.\n\ndf['float_time'] = find_float_time(df, h = -4)\ndf['date'] = find_date(df, h = -4)\ndf[['original_logtime','date', 'float_time']].head(5)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n0.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n0.966667\n\n\n3\n2018-02-22 21:52:00+00:00\n2018-02-23\n-2.133333\n\n\n4\n2018-02-22 22:53:00+00:00\n2018-02-23\n-1.116667\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n week_from_start (data_source:Union[str,pandas.core.frame.DataFrame],\n                  identifier:int=1)\n\nCalculates the number of weeks between each logging entry and the first logging entry for each participant. A ‘date’ column must exist in the provided data source. Using the provided find_date function is recommended.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\nReturns\nnp.array\n\nArray of weeks passed from log date to the minimum date for each participant.\n\n\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\nUsing find_date to ensure that a date column exists in the data source is recommended. A column labeled ‘date’ is a requirement of this function.\n\ndf['date'] = find_date(df)\ndf['week_from_start'] = week_from_start(df)\ndf[['unique_code','original_logtime','week_from_start']][2:4]\n\n\n\n\n\n\n\n\n\nunique_code\noriginal_logtime\nweek_from_start\n\n\n\n\n2\nalqt14018795225\n2017-12-09 00:58:00+00:00\n1\n\n\n3\nalqt14018795225\n2018-02-22 21:52:00+00:00\n11\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_phase_duration (df:pandas.core.frame.DataFrame)\n\nCalculates the duration (in days) of the study phase for each row.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nParticipant information dataframe with columns for start and ending date for that row’s study phase.The expected column numbers for starting and ending dates are outlined in the HOWTO document that accompanies TREETS.\n\n\nReturns\npd.DataFrame\nDataframe with an additional column describing study phase duration.\n\n\n\n\nfind_phase_duration(pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))[['phase_duration']]\n\n\n\n\n\n\n\n\n\nphase_duration\n\n\n\n\n0\n3 days\n\n\n1\n4 days\n\n\n2\n3 days\n\n\n3\n4 days\n\n\n4\nNaT\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n load_food_data (data_source:Union[str,pandas.core.frame.DataFrame],\n                 h:int, identifier:int=1, datetime_col:int=5)\n\nLoads and processes existing logging data, adding specific datetime information in formats more suitable for TREETS functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n\nNumber of hours to shift the definition of ‘date’ by. h = 4 would indicate that a log date begins at4:00 AM and ends the following calendar day at 3:59:59 AM. Float representations of time would thereforego from 4.0 (inclusive) to 28.0 (exclusive) to represent ‘date’ membership for days shifted from theiroriginal calendar date.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndatetime_col\nint\n5\nColumn number for an existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.DataFrame\n\nDataframe with additional date, float time, and week from start columns.\n\n\n\n\nload_food_data('data/test_food_details.csv', h = 4).head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n in_good_logging_day (data_source:Union[str,pandas.core.frame.DataFrame],\n                      min_log_num:int=2, min_separation:int=5,\n                      identifier:int=1, date_col:int=6, time_col:int=7)\n\nCalculates if each log is considered to be within a ‘good logging day’. A log day is considered ‘good’ if there are at least the minimum number of required logs, with a minimum specified hour separation between the first and last log for that log date. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n5\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nnp.array\n\nBoolean array describing whether each log is a ‘good’ logging day.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ndf['in_good_logging_day'] = in_good_logging_day(df)\ndf.head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\nin_good_logging_day\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\nTrue\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\nTrue\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n FoodParser ()\n\nFood parser handles taking unprocessed food log entries and adding relevant information from a pre-made dictionary. This includes matching unprocessed terms to their likely matches, adding food type and other identifying information.\n\nsource\n\n\n\n\n clean_loggings (data_source:Union[str,pandas.core.frame.DataFrame],\n                 identifier:int=1)\n\nCleans and attempts typo correction for all logging text entries.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\nReturns\npd.DataFrame\n\nDataframe with an additional column containing cleaned and typo corrected item entries.\n\n\n\nText descriptions of food items are cleaned using a built-in dictionary of common typos and corrections for each phrase. Phrases are then matched using a dictionary of known n-gram item names. The resulting item(s) are provided as a list.\n\nclean_loggings('data/output/public.json').head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\ncleaned\n\n\n\n\n0\nalqt14018795225\nWater\n[water]\n\n\n1\nalqt14018795225\nCoffee White\n[coffee, white]\n\n\n2\nalqt14018795225\nSalad\n[salad]\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_types (data_source:Union[str,pandas.core.frame.DataFrame],\n            food_type:Union[str,list])\n\nFilters logs for only logs of specified type(s).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder pathswith files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. A column ‘food_type’ is required to be within the data.\n\n\nfood_type\nstr | list\nA single food type, or list of food types. Valid types are ‘f’: food, ‘b’: beverage, ‘w’: water,and ‘m’: medication.\n\n\nReturns\npd.DataFrame\nDataframe filtered for only logs of specific type(s). \n\n\n\nType selection accepts multiple types at once as a list of entry types. All types chosen must be valid.\nAvailable food types include:\n\n‘f’: Food\n‘b’: Beverage\n‘w’: Water\n‘m’: Medication\n\nFlavored water beverages such as La Croix are counted as ‘water’ and not as ‘beverage’.\n\nget_types('data/output/baseline.json',['w', 'f'])[['unique_code','desc_text','food_type']].head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\nfood_type\n\n\n\n\n0\nalqt14018795225\nWater\nw\n\n\n2\nalqt14018795225\nSalad\nf\n\n\n3\nalqt78896444285\nWater\nw\n\n\n\n\n\n\n\n\nFiltering for a single type is also possible.\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nget_types(df, 'm')[['unique_code','desc_text','food_type']].head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\nfood_type\n\n\n\n\n323\nalqt14018795225\nCaffeine\nm\n\n\n361\nalqt14018795225\nCaffeine\nm\n\n\n420\nalqt14018795225\nCaffeine\nm\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n count_caloric_entries (df:pandas.core.frame.DataFrame)\n\nCounts the number of food (‘f’) and beverage (‘b’) loggings.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nDataframe of food logging data.\n\n\nReturns\nint\nNumber of caloric (food or beverage) entries found.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ncount_caloric_entries(df)\n\n4603\n\n\n\nsource\n\n\n\n\n mean_daily_eating_duration (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates mean daily eating window by taking the average of each day’s eating window. An eating window is defined as the duration of time between first and last caloric (food or beverage) intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average daily eating window duration.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_duration(df)\n\n14.038679245283017\n\n\n\nsource\n\n\n\n\n std_daily_eating_duration (df:pandas.core.frame.DataFrame,\n                            date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the daily eating window. An eating window is defined as the duration of time between first and last caloric (food or beverage) intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of daily eating window duration.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_duration(df)\n\n7.018679942775867\n\n\n\nsource\n\n\n\n\n earliest_entry (df:pandas.core.frame.DataFrame, time_col:int=7)\n\nCalculates the earliest recorded caloric (food or beverage) entry. It is recommended that you use find_float_time to generate necessary the time column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the earliest logtime on any date.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nearliest_entry(df)\n\n4.0\n\n\n\nsource\n\n\n\n\n mean_first_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                 time_col:int=7)\n\nCalculates the average time of first caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average first caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_first_cal(df)\n\n9.22680817610063\n\n\n\n# find the average mean first cal time for each participant\ndf.groupby(['unique_code']).agg(mean_first_cal, date_col = 6, time_col = 7).iloc[:,0]\n\nunique_code\nalqt1148284857      7.315278\nalqt14018795225     7.635938\nalqt16675467779     6.153904\nalqt21525720972    13.211957\nalqt45631586569    15.056295\nalqt5833085442     12.551515\nalqt62359040167     7.252137\nalqt6695047873      7.573077\nalqt78896444285     6.347510\nalqt8668165687      9.702555\nName: ID, dtype: float64\n\n\n\nsource\n\n\n\n\n std_first_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                time_col:int=7)\n\nCalculates the standard deviation for time of first caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of first caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_first_cal(df)\n\n4.591417471559444\n\n\n\nsource\n\n\n\n\n mean_last_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                time_col:int=7)\n\nCalculates the average time of last caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average last caloric entry time.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_last_cal(df)\n\n23.265487421383646\n\n\n\nsource\n\n\n\n\n std_last_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n               time_col:int=7)\n\nCalculates the standard deviation for time of last caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of last caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_last_cal(df, 'date', 'float_time')\n\n4.359435007580498\n\n\n\nsource\n\n\n\n\n mean_daily_eating_occasions (df:pandas.core.frame.DataFrame,\n                              date_col:int=6, time_col:int=7)\n\nCalculates the average number of daily eating occasions. An eating occasion is a single caloric (food or beverage) log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nAverage number of daily eating occasions. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_occasions(df, 'date', 'float_time')\n\n6.8915094339622645\n\n\n\nsource\n\n\n\n\n std_daily_eating_occasions (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the number of daily eating occasions. An eating occasion is a single caloric (food or beverage) log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nStandard deviation of the number of daily eating occasions. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_occasions(df, 'date', 'float_time')\n\n4.44839423402741\n\n\n\nsource\n\n\n\n\n mean_daily_eating_midpoint (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates the average daily midpoint eating occasion time. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nFloat representation of the average daily midpoint eating occasion time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_midpoint(df, 'date', 'float_time')\n\n16.536425576519914\n\n\n\nsource\n\n\n\n\n std_daily_eating_midpoint (df:pandas.core.frame.DataFrame,\n                            date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the daily midpoint eating occasion time. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nFloat representation of the standard deviation of the daily midpoint eating occasion time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_midpoint(df, 'date', 'float_time')\n\n4.107072970435106\n\n\n\nsource\n\n\n\n\n logging_day_counts (df:pandas.core.frame.DataFrame)\n\nCalculates the number of days that contain any logs. It is recommended that you use find_date to generate the necessary date column for this function.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nDataframe of food logging data. A column for ‘date’ must exist within the data.\n\n\nReturns\nint\nNumber of days with at least one log on that day.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nlogging_day_counts(df)\n\n636\n\n\n\nsource\n\n\n\n\n find_missing_logging_days (df:pandas.core.frame.DataFrame,\n                            start_date:datetime.date='not_defined',\n                            end_date:datetime.date='not_defined')\n\nFinds days that have no log entries between a start (inclusive) and end date (inclusive). It is recommended that you use find_date to generate the necessary date column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data.\n\n\nstart_date\ndatetime.date\nnot_defined\nStarting date for missing day evaluation. By default the earliest date in the data will be used.\n\n\nend_date\ndatetime.date\nnot_defined\nEnding date for missing day evaluation. By default the latest date in the data will be used.\n\n\nReturns\nlist\n\nList of days within the given timeframe that have no log entries.\n\n\n\nThe phrase ‘not_defined’ is the intended default value for start and end dates to signify that the earliest and/or latest date within the data should be used. If a participant is missing a valid start or end date, null is returned.\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nfind_missing_logging_days(df, datetime.date(2017, 12, 7), datetime.date(2017, 12, 10))\n\n[datetime.date(2017, 12, 7),\n datetime.date(2017, 12, 9),\n datetime.date(2017, 12, 10)]\n\n\n\ndf[df['date'].astype(str).str.contains(\"2017-12\")]\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\n\n\n2\n8409118\nalqt14018795225\n150\nSalad\nf\n2017-12-09 00:58:00+00:00\n2017-12-08\n24.966667\n00:58:00\n1\n2017\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n good_lwa_day_counts (df:pandas.core.frame.DataFrame,\n                      window_start:datetime.time,\n                      window_end:datetime.time, min_log_num:int=2,\n                      min_separation:int=5, buffer_time:str='15 minutes',\n                      h:int=4, start_date:datetime.date='not_defined',\n                      end_date:datetime.date='not_defined',\n                      time_col:int=7)\n\nCalculates the number of ‘good’ logging days, ‘good’ window days, ‘outside’ window days and adherent days.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data.\n\n\nwindow_start\ndatetime.time\n\nStarting time for a time restriction window.\n\n\nwindow_end\ndatetime.time\n\nEnding time for a time restriction window.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n5\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nbuffer_time\nstr\n15 minutes\npd.Timedelta parsable string, representing ‘wiggle room’ for adherence.\n\n\nh\nint\n4\nNumber of hours to shift the definition of ‘date’ by. h = 4 would indicate that a log date begins at4:00 AM and ends the following calendar day at 3:59:59. Float representations of time would thereforego from 4.0 (inclusive) to 28.0 (exclusive) to represent ‘date’ membership for days shifted from theiroriginal calendar date.\n\n\nstart_date\ndatetime.date\nnot_defined\nStarting date for missing day evaluation. By default the earliest date in the data will be used.\n\n\nend_date\ndatetime.date\nnot_defined\nEnding date for missing day evaluation. By default the latest date in the data will be used.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\ntuple[list, list]\n\nList containing number of ‘good’ logging days, ‘good’ window days, ‘outside’ window days, and adherent days.List of three lists. The lists contains dates that are not considered ‘good’ logging days, ‘good’ window days,or adherent days (in that order).\n\n\n\nThe main use of this function is to calculate window and logging adherence. These are represented as ‘good’ (valid) logging days, ‘good’ window days, ‘outside’ (invalid) window days, and adherent days.\nThe definition of each is:\n\n‘Good’ Logging Day\n\nA day with at least a specified minimum number of caloric (food or beverage) logs with a minimum specified number of hours between the first and last log for that day.\n\n‘Good’ Window Day\n\nA day where all food loggings are within the participant’s assigned eating restriction window plus any wiggle room, if allowed.\n\nAdherent Day\n\nA day that is both a ‘good’ logging day and a ‘good’ window day.\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ndates, bad_dates = good_lwa_day_counts(df, datetime.time(8,0,0), datetime.time(23,59,59))\ndates\n\nThe second product of this function is three lists that outline which days are not compliant with one of the definitions above. The first list (index 0) consists of dates that are not ‘good’ logging days, the second contains days that are not ‘good’ window days. The final list consists of dates that are not adherent (neither ‘good’ window nor ‘good’ logging dates).\n\nbad_dates[0][:5]",
    "crumbs": [
      "Time Restricted Eating ExperimenTS API"
    ]
  },
  {
    "objectID": "core.html#utils",
    "href": "core.html#utils",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "",
    "text": "These functions primarily serve as parts of other functions, but are provided here for utility.\n\nsource\n\n\n\n file_loader (data_source:Union[str,pandas.core.frame.DataFrame])\n\nFlexible file loader able to read a single file path or folder path. Accepts .csv and .json file format loading.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame.Existing dataframes are read as is.\n\n\nReturns\npd.DataFrame\nA single dataframe consisting of all data matching the provided file or folder path.\n\n\n\nProviding the file loader with a specific file path outputs a single Pandas dataframe generated from that data source.\n\nfile_loader(\"data/col_test_data/toy_data_2000.csv\").head(2)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nmilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nsome medication\nm\nyrt1999\n\n\n\n\n\n\n\n\nThe file loader can also accept string patterns to read in multiple files at once. Providing a patterened path such as yrt*_food_data*.csv would load all data matching this pattern.\n\nfile_loader('data/col_test_data/yrt*_food_data*.csv').head(2)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndesc_text\nfood_type\nPID\n\n\n\n\n0\n2021-05-12 02:30:00 +0000\nMilk\nb\nyrt1999\n\n\n1\n2021-05-12 02:45:00 +0000\nSome Medication\nm\nyrt1999\n\n\n\n\n\n\n\n\nIt can also handle reading mixed file types. The below dataframe consists of data read from all .json and .csv files in the data/output/ folder.\n\nfile_loader('data/output/*').head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nlocal_time\ntime\nweek_from_start\nyear\ncleaned\nday_count\n\n\n\n\n0\n7572733.0\nalqt14018795225\n150.0\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1.0\n2017.0\nNaN\nNaN\n\n\n1\n411111.0\nalqt14018795225\n150.0\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1.0\n2017.0\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_date (data_source:Union[str,pandas.core.frame.DataFrame], h:int=4,\n            date_col:int=5)\n\nExtracts date from a datetime column after shifting datetime by ‘h’ hours. A day starts ‘h’ hours early if ‘h’ is negative, or ‘h’ hours later if ‘h’ is positive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n4\nNumber of hours to shift the definition for ‘date’ by. h = 4 would shift days so that time membershipto each date starts at 4:00 AM and ends at 3:59:59 AM the next calendar day.\n\n\ndate_col\nint\n5\nColumn number for existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.Series\n\nSeries of dates in ISO 8601 format.\n\n\n\nBy default, find_date expects log dates for studies to begin at 4:00 AM. To use regular calendar dates, remember to set h = 0.\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\ndf['date'] = find_date(df, h = 0)\ndf[['original_logtime', 'date']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n\n\n\n\n\n\n\n\nIn this example, with log dates starting at the default value of 4 (4:00 AM), we see that two logs from very early morning on 2017-12-09 are counted as being logged on 2017-12-08 instead.\n\ndf['date'] = find_date(df, h = 4)\ndf[['original_logtime', 'date']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-08\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-08\n\n\n\n\n\n\n\n\nSimilarly, in an example where we start log days four hours earlier, the last two rows have dates that are shifted so their log date is one day later than their exact calendar datetime.\n\ndf['date'] = find_date(df, h = -4)\ndf[['original_logtime', 'date']].head(5)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n\n\n3\n2018-02-22 21:52:00+00:00\n2018-02-23\n\n\n4\n2018-02-22 22:53:00+00:00\n2018-02-23\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_float_time (data_source:Union[str,pandas.core.frame.DataFrame],\n                  h:int=4, date_col:int=5)\n\nExtracts time from a datetime column after shifting datetime by ‘h’ hours. A day starts ‘h’ hours early if ‘h’ is negative, or ‘h’ hours later if ‘h’ is positive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n4\nNumber of hours to shift the definition for ‘time’ by. h = 4 would allow float representations of timebetween 4 (inclusive) and 28 (exclusive), representing time that goes from 4:00 AM to 3:59:59 AM the nextcalendar day. NOTE: h value for this function should match the h value used for generating dates.\n\n\ndate_col\nint\n5\nColumn number for existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.Series\n\nSeries of times in float format (e.g. 4:36 AM -&gt; 4.6).\n\n\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\nBy default, find_float_time expects studies to begin at 4:00 AM. To preserve regular calendar dates use h = 0.\n\ndf['float_time'] = find_float_time(df, h = 0)\ndf[['original_logtime', 'float_time']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n0.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n0.966667\n\n\n\n\n\n\n\n\nUsing positive values for h for both date and float time functions changes date ownership for a row based on its original logtime. Float time should be shifted by the same h value as date membership so that times belonging to a different calendar date can be differentiated when necessary (e.g. 2:00 AM –&gt; 2.0, whereas 2:00 AM the next calendar day –&gt; 26.0, for cases where these rows should still be grouped together on the same logging date).\n\ndf['float_time'] = find_float_time(df, h = 4)\ndf['date'] = find_date(df, h = 4)\ndf[['original_logtime','date', 'float_time']].head(3)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-08\n24.966667\n\n\n\n\n\n\n\n\nIn rare cases, it may be valuable to shift date and time by negative values. In this example where a log date starts at 8:00 PM the previous calendar day and ends at 8:00 PM the current calendar day, note that the last two rows have negative float times and their date membership is shifted one date further than their original calendar datetime.\n\ndf['float_time'] = find_float_time(df, h = -4)\ndf['date'] = find_date(df, h = -4)\ndf[['original_logtime','date', 'float_time']].head(5)\n\n\n\n\n\n\n\n\n\noriginal_logtime\ndate\nfloat_time\n\n\n\n\n0\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n\n\n1\n2017-12-09 00:01:00+00:00\n2017-12-09\n0.016667\n\n\n2\n2017-12-09 00:58:00+00:00\n2017-12-09\n0.966667\n\n\n3\n2018-02-22 21:52:00+00:00\n2018-02-23\n-2.133333\n\n\n4\n2018-02-22 22:53:00+00:00\n2018-02-23\n-1.116667\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n week_from_start (data_source:Union[str,pandas.core.frame.DataFrame],\n                  identifier:int=1)\n\nCalculates the number of weeks between each logging entry and the first logging entry for each participant. A ‘date’ column must exist in the provided data source. Using the provided find_date function is recommended.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\nReturns\nnp.array\n\nArray of weeks passed from log date to the minimum date for each participant.\n\n\n\n\ndf = file_loader('data/test_food_details.csv')\ndf['original_logtime'] = pd.to_datetime(df['original_logtime'])\n\nUsing find_date to ensure that a date column exists in the data source is recommended. A column labeled ‘date’ is a requirement of this function.\n\ndf['date'] = find_date(df)\ndf['week_from_start'] = week_from_start(df)\ndf[['unique_code','original_logtime','week_from_start']][2:4]\n\n\n\n\n\n\n\n\n\nunique_code\noriginal_logtime\nweek_from_start\n\n\n\n\n2\nalqt14018795225\n2017-12-09 00:58:00+00:00\n1\n\n\n3\nalqt14018795225\n2018-02-22 21:52:00+00:00\n11\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n find_phase_duration (df:pandas.core.frame.DataFrame)\n\nCalculates the duration (in days) of the study phase for each row.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nParticipant information dataframe with columns for start and ending date for that row’s study phase.The expected column numbers for starting and ending dates are outlined in the HOWTO document that accompanies TREETS.\n\n\nReturns\npd.DataFrame\nDataframe with an additional column describing study phase duration.\n\n\n\n\nfind_phase_duration(pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))[['phase_duration']]\n\n\n\n\n\n\n\n\n\nphase_duration\n\n\n\n\n0\n3 days\n\n\n1\n4 days\n\n\n2\n3 days\n\n\n3\n4 days\n\n\n4\nNaT\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n load_food_data (data_source:Union[str,pandas.core.frame.DataFrame],\n                 h:int, identifier:int=1, datetime_col:int=5)\n\nLoads and processes existing logging data, adding specific datetime information in formats more suitable for TREETS functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nh\nint\n\nNumber of hours to shift the definition of ‘date’ by. h = 4 would indicate that a log date begins at4:00 AM and ends the following calendar day at 3:59:59 AM. Float representations of time would thereforego from 4.0 (inclusive) to 28.0 (exclusive) to represent ‘date’ membership for days shifted from theiroriginal calendar date.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndatetime_col\nint\n5\nColumn number for an existing datetime column in provided data source. Data exported from mCC typicallyhas datetime as its 5th column (with indexing starting from 0).\n\n\nReturns\npd.DataFrame\n\nDataframe with additional date, float time, and week from start columns.\n\n\n\n\nload_food_data('data/test_food_details.csv', h = 4).head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n in_good_logging_day (data_source:Union[str,pandas.core.frame.DataFrame],\n                      min_log_num:int=2, min_separation:int=5,\n                      identifier:int=1, date_col:int=6, time_col:int=7)\n\nCalculates if each log is considered to be within a ‘good logging day’. A log day is considered ‘good’ if there are at least the minimum number of required logs, with a minimum specified hour separation between the first and last log for that log date. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n5\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nnp.array\n\nBoolean array describing whether each log is a ‘good’ logging day.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ndf['in_good_logging_day'] = in_good_logging_day(df)\ndf.head(2)\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\nin_good_logging_day\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\nTrue\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\nTrue\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n FoodParser ()\n\nFood parser handles taking unprocessed food log entries and adding relevant information from a pre-made dictionary. This includes matching unprocessed terms to their likely matches, adding food type and other identifying information.\n\nsource\n\n\n\n\n clean_loggings (data_source:Union[str,pandas.core.frame.DataFrame],\n                 identifier:int=1)\n\nCleans and attempts typo correction for all logging text entries.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\nReturns\npd.DataFrame\n\nDataframe with an additional column containing cleaned and typo corrected item entries.\n\n\n\nText descriptions of food items are cleaned using a built-in dictionary of common typos and corrections for each phrase. Phrases are then matched using a dictionary of known n-gram item names. The resulting item(s) are provided as a list.\n\nclean_loggings('data/output/public.json').head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\ncleaned\n\n\n\n\n0\nalqt14018795225\nWater\n[water]\n\n\n1\nalqt14018795225\nCoffee White\n[coffee, white]\n\n\n2\nalqt14018795225\nSalad\n[salad]\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_types (data_source:Union[str,pandas.core.frame.DataFrame],\n            food_type:Union[str,list])\n\nFilters logs for only logs of specified type(s).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder pathswith files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. A column ‘food_type’ is required to be within the data.\n\n\nfood_type\nstr | list\nA single food type, or list of food types. Valid types are ‘f’: food, ‘b’: beverage, ‘w’: water,and ‘m’: medication.\n\n\nReturns\npd.DataFrame\nDataframe filtered for only logs of specific type(s). \n\n\n\nType selection accepts multiple types at once as a list of entry types. All types chosen must be valid.\nAvailable food types include:\n\n‘f’: Food\n‘b’: Beverage\n‘w’: Water\n‘m’: Medication\n\nFlavored water beverages such as La Croix are counted as ‘water’ and not as ‘beverage’.\n\nget_types('data/output/baseline.json',['w', 'f'])[['unique_code','desc_text','food_type']].head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\nfood_type\n\n\n\n\n0\nalqt14018795225\nWater\nw\n\n\n2\nalqt14018795225\nSalad\nf\n\n\n3\nalqt78896444285\nWater\nw\n\n\n\n\n\n\n\n\nFiltering for a single type is also possible.\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nget_types(df, 'm')[['unique_code','desc_text','food_type']].head(3)\n\n\n\n\n\n\n\n\n\nunique_code\ndesc_text\nfood_type\n\n\n\n\n323\nalqt14018795225\nCaffeine\nm\n\n\n361\nalqt14018795225\nCaffeine\nm\n\n\n420\nalqt14018795225\nCaffeine\nm\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n count_caloric_entries (df:pandas.core.frame.DataFrame)\n\nCounts the number of food (‘f’) and beverage (‘b’) loggings.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nDataframe of food logging data.\n\n\nReturns\nint\nNumber of caloric (food or beverage) entries found.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ncount_caloric_entries(df)\n\n4603\n\n\n\nsource\n\n\n\n\n mean_daily_eating_duration (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates mean daily eating window by taking the average of each day’s eating window. An eating window is defined as the duration of time between first and last caloric (food or beverage) intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average daily eating window duration.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_duration(df)\n\n14.038679245283017\n\n\n\nsource\n\n\n\n\n std_daily_eating_duration (df:pandas.core.frame.DataFrame,\n                            date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the daily eating window. An eating window is defined as the duration of time between first and last caloric (food or beverage) intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of daily eating window duration.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_duration(df)\n\n7.018679942775867\n\n\n\nsource\n\n\n\n\n earliest_entry (df:pandas.core.frame.DataFrame, time_col:int=7)\n\nCalculates the earliest recorded caloric (food or beverage) entry. It is recommended that you use find_float_time to generate necessary the time column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the earliest logtime on any date.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nearliest_entry(df)\n\n4.0\n\n\n\nsource\n\n\n\n\n mean_first_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                 time_col:int=7)\n\nCalculates the average time of first caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average first caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_first_cal(df)\n\n9.22680817610063\n\n\n\n# find the average mean first cal time for each participant\ndf.groupby(['unique_code']).agg(mean_first_cal, date_col = 6, time_col = 7).iloc[:,0]\n\nunique_code\nalqt1148284857      7.315278\nalqt14018795225     7.635938\nalqt16675467779     6.153904\nalqt21525720972    13.211957\nalqt45631586569    15.056295\nalqt5833085442     12.551515\nalqt62359040167     7.252137\nalqt6695047873      7.573077\nalqt78896444285     6.347510\nalqt8668165687      9.702555\nName: ID, dtype: float64\n\n\n\nsource\n\n\n\n\n std_first_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                time_col:int=7)\n\nCalculates the standard deviation for time of first caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of first caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_first_cal(df)\n\n4.591417471559444\n\n\n\nsource\n\n\n\n\n mean_last_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n                time_col:int=7)\n\nCalculates the average time of last caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of average last caloric entry time.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_last_cal(df)\n\n23.265487421383646\n\n\n\nsource\n\n\n\n\n std_last_cal (df:pandas.core.frame.DataFrame, date_col:int=6,\n               time_col:int=7)\n\nCalculates the standard deviation for time of last caloric intake. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nfloat\n\nFloat representation of the standard deviation of last caloric entry time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_last_cal(df, 'date', 'float_time')\n\n4.359435007580498\n\n\n\nsource\n\n\n\n\n mean_daily_eating_occasions (df:pandas.core.frame.DataFrame,\n                              date_col:int=6, time_col:int=7)\n\nCalculates the average number of daily eating occasions. An eating occasion is a single caloric (food or beverage) log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nAverage number of daily eating occasions. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_occasions(df, 'date', 'float_time')\n\n6.8915094339622645\n\n\n\nsource\n\n\n\n\n std_daily_eating_occasions (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the number of daily eating occasions. An eating occasion is a single caloric (food or beverage) log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nStandard deviation of the number of daily eating occasions. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_occasions(df, 'date', 'float_time')\n\n4.44839423402741\n\n\n\nsource\n\n\n\n\n mean_daily_eating_midpoint (df:pandas.core.frame.DataFrame,\n                             date_col:int=6, time_col:int=7)\n\nCalculates the average daily midpoint eating occasion time. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nFloat representation of the average daily midpoint eating occasion time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nmean_daily_eating_midpoint(df, 'date', 'float_time')\n\n16.536425576519914\n\n\n\nsource\n\n\n\n\n std_daily_eating_midpoint (df:pandas.core.frame.DataFrame,\n                            date_col:int=6, time_col:int=7)\n\nCalculates the standard deviation of the daily midpoint eating occasion time. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column for ‘food_type’ must exist within the data.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nint\n\nFloat representation of the standard deviation of the daily midpoint eating occasion time. \n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nstd_daily_eating_midpoint(df, 'date', 'float_time')\n\n4.107072970435106\n\n\n\nsource\n\n\n\n\n logging_day_counts (df:pandas.core.frame.DataFrame)\n\nCalculates the number of days that contain any logs. It is recommended that you use find_date to generate the necessary date column for this function.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nDataframe of food logging data. A column for ‘date’ must exist within the data.\n\n\nReturns\nint\nNumber of days with at least one log on that day.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nlogging_day_counts(df)\n\n636\n\n\n\nsource\n\n\n\n\n find_missing_logging_days (df:pandas.core.frame.DataFrame,\n                            start_date:datetime.date='not_defined',\n                            end_date:datetime.date='not_defined')\n\nFinds days that have no log entries between a start (inclusive) and end date (inclusive). It is recommended that you use find_date to generate the necessary date column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data.\n\n\nstart_date\ndatetime.date\nnot_defined\nStarting date for missing day evaluation. By default the earliest date in the data will be used.\n\n\nend_date\ndatetime.date\nnot_defined\nEnding date for missing day evaluation. By default the latest date in the data will be used.\n\n\nReturns\nlist\n\nList of days within the given timeframe that have no log entries.\n\n\n\nThe phrase ‘not_defined’ is the intended default value for start and end dates to signify that the earliest and/or latest date within the data should be used. If a participant is missing a valid start or end date, null is returned.\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nfind_missing_logging_days(df, datetime.date(2017, 12, 7), datetime.date(2017, 12, 10))\n\n[datetime.date(2017, 12, 7),\n datetime.date(2017, 12, 9),\n datetime.date(2017, 12, 10)]\n\n\n\ndf[df['date'].astype(str).str.contains(\"2017-12\")]\n\n\n\n\n\n\n\n\n\nID\nunique_code\nresearch_info_id\ndesc_text\nfood_type\noriginal_logtime\ndate\nfloat_time\ntime\nweek_from_start\nyear\n\n\n\n\n0\n7572733\nalqt14018795225\n150\nWater\nw\n2017-12-08 17:30:00+00:00\n2017-12-08\n17.500000\n17:30:00\n1\n2017\n\n\n1\n411111\nalqt14018795225\n150\nCoffee White\nb\n2017-12-09 00:01:00+00:00\n2017-12-08\n24.016667\n00:01:00\n1\n2017\n\n\n2\n8409118\nalqt14018795225\n150\nSalad\nf\n2017-12-09 00:58:00+00:00\n2017-12-08\n24.966667\n00:58:00\n1\n2017\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n good_lwa_day_counts (df:pandas.core.frame.DataFrame,\n                      window_start:datetime.time,\n                      window_end:datetime.time, min_log_num:int=2,\n                      min_separation:int=5, buffer_time:str='15 minutes',\n                      h:int=4, start_date:datetime.date='not_defined',\n                      end_date:datetime.date='not_defined',\n                      time_col:int=7)\n\nCalculates the number of ‘good’ logging days, ‘good’ window days, ‘outside’ window days and adherent days.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data.\n\n\nwindow_start\ndatetime.time\n\nStarting time for a time restriction window.\n\n\nwindow_end\ndatetime.time\n\nEnding time for a time restriction window.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n5\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nbuffer_time\nstr\n15 minutes\npd.Timedelta parsable string, representing ‘wiggle room’ for adherence.\n\n\nh\nint\n4\nNumber of hours to shift the definition of ‘date’ by. h = 4 would indicate that a log date begins at4:00 AM and ends the following calendar day at 3:59:59. Float representations of time would thereforego from 4.0 (inclusive) to 28.0 (exclusive) to represent ‘date’ membership for days shifted from theiroriginal calendar date.\n\n\nstart_date\ndatetime.date\nnot_defined\nStarting date for missing day evaluation. By default the earliest date in the data will be used.\n\n\nend_date\ndatetime.date\nnot_defined\nEnding date for missing day evaluation. By default the latest date in the data will be used.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\ntuple[list, list]\n\nList containing number of ‘good’ logging days, ‘good’ window days, ‘outside’ window days, and adherent days.List of three lists. The lists contains dates that are not considered ‘good’ logging days, ‘good’ window days,or adherent days (in that order).\n\n\n\nThe main use of this function is to calculate window and logging adherence. These are represented as ‘good’ (valid) logging days, ‘good’ window days, ‘outside’ (invalid) window days, and adherent days.\nThe definition of each is:\n\n‘Good’ Logging Day\n\nA day with at least a specified minimum number of caloric (food or beverage) logs with a minimum specified number of hours between the first and last log for that day.\n\n‘Good’ Window Day\n\nA day where all food loggings are within the participant’s assigned eating restriction window plus any wiggle room, if allowed.\n\nAdherent Day\n\nA day that is both a ‘good’ logging day and a ‘good’ window day.\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\ndates, bad_dates = good_lwa_day_counts(df, datetime.time(8,0,0), datetime.time(23,59,59))\ndates\n\nThe second product of this function is three lists that outline which days are not compliant with one of the definitions above. The first list (index 0) consists of dates that are not ‘good’ logging days, the second contains days that are not ‘good’ window days. The final list consists of dates that are not adherent (neither ‘good’ window nor ‘good’ logging dates).\n\nbad_dates[0][:5]",
    "crumbs": [
      "Time Restricted Eating ExperimenTS API"
    ]
  },
  {
    "objectID": "core.html#experiment-design",
    "href": "core.html#experiment-design",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "Experiment Design",
    "text": "Experiment Design\nThis group of functions provides methods for filtering participant data.\n\nsource\n\nfiltering_usable_data\n\n filtering_usable_data (df:pandas.core.frame.DataFrame, num_items:int,\n                        num_days:int, identifier:int=1, date_col:int=6)\n\nFilters data for only participants who’s data satisfies the minimum number of days and logs. It is recommended that you use find_date to generate the necessary date column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nDataframe of food logging data. A column ‘desc_text’, typically found in mCC datais required.\n\n\nnum_items\nint\n\nMinimum number of logs required to pass filter criteria.\n\n\nnum_days\nint\n\nMinimum number of unique logging days required to pass filter criteria.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\nReturns\ntuple[pd.DataFrame, set]\n\nData filtered to only include data from participants that have passed filtering criteria.Set of participants that passed filtering criteria.\n\n\n\n\ndf = file_loader('data/output/public.json')\n\n\nfiltering_usable_data(df, num_items = 1000, num_days = 14)[0].shape\n\n\nsource\n\n\nprepare_baseline_and_intervention_usable_data\n\n prepare_baseline_and_intervention_usable_data\n                                                (data_source:Union[str,pan\n                                                das.core.frame.DataFrame],\n                                                baseline_num_items:int,\n                                                baseline_num_days:int, int\n                                                ervention_num_items:int,\n                                                intervention_num_days:int,\n                                                identifier:int=1,\n                                                date_col:int=6)\n\nFilters data for ‘usable’ data within baseline and last two weeks of intervention (weeks 13 and 14). It is recommended that you use the function ‘week_from_start’ to generate the necessary week column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nbaseline_num_items\nint\n\nNumber of logs for a participant’s baseline data to pass filter criteria.\n\n\nbaseline_num_days\nint\n\nNumber of unique logging days for a participant’s baseline data to pass filter criteria.\n\n\nintervention_num_items\nint\n\nNumber of logs for a participant’s intervention data to pass filter criteria.\n\n\nintervention_num_days\nint\n\nNumber of unique logging days for a participant’s intervention data to pass filter criteria.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\nReturns\nlist\n\nList of two dataframes: usable baseline data, usable intervention data.\n\n\n\n\ndf= prepare_baseline_and_intervention_usable_data('data/output/public.json', 20, 10, 40, 12)[0]\ndf.head(2)\n\n\ndf.shape",
    "crumbs": [
      "Time Restricted Eating ExperimenTS API"
    ]
  },
  {
    "objectID": "core.html#analysis-and-data-summaries",
    "href": "core.html#analysis-and-data-summaries",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "Analysis and Data Summaries",
    "text": "Analysis and Data Summaries\nData analysis and summary functions, including summary functions for specific statistics.\n\nsource\n\nusers_sorted_by_logging\n\n users_sorted_by_logging\n                          (data_source:Union[str,pandas.core.frame.DataFra\n                          me], food_type:list=['f', 'b', 'm', 'w'],\n                          min_log_num:int=2, min_separation:int=4,\n                          identifier:int=1, date_col:int=6,\n                          time_col:int=7)\n\nReports the number of ‘good’ logging days for each user, in descending order based on number of ‘good’ logging days.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nfood_type\nlist\n[‘f’, ‘b’, ‘m’, ‘w’]\nA single food type, or list of food types. Valid types are ‘f’: food, ‘b’: beverage,‘w’: water, and ‘m’: medication.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\npd.DataFrame\n\nDataframe containing the number of good logging days for each user. \n\n\n\n\nusers_sorted_by_logging('data/output/public.json', ['f','b']).head(2)\n\n\nsource\n\n\neating_intervals_percentile\n\n eating_intervals_percentile\n                              (data_source:Union[str,pandas.core.frame.Dat\n                              aFrame], identifier:int=1, time_col:int=7)\n\nCalculates the 2.5, 5, 10, 12.5, 25, 50, 75, 87.5, 90, 95, and 97.5 percentile eating time for each participant. It also calculates the middle 95, 90, 80, 75, and 50 percentile eating windows for each participant. It is recommended that you use find_float_time to generate necessary the time column for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\npd.DataFrame\n\nDataframe with count, mean, std, min, quantiles and mid XX%tile eating window durations for all participants.\n\n\n\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\neating_intervals_percentile(df).iloc[:2]\n\n\nsource\n\n\nfirst_cal_analysis_summary\n\n first_cal_analysis_summary\n                             (data_source:Union[str,pandas.core.frame.Data\n                             Frame], min_log_num:int=2,\n                             min_separation:int=4, identifier:int=1,\n                             date_col:int=6, time_col:int=7)\n\nCalculates the 5, 10, 25 , 50, 75, 90, 95 percentile of first caloric entry time for each participant on ‘good’ logging days. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\npd.DataFrame\n\nDataframe with 5, 10, 25, 50, 75, 90, 95 percentile of first caloric entry time for all participants.\n\n\n\n\nfirst_cal_analysis_summary('data/output/baseline.json').head(3)\n\n\nsource\n\n\nlast_cal_analysis_summary\n\n last_cal_analysis_summary\n                            (data_source:Union[str,pandas.core.frame.DataF\n                            rame], min_log_num:int=2,\n                            min_separation:int=4, identifier:int=1,\n                            date_col:int=6, time_col:int=7)\n\nCalculates the 5, 10, 25 , 50, 75, 90, 95 percentile of last caloric entry time for each participant on ‘good’ logging days. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\npd.DataFrame\n\nDataframe with 5, 10, 25, 50, 75, 90, 95 percentile of last caloric entry time for all participants.\n\n\n\n\nlast_cal_analysis_summary('data/output/baseline.json').head(3)\n\n\nsource\n\n\nsummarize_data\n\n summarize_data (data_source:Union[str,pandas.core.frame.DataFrame],\n                 min_log_num:int=2, min_separation:int=4,\n                 identifier:int=1, date_col:int=6, time_col:int=7)\n\nSummarizes participant data, including number of days, total number of logs, number of food/beverage logs, number of medication logs, number of water logs, eating window duration information, first and last caloric log information, and adherence.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column (with indexing starting from 0).\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\npd.DataFrame\n\nSummary dataframe.\n\n\n\nThis function provides summary data for an entire study, without separating for study phases. Summaries include statistics for first and last caloric log, eating window, and relevant calculations for middle 95 percentile eating window.\n\ndf = load_food_data('data/test_food_details.csv', h = 4)\nsummarize_data(df)\n\n\nsource\n\n\nsummarize_data_with_experiment_phases\n\n summarize_data_with_experiment_phases\n                                        (food_data:pandas.core.frame.DataF\n                                        rame, ref_tbl:pandas.core.frame.Da\n                                        taFrame, min_log_num:int=2,\n                                        min_separation:int=5,\n                                        buffer_time:str='15 minutes',\n                                        h:int=4, report_level:int=2,\n                                        txt:bool=False)\n\nSummarizes participant data for each experiment phase and eating window assignment. Summary includes number of days, total number of logs, number of food/beverage logs, number of medication logs, number of water logs, eating window duration information, first and last caloric log information, and adherence.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfood_data\npd.DataFrame\n\nDataframe of food logging data. A column for “original_logtime” must exist within the data. mCC output styledata is expected.\n\n\nref_tbl\npd.DataFrame\n\nParticipant data reference table. See the accompanying HOWTO document for required column positions andformatting.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n5\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nbuffer_time\nstr\n15 minutes\npd.Timedelta parsable string, representing ‘wiggle room’ for adherence.\n\n\nh\nint\n4\nNumber of hours to shift the definition of ‘date’ by. h = 4 would indicate that a log date begins at4:00 AM and ends the following calendar day at 3:59:59. Float representations of time would thereforego from 4.0 (inclusive) to 28.0 (exclusive) to represent ‘date’ membership for days shifted from theiroriginal calendar date.\n\n\nreport_level\nint\n2\nAdditional printed info detail level. 0 = No Report. 1 = Report ‘No Logging Days’. 2 = Report ‘No Logging Days’, ‘Bad Logging Days’, ‘Bad Window Days’, and ‘Non-Adherent Days’.\n\n\ntxt\nbool\nFalse\nIf True, a text format (.txt) report will be saved in the current directory, with the name‘treets_warning_dates.txt’\n\n\nReturns\npd.DataFrame\n\nSummary dataframe, where each row represents the summary for a participant during a particularstudy phase. Participants can have multiple rows for a single study phase if, during that study phase,their assigned eating window is altered.\n\n\n\n\ndf = summarize_data_with_experiment_phases(pd.read_csv('data/col_test_data/toy_data_2000.csv')\\\n                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'), report_level = 2)\ndf.T",
    "crumbs": [
      "Time Restricted Eating ExperimenTS API"
    ]
  },
  {
    "objectID": "core.html#plots",
    "href": "core.html#plots",
    "title": "Time Restricted Eating ExperimenTS API",
    "section": "Plots",
    "text": "Plots\nPlotting functions.\n\nsource\n\nfirst_cal_mean_with_error_bar\n\n first_cal_mean_with_error_bar\n                                (data_source:Union[str,pandas.core.frame.D\n                                ataFrame], min_log_num:int=2,\n                                min_separation:int=4, identifier:int=1,\n                                date_col:int=6, time_col:int=7)\n\nRepresents mean and standard deviation of first caloric intake time for each participant as a scatter plot, with participants as the x-axis and time as the y-axis. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object.\n\n\n\n\nfirst_cal_mean_fig = first_cal_mean_with_error_bar('data/output/baseline.json')\n\n\nsource\n\n\nlast_cal_mean_with_error_bar\n\n last_cal_mean_with_error_bar\n                               (data_source:Union[str,pandas.core.frame.Da\n                               taFrame], min_log_num:int=2,\n                               min_separation:int=4, identifier:int=1,\n                               date_col:int=6, time_col:int=7)\n\nRepresents mean and standard deviation of last caloric intake time for each participant as a scatter plot, with the x-axis as participants and the y-axis as time. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object.\n\n\n\n\nlast_cal_mean_fig = last_cal_mean_with_error_bar('data/output/baseline.json')\n\n\nsource\n\n\nfirst_cal_analysis_variability_plot\n\n first_cal_analysis_variability_plot\n                                      (data_source:Union[str,pandas.core.f\n                                      rame.DataFrame], min_log_num:int=2,\n                                      min_separation:int=4,\n                                      identifier:int=1, date_col:int=6,\n                                      time_col:int=7)\n\nCalculates first caloric log time variability for ‘good’ logging days by subtracting 5, 10, 25, 50, 75, 90, 95 percentile of first caloric intake time from the 50th percentile first caloric intake time. It also produces a histogram that represents the 90%-10% interval for all participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nfirst_cal_var_plot = first_cal_analysis_variability_plot('data/output/baseline.json')\n\n\nsource\n\n\nlast_cal_analysis_variability_plot\n\n last_cal_analysis_variability_plot\n                                     (data_source:Union[str,pandas.core.fr\n                                     ame.DataFrame], min_log_num:int=2,\n                                     min_separation:int=4,\n                                     identifier:int=1, date_col:int=6,\n                                     time_col:int=7)\n\nCalculates last caloric log time variability for ‘good’ logging days by subtracting 5, 10, 25, 50, 75, 90, 95 percentile of last caloric intake time from the 50th percentile last caloric intake time. It also produces a histogram that represents the 90%-10% interval for all participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmin_log_num\nint\n2\nMinimum number of logs required for a day to be considered a ‘good’ logging day.\n\n\nmin_separation\nint\n4\nMinimum number of hours between first and last log on a log day for it to be considered a ‘good’ logging day.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nlast_cal_var_plot = last_cal_analysis_variability_plot('data/output/baseline.json')\n\n\nsource\n\n\nfirst_cal_avg_histplot\n\n first_cal_avg_histplot\n                         (data_source:Union[str,pandas.core.frame.DataFram\n                         e], identifier:int=1, date_col:int=6,\n                         time_col:int=7)\n\nPlots a histogram of average first caloric intake for all participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nfirst_cal_avg_plot = first_cal_avg_histplot('data/output/baseline.json')\n\n\nsource\n\n\nfirst_cal_sample_distplot\n\n first_cal_sample_distplot\n                            (data_source:Union[str,pandas.core.frame.DataF\n                            rame], n:int, replace:bool=False,\n                            identifier:int=1, date_col:int=6,\n                            time_col:int=7)\n\nCreates a distplot for the first caloric intake time for a random selection of ‘n’ number of participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nn\nint\n\nNumber of participants to plot for, selected randomly without replacement.\n\n\nreplace\nbool\nFalse\nIf true, samples with replacement. Samples without replacement by default.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nfirst_cal_distplot = first_cal_sample_distplot('data/output/intervention.json', n = 5, replace = False)\n\n\nsource\n\n\nlast_cal_avg_histplot\n\n last_cal_avg_histplot\n                        (data_source:Union[str,pandas.core.frame.DataFrame\n                        ], identifier:int=1, date_col:int=6,\n                        time_col:int=7)\n\nPlots a histogram of average last caloric intake for all participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nlast_cal_avg_hist = last_cal_avg_histplot('data/output/baseline.json')\n\n\nsource\n\n\nlast_cal_sample_distplot\n\n last_cal_sample_distplot\n                           (data_source:Union[str,pandas.core.frame.DataFr\n                           ame], n:int, replace:bool=False,\n                           identifier:int=1, date_col:int=6,\n                           time_col:int=7)\n\nCreates a distplot for the last caloric intake time for a random selection of ‘n’ number of participants. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nn\nint\n\nNumber of participants to plot for, selected randomly without replacement.\n\n\nreplace\nbool\nFalse\nIf true, samples with replacement. Samples without replacement by default.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object. \n\n\n\n\nlast_cal_distplot = last_cal_sample_distplot('data/output/intervention.json', n = 5, replace=False)\n\n\nsource\n\n\nswarmplot\n\n swarmplot (data_source:Union[str,pandas.core.frame.DataFrame],\n            max_loggings:int, identifier:int=1, date_col:int=6,\n            time_col:int=7)\n\nCreates a swarmplot for participants logging data. It is recommended that you use find_date and find_float_time to generate necessary date and time columns for this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\nstr | pd.DataFrame\n\nString file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existingdataframes are read as is. Must have a column for ‘food_type’ within the data.\n\n\nmax_loggings\nint\n\nMaximum number of randomly selected logs to be plotted for each participant.\n\n\nidentifier\nint\n1\nColumn number for an existing unique identifier column in provided data source. Data exported from mCC typicallyhas a unique identifier as its 1st column.\n\n\ndate_col\nint\n6\nColumn number for an existing date column in provided data source.\n\n\ntime_col\nint\n7\nColumn number for an existing time column in provided data source.\n\n\nReturns\nmatplotlib.figure.Figure\n\nMatplotlib figure object.\n\n\n\n\nswarm = swarmplot('data/output/public.json', max_loggings = 20)",
    "crumbs": [
      "Time Restricted Eating ExperimenTS API"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-get-started",
    "href": "CONTRIBUTING.html#how-to-get-started",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks"
  },
  {
    "objectID": "CONTRIBUTING.html#did-you-find-a-bug",
    "href": "CONTRIBUTING.html#did-you-find-a-bug",
    "title": "How to contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable."
  },
  {
    "objectID": "CONTRIBUTING.html#pr-submission-guidelines",
    "href": "CONTRIBUTING.html#pr-submission-guidelines",
    "title": "How to contribute",
    "section": "",
    "text": "Keep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another."
  },
  {
    "objectID": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "href": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to contribute",
    "section": "",
    "text": "Docs are automatically created from the notebooks in the nbs folder."
  }
]