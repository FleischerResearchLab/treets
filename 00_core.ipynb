{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public data food analysis\n",
    "\n",
    "> Process collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from public_data_food_analysis_3.app_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import date \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_public_data(in_path, out_path = 'data/public.pickle'):\n",
    "    \"\"\"\n",
    "    Load public data and output processed data in pickle format.\\n\n",
    "    \n",
    "    Process include:\\n\n",
    "    1. Dropping 'foodimage_file_name' column.\\n\n",
    "    2. Handling the format of time by generating a new column, 'original_logtime_notz'\\n\n",
    "    3. Generating the date column, 'date'\\n\n",
    "    4. Converting time into float number into a new column, 'local_time'\\n\n",
    "    5. Converting time in the 'local_time' column so that day starts at 4 am.\\n\n",
    "    6. Converting time to a format of HH:MM:SS, 'time'\\n\n",
    "    7. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\\n\n",
    "    8. Generating 'year' column based on the input data.\\n\n",
    "    9. Outputing the data into a pickle format file.\\n\n",
    "    \n",
    "    \\n\n",
    "    @param in_path : input path\\n\n",
    "    @param out_path: output path\\n\n",
    "    @return: nothing is returned.\n",
    "    \"\"\"\n",
    "    public_all = pd.read_csv(in_path).drop(columns = ['foodimage_file_name'])\n",
    "    \n",
    "    def handle_public_time(s):\n",
    "        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n",
    "        try:\n",
    "            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "        except:\n",
    "            try:\n",
    "                if int(tmp_s.split()[1][:2]) > 12:\n",
    "                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n",
    "                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "    original_logtime_notz_lst = []\n",
    "    for t in (public_all.original_logtime.values):\n",
    "        original_logtime_notz_lst.append(handle_public_time(t)) \n",
    "    public_all['original_logtime_notz'] = original_logtime_notz_lst\n",
    "    \n",
    "    public_all = public_all.dropna().reset_index(drop = True)\n",
    "    \n",
    "    def find_date(d):\n",
    "        if d.hour < 4:\n",
    "            return d.date() - pd.Timedelta('1 day')\n",
    "        else:\n",
    "            return d.date()\n",
    "    public_all['date'] = public_all['original_logtime_notz'].apply(find_date)\n",
    "    \n",
    "    \n",
    "    # Handle the time - Time in floating point format\n",
    "    public_all['local_time'] = public_all.original_logtime_notz.apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.).values\n",
    "    day_begins_at = 4\n",
    "    public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time'] = 24.0 + public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time']\n",
    "    \n",
    "    # Handle the time - Time in Datetime object format\n",
    "    public_all['time'] = pd.DatetimeIndex(public_all.original_logtime_notz).time\n",
    "    \n",
    "    # Handle week from start\n",
    "    public_start_time_dic = dict(public_all.groupby('unique_code').agg(np.min)['date'])\n",
    "    def count_week_public(s):\n",
    "        return (s.date - public_start_time_dic[s.unique_code]).days // 7 + 1\n",
    "    public_all['week_from_start'] = public_all.apply(count_week_public, axis = 1)\n",
    "    \n",
    "    public_all['year'] = public_all.date.apply(lambda d: d.year)\n",
    "    \n",
    "    public_all_pickle_file = open(out_path, 'wb')\n",
    "    pickle.dump(public_all, public_all_pickle_file)     \n",
    "    print('data is saved at {}'.format(out_path))\n",
    "    public_all_pickle_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is saved at data/public.pickle\n"
     ]
    }
   ],
   "source": [
    "load_public_data('data/test_food_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_baseline_and_intervention_usable_data(in_path, \n",
    "                                                  baseline_expanded_out_path='data/public_basline_usable_expanded.pickle',\n",
    "                                                 intervention_out_path = 'data/public_intervention_usable.pickle'):\n",
    "    \"\"\"\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param out_path: output path\\n\n",
    "    @return: nothing is returned.\n",
    "    \"\"\"\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # create baseline data\n",
    "    df_public_baseline = public_all.query('week_from_start <= 2')\n",
    "    df_public_baseline_usable, public_baseline_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_baseline, num_items = 40, num_days = 12)\n",
    "    \n",
    "    # create intervention data\n",
    "    df_public_intervention = public_all.query('week_from_start in [13, 14]')\n",
    "    df_public_intervention_usable, public_intervention_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_intervention, num_items = 20, num_days = 8)\n",
    "    \n",
    "    # create df that contains both baseline and intervention id_set\n",
    "    expanded_baseline_usable_id_set = set(list(public_baseline_usable_id_set) + list(public_intervention_usable_id_set))\n",
    "    df_public_basline_usable_expanded = public_all.loc[public_all.apply(lambda s: s.week_from_start <= 2 \\\n",
    "                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n",
    "    \n",
    "    # save baseline_expanded and intervention to pickle_file\n",
    "    public_pickle_file = open(baseline_expanded_out_path, 'wb')\n",
    "    pickle.dump(df_public_basline_usable_expanded, public_pickle_file)                      \n",
    "    public_pickle_file.close() \n",
    "    print('baseline_expanded data is saved at {}'.format(baseline_expanded_out_path))\n",
    "    \n",
    "    public_pickle_file = open(intervention_out_path, 'wb')\n",
    "    pickle.dump(df_public_intervention_usable, public_pickle_file)                      \n",
    "    public_pickle_file.close() \n",
    "    print('intervention data is saved at {}'.format(intervention_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => filtering_usable_data()\n",
      "  => using the following criteria: 40 items and 12 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 3\n",
      " => filtering_usable_data()\n",
      "  => using the following criteria: 20 items and 8 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 0\n",
      "baseline_expanded data is saved at data/public_basline_usable_expanded.pickle\n",
      "intervention data is saved at data/public_intervention_usable.pickle\n"
     ]
    }
   ],
   "source": [
    "prepare_baseline_and_intervention_usable_data('data/public.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
