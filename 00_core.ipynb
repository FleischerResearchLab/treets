{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public data food analysis\n",
    "\n",
    "> Process collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from public_data_food_analysis_3.app_data import *\n",
    "from public_data_food_analysis_3.food_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_public_data(in_path, out_path = 'data/public.pickle'):\n",
    "    \"\"\"\n",
    "    Load public data and output processed data in pickle format.\\n\n",
    "    \n",
    "    Process include:\\n\n",
    "    1. Dropping 'foodimage_file_name' column.\\n\n",
    "    2. Handling the format of time by generating a new column, 'original_logtime_notz'\\n\n",
    "    3. Generating the date column, 'date'\\n\n",
    "    4. Converting time into float number into a new column, 'local_time'\\n\n",
    "    5. Converting time in the 'local_time' column so that day starts at 4 am.\\n\n",
    "    6. Converting time to a format of HH:MM:SS, 'time'\\n\n",
    "    7. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\\n\n",
    "    8. Generating 'year' column based on the input data.\\n\n",
    "    9. Outputing the data into a pickle format file.\\n\n",
    "    \n",
    "    \\n\n",
    "    @param in_path : input path\\n\n",
    "    @param out_path: output path\\n\n",
    "    @return: nothing is returned.\n",
    "    \"\"\"\n",
    "    public_all = pd.read_csv(in_path).drop(columns = ['foodimage_file_name'])\n",
    "    \n",
    "    def handle_public_time(s):\n",
    "        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n",
    "        try:\n",
    "            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "        except:\n",
    "            try:\n",
    "                if int(tmp_s.split()[1][:2]) > 12:\n",
    "                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n",
    "                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "    original_logtime_notz_lst = []\n",
    "    for t in (public_all.original_logtime.values):\n",
    "        original_logtime_notz_lst.append(handle_public_time(t)) \n",
    "    public_all['original_logtime_notz'] = original_logtime_notz_lst\n",
    "    \n",
    "    public_all = public_all.dropna().reset_index(drop = True)\n",
    "    \n",
    "    def find_date(d):\n",
    "        if d.hour < 4:\n",
    "            return d.date() - pd.Timedelta('1 day')\n",
    "        else:\n",
    "            return d.date()\n",
    "    public_all['date'] = public_all['original_logtime_notz'].apply(find_date)\n",
    "    \n",
    "    \n",
    "    # Handle the time - Time in floating point format\n",
    "    public_all['local_time'] = public_all.original_logtime_notz.apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.).values\n",
    "    day_begins_at = 4\n",
    "    public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time'] = 24.0 + public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time']\n",
    "    \n",
    "    # Handle the time - Time in Datetime object format\n",
    "    public_all['time'] = pd.DatetimeIndex(public_all.original_logtime_notz).time\n",
    "    \n",
    "    # Handle week from start\n",
    "    public_start_time_dic = dict(public_all.groupby('unique_code').agg(np.min)['date'])\n",
    "    def count_week_public(s):\n",
    "        return (s.date - public_start_time_dic[s.unique_code]).days // 7 + 1\n",
    "    public_all['week_from_start'] = public_all.apply(count_week_public, axis = 1)\n",
    "    \n",
    "    public_all['year'] = public_all.date.apply(lambda d: d.year)\n",
    "    \n",
    "    public_all_pickle_file = open(out_path, 'wb')\n",
    "    pickle.dump(public_all, public_all_pickle_file)     \n",
    "    print('data is saved at {}'.format(out_path))\n",
    "    public_all_pickle_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is saved at data/public.pickle\n"
     ]
    }
   ],
   "source": [
    "load_public_data('data/test_food_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_baseline_and_intervention_usable_data(in_path, \n",
    "                                                  baseline_expanded_out_path='data/public_basline_usable_expanded.pickle',\n",
    "                                                 intervention_out_path = 'data/public_intervention_usable.pickle'):\n",
    "    \"\"\"\n",
    "    Filter and create baseline_expanded and intervention groups based on in_path pickle file.\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param out_path: output path\\n\n",
    "    @return: nothing is returned.\n",
    "    \"\"\"\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # create baseline data\n",
    "    df_public_baseline = public_all.query('week_from_start <= 2')\n",
    "    df_public_baseline_usable, public_baseline_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_baseline, num_items = 40, num_days = 12)\n",
    "    \n",
    "    # create intervention data\n",
    "    df_public_intervention = public_all.query('week_from_start in [13, 14]')\n",
    "    df_public_intervention_usable, public_intervention_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_intervention, num_items = 20, num_days = 8)\n",
    "    \n",
    "    # create df that contains both baseline and intervention id_set that contains data for the first two weeks\n",
    "    expanded_baseline_usable_id_set = set(list(public_baseline_usable_id_set) + list(public_intervention_usable_id_set))\n",
    "    df_public_basline_usable_expanded = public_all.loc[public_all.apply(lambda s: s.week_from_start <= 2 \\\n",
    "                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n",
    "    \n",
    "    # save baseline_expanded and intervention to pickle_file\n",
    "    public_pickle_file = open(baseline_expanded_out_path, 'wb')\n",
    "    pickle.dump(df_public_basline_usable_expanded, public_pickle_file)                      \n",
    "    public_pickle_file.close() \n",
    "    print('baseline_expanded data is saved at {}'.format(baseline_expanded_out_path))\n",
    "    \n",
    "    public_pickle_file = open(intervention_out_path, 'wb')\n",
    "    pickle.dump(df_public_intervention_usable, public_pickle_file)                      \n",
    "    public_pickle_file.close() \n",
    "    print('intervention data is saved at {}'.format(intervention_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => filtering_usable_data()\n",
      "  => using the following criteria: 40 items and 12 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 9\n",
      " => filtering_usable_data()\n",
      "  => using the following criteria: 20 items and 8 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 5\n",
      "baseline_expanded data is saved at data/public_basline_usable_expanded.pickle\n",
      "intervention data is saved at data/public_intervention_usable.pickle\n"
     ]
    }
   ],
   "source": [
    "prepare_baseline_and_intervention_usable_data('data/public.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adherent(s):\n",
    "    \"\"\"\n",
    "    return True if the there are more than 2 loggings in one day w/ more than 4hrs apart from each other.\n",
    "    \"\"\"\n",
    "    if len(s.values) >= 2 and (max(s.values) - min(s.values)) >= 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_active_user(in_path, n, user_day_counts_path = 'data/public_top_users_day_counts.csv'):\n",
    "    \"\"\"\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param n: number of users with most number of logging days\\n\n",
    "    @param user_data_counts_path: output path for top_users_day_counts in csv format\\n\n",
    "    @return: top_users_day_counts dataframe\\n\n",
    "    \n",
    "    This function returns a dataframe in csv format that finds top n users with the most number of days that they logged. \n",
    "    \"\"\"\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # filter the dataframe so it only contains food and beverage food type, then get the top n users who input the\n",
    "    # most loggings in descending order\n",
    "    top_users = public_all.query('food_type in [\"f\", \"b\"]')[['ID', 'unique_code']]\\\n",
    "    .groupby('unique_code').agg('count').sort_values(by = 'ID', ascending = False).index[:n]\n",
    "    top_users = set(list(top_users))\n",
    "    public_top_users = public_all.query('food_type in [\"f\", \"b\"]')\\\n",
    "    .loc[public_all.unique_code.apply(lambda x: x in top_users)].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    adherent_dict = dict(public_top_users.groupby(['unique_code', 'date'])['local_time'].agg(adherent))\n",
    "    public_top_users['adherence'] = public_top_users\\\n",
    "    .apply(lambda x: adherent_dict[(x.unique_code, x.date)], axis = 1)\n",
    "    \n",
    "    public_top_users_day_counts = pd.DataFrame(public_top_users.query('adherence == True')\\\n",
    "                            [['date', 'unique_code']].groupby('unique_code')['date'].nunique())\\\n",
    "                            .sort_values(by = 'date', ascending = False).rename(columns = {'date': 'day_count'})\n",
    "    \n",
    "    public_top_users_day_counts.to_csv(user_day_counts_path)\n",
    "    \n",
    "    print('user day counts data is saved at {}'.format(user_day_counts_path))\n",
    "    \n",
    "    return public_top_users_day_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user day counts data is saved at data/public_top_users_day_counts.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alqt45631586569</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt8668165687</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt21525720972</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt78896444285</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt14018795225</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 day_count\n",
       "unique_code               \n",
       "alqt45631586569        145\n",
       "alqt8668165687         135\n",
       "alqt21525720972         89\n",
       "alqt78896444285         81\n",
       "alqt14018795225         60"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_active_user('data/public.pickle',5,'data/public_top_users_day_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def convert_loggings(in_path, parsed_food_path='data/public_all_parsed.csv', ascending = False):\n",
    "    \"\"\"\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param parsed_food_path : output path for cleaned food loggings in csv format \\n\n",
    "    @return: frequency count of food loggings in the descending order by default\\n\n",
    "    \n",
    "    This function takes in a pickle file, convert all the loggings into individual items of food and count the frequency of all food loggings. \n",
    "    \"\"\"\n",
    "    \n",
    "    # load data\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # initialize food parser instance\n",
    "    fp = FoodParser()\n",
    "    fp.initialization()\n",
    "    \n",
    "    # parse food\n",
    "    parsed = [fp.parse_food(i, return_sentence_tag = True) for i in public_all.desc_text.values]\n",
    "    \n",
    "    print('All food is parsed!')\n",
    "    \n",
    "    public_all_parsed = pd.DataFrame({\n",
    "    'ID': public_all.ID,\n",
    "    'food_type': public_all.food_type,\n",
    "    'desc_text': public_all.desc_text,\n",
    "    'cleaned': parsed\n",
    "    })\n",
    "    \n",
    "    public_all_parsed['cleaned'] = public_all_parsed['cleaned'].apply(lambda x: x[0])\n",
    "    \n",
    "    public_all_parsed.to_csv()\n",
    "    \n",
    "    # count food\n",
    "    all_count_dict = defaultdict(lambda : 0) \n",
    "    for food in public_all_parsed.cleaned.values:\n",
    "        if len(food) > 0:\n",
    "            for f in food:\n",
    "                all_count_dict[f] += 1\n",
    "                \n",
    "    print('All food is counted!')\n",
    "    \n",
    "    all_count_dict = dict(all_count_dict)\n",
    "    public_freq_count = pd.DataFrame({\n",
    "        'food': list(all_count_dict.keys()),\n",
    "        'count': [all_count_dict[k] for k in list(all_count_dict.keys())]\n",
    "    }).sort_values(by = 'count', ascending = ascending).reset_index(drop = True)\n",
    "    \n",
    "    return public_freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/qiwenzhang/Documents/GitHub/public_data_food_analysis_3/public_data_food_analysis_3/data/parser_keys.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69adb39b5f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_loggings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/public.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-22460843d5bc>\u001b[0m in \u001b[0;36mconvert_loggings\u001b[0;34m(in_path, parsed_food_path, ascending)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# initialize food parser instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFoodParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# parse food\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/public_data_food_analysis_3/public_data_food_analysis_3/food_parser.py\u001b[0m in \u001b[0;36minitialization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# 1. read in manually annotated file and bind it to the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mparser_keys_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/parser_keys.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mall_gram_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood_type_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood2tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_parser_keys_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_keys_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcorrection_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/correction_dic.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresource_stream\u001b[0;34m(self, package_or_requirement, resource_name)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresource_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_or_requirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;34m\"\"\"Return a readable file-like object for specified resource\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m         return get_provider(package_or_requirement).get_resource_stream(\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_resource_stream\u001b[0;34m(self, manager, resource_name)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_resource_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/qiwenzhang/Documents/GitHub/public_data_food_analysis_3/public_data_food_analysis_3/data/parser_keys.csv'"
     ]
    }
   ],
   "source": [
    "convert_loggings('data/public.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
