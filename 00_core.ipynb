{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public data food analysis\n",
    "\n",
    "> Process collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qiwenzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from public_data_food_analysis_3.app_data import *\n",
    "from public_data_food_analysis_3.food_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_public_data(in_path, export = False, out_path = 'data/output/public.pickle'):\n",
    "    \"\"\"\n",
    "    Load public data and output processed data in pickle format.\\n\n",
    "    \n",
    "    Process include:\\n\n",
    "    1. Dropping 'foodimage_file_name' column.\\n\n",
    "    2. Handling the format of time by generating a new column, 'original_logtime_notz'\\n\n",
    "    3. Generating the date column, 'date'\\n\n",
    "    4. Converting time into float number into a new column, 'local_time'\\n\n",
    "    5. Converting time in the 'local_time' column so that day starts at 4 am.\\n\n",
    "    6. Converting time to a format of HH:MM:SS, 'time'\\n\n",
    "    7. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\\n\n",
    "    8. Generating 'year' column based on the input data.\\n\n",
    "    9. Outputing the data into a pickle format file.\\n\n",
    "    \n",
    "    \\n\n",
    "    @param in_path : input path\\n\n",
    "    @param out_path: output path\\n\n",
    "    @param export : whether to save the processed dataframe locally\\n\n",
    "    @return: processed dataframe\n",
    "    \"\"\"\n",
    "    public_all = pd.read_csv(in_path).drop(columns = ['foodimage_file_name'])\n",
    "    \n",
    "    def handle_public_time(s):\n",
    "        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n",
    "        try:\n",
    "            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "        except:\n",
    "            try:\n",
    "                if int(tmp_s.split()[1][:2]) > 12:\n",
    "                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n",
    "                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "    original_logtime_notz_lst = []\n",
    "    for t in (public_all.original_logtime.values):\n",
    "        original_logtime_notz_lst.append(handle_public_time(t)) \n",
    "    public_all['original_logtime_notz'] = original_logtime_notz_lst\n",
    "    \n",
    "    public_all = public_all.dropna().reset_index(drop = True)\n",
    "    \n",
    "    def find_date(d):\n",
    "        if d.hour < 4:\n",
    "            return d.date() - pd.Timedelta('1 day')\n",
    "        else:\n",
    "            return d.date()\n",
    "    public_all['date'] = public_all['original_logtime_notz'].apply(find_date)\n",
    "    \n",
    "    \n",
    "    # Handle the time - Time in floating point format\n",
    "    public_all['local_time'] = public_all.original_logtime_notz.apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.).values\n",
    "    day_begins_at = 4\n",
    "    public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time'] = 24.0 + public_all.loc[(public_all['local_time'] < day_begins_at), 'local_time']\n",
    "    \n",
    "    # Handle the time - Time in Datetime object format\n",
    "    public_all['time'] = pd.DatetimeIndex(public_all.original_logtime_notz).time\n",
    "    \n",
    "    # Handle week from start\n",
    "    public_start_time_dic = dict(public_all.groupby('unique_code').agg(np.min)['date'])\n",
    "    def count_week_public(s):\n",
    "        return (s.date - public_start_time_dic[s.unique_code]).days // 7 + 1\n",
    "    public_all['week_from_start'] = public_all.apply(count_week_public, axis = 1)\n",
    "    \n",
    "    public_all['year'] = public_all.date.apply(lambda d: d.year)\n",
    "    \n",
    "    if export == True:\n",
    "        public_all_pickle_file = open(out_path, 'wb')\n",
    "        pickle.dump(public_all, public_all_pickle_file)     \n",
    "        print('data is saved at {}'.format(out_path))\n",
    "        public_all_pickle_file.close() \n",
    "    \n",
    "    return public_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is saved at data/output/public.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>original_logtime_notz</th>\n",
       "      <th>date</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340147</td>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1340148</td>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1340149</td>\n",
       "      <td>8409118</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Salad</td>\n",
       "      <td>f</td>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.966667</td>\n",
       "      <td>00:58:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340150</td>\n",
       "      <td>9615131</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Tea Black</td>\n",
       "      <td>b</td>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>21.866667</td>\n",
       "      <td>21:52:00</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1340151</td>\n",
       "      <td>8100781</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Soup Vegetable</td>\n",
       "      <td>f</td>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>22.883333</td>\n",
       "      <td>22:53:00</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>5374473</td>\n",
       "      <td>1221048</td>\n",
       "      <td>alqt62359040167</td>\n",
       "      <td>150</td>\n",
       "      <td>Rice, Cheese, Corn Tortilla, Mushrooms, Beans</td>\n",
       "      <td>f</td>\n",
       "      <td>2020-07-28 08:00:00+00:00</td>\n",
       "      <td>2020-07-28 08:00:00+00:00</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>5374474</td>\n",
       "      <td>5114874</td>\n",
       "      <td>alqt62359040167</td>\n",
       "      <td>150</td>\n",
       "      <td>Eggs, Cheese, Mushrooms</td>\n",
       "      <td>f</td>\n",
       "      <td>2020-07-28 13:30:00+00:00</td>\n",
       "      <td>2020-07-28 13:30:00+00:00</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>5374475</td>\n",
       "      <td>5432126</td>\n",
       "      <td>alqt62359040167</td>\n",
       "      <td>150</td>\n",
       "      <td>Orange, Chocolate, Banana Pancakes</td>\n",
       "      <td>f</td>\n",
       "      <td>2020-07-29 03:36:00+00:00</td>\n",
       "      <td>2020-07-29 03:36:00+00:00</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>03:36:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>5374476</td>\n",
       "      <td>3810969</td>\n",
       "      <td>alqt62359040167</td>\n",
       "      <td>150</td>\n",
       "      <td>Cheese, Corn Tortilla</td>\n",
       "      <td>f</td>\n",
       "      <td>2020-07-29 14:47:00+00:00</td>\n",
       "      <td>2020-07-29 14:47:00+00:00</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>14.783333</td>\n",
       "      <td>14:47:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>5374477</td>\n",
       "      <td>4341467</td>\n",
       "      <td>alqt62359040167</td>\n",
       "      <td>150</td>\n",
       "      <td>Ramen</td>\n",
       "      <td>f</td>\n",
       "      <td>2020-07-29 08:00:00+00:00</td>\n",
       "      <td>2020-07-29 08:00:00+00:00</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6854 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       ID      unique_code  research_info_id  \\\n",
       "0        1340147  7572733  alqt14018795225               150   \n",
       "1        1340148   411111  alqt14018795225               150   \n",
       "2        1340149  8409118  alqt14018795225               150   \n",
       "3        1340150  9615131  alqt14018795225               150   \n",
       "4        1340151  8100781  alqt14018795225               150   \n",
       "...          ...      ...              ...               ...   \n",
       "6849     5374473  1221048  alqt62359040167               150   \n",
       "6850     5374474  5114874  alqt62359040167               150   \n",
       "6851     5374475  5432126  alqt62359040167               150   \n",
       "6852     5374476  3810969  alqt62359040167               150   \n",
       "6853     5374477  4341467  alqt62359040167               150   \n",
       "\n",
       "                                          desc_text food_type  \\\n",
       "0                                             Water         w   \n",
       "1                                      Coffee White         b   \n",
       "2                                             Salad         f   \n",
       "3                                         Tea Black         b   \n",
       "4                                    Soup Vegetable         f   \n",
       "...                                             ...       ...   \n",
       "6849  Rice, Cheese, Corn Tortilla, Mushrooms, Beans         f   \n",
       "6850                        Eggs, Cheese, Mushrooms         f   \n",
       "6851             Orange, Chocolate, Banana Pancakes         f   \n",
       "6852                          Cheese, Corn Tortilla         f   \n",
       "6853                                          Ramen         f   \n",
       "\n",
       "               original_logtime     original_logtime_notz        date  \\\n",
       "0     2017-12-08 17:30:00+00:00 2017-12-08 17:30:00+00:00  2017-12-08   \n",
       "1     2017-12-09 00:01:00+00:00 2017-12-09 00:01:00+00:00  2017-12-08   \n",
       "2     2017-12-09 00:58:00+00:00 2017-12-09 00:58:00+00:00  2017-12-08   \n",
       "3     2018-02-22 21:52:00+00:00 2018-02-22 21:52:00+00:00  2018-02-22   \n",
       "4     2018-02-22 22:53:00+00:00 2018-02-22 22:53:00+00:00  2018-02-22   \n",
       "...                         ...                       ...         ...   \n",
       "6849  2020-07-28 08:00:00+00:00 2020-07-28 08:00:00+00:00  2020-07-28   \n",
       "6850  2020-07-28 13:30:00+00:00 2020-07-28 13:30:00+00:00  2020-07-28   \n",
       "6851  2020-07-29 03:36:00+00:00 2020-07-29 03:36:00+00:00  2020-07-28   \n",
       "6852  2020-07-29 14:47:00+00:00 2020-07-29 14:47:00+00:00  2020-07-29   \n",
       "6853  2020-07-29 08:00:00+00:00 2020-07-29 08:00:00+00:00  2020-07-29   \n",
       "\n",
       "      local_time      time  week_from_start  year  \n",
       "0      17.500000  17:30:00                1  2017  \n",
       "1      24.016667  00:01:00                1  2017  \n",
       "2      24.966667  00:58:00                1  2017  \n",
       "3      21.866667  21:52:00               11  2018  \n",
       "4      22.883333  22:53:00               11  2018  \n",
       "...          ...       ...              ...   ...  \n",
       "6849    8.000000  08:00:00                6  2020  \n",
       "6850   13.500000  13:30:00                6  2020  \n",
       "6851   27.600000  03:36:00                6  2020  \n",
       "6852   14.783333  14:47:00                6  2020  \n",
       "6853    8.000000  08:00:00                6  2020  \n",
       "\n",
       "[6854 rows x 13 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_public_data('data/test_food_details.csv',export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_baseline_and_intervention_usable_data(in_path, \n",
    "                                                  export = False,\n",
    "                                                  baseline_expanded_out_path='data/output/public_basline_usable_expanded.pickle',\n",
    "                                                 intervention_out_path = 'data/output/public_intervention_usable.pickle'):\n",
    "    \"\"\"\n",
    "    Filter and create baseline_expanded and intervention groups based on in_path pickle file.\\n\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param out_path: output path\\n\n",
    "    @param export : whether to save the processed dataframes locally\\n\n",
    "    @return: baseline expanded and intervention dataframes in a list format where index 0 is the baseline dataframe and 1 is the intervention dataframe.\n",
    "    \"\"\"\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # create baseline data\n",
    "    df_public_baseline = public_all.query('week_from_start <= 2')\n",
    "    df_public_baseline_usable, public_baseline_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_baseline, num_items = 40, num_days = 12)\n",
    "    \n",
    "    # create intervention data\n",
    "    df_public_intervention = public_all.query('week_from_start in [13, 14]')\n",
    "    df_public_intervention_usable, public_intervention_usable_id_set = \\\n",
    "    filtering_usable_data(df_public_intervention, num_items = 20, num_days = 8)\n",
    "    \n",
    "    # create df that contains both baseline and intervention id_set that contains data for the first two weeks\n",
    "    expanded_baseline_usable_id_set = set(list(public_baseline_usable_id_set) + list(public_intervention_usable_id_set))\n",
    "    df_public_basline_usable_expanded = public_all.loc[public_all.apply(lambda s: s.week_from_start <= 2 \\\n",
    "                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n",
    "    \n",
    "    if export == True:\n",
    "        \n",
    "        # save baseline_expanded and intervention to pickle_file\n",
    "        public_pickle_file = open(baseline_expanded_out_path, 'wb')\n",
    "        pickle.dump(df_public_basline_usable_expanded, public_pickle_file)                      \n",
    "        public_pickle_file.close() \n",
    "        print('baseline_expanded data is saved at {}'.format(baseline_expanded_out_path))\n",
    "\n",
    "        public_pickle_file = open(intervention_out_path, 'wb')\n",
    "        pickle.dump(df_public_intervention_usable, public_pickle_file)                      \n",
    "        public_pickle_file.close() \n",
    "        print('intervention data is saved at {}'.format(intervention_out_path))\n",
    "        \n",
    "    return [df_public_basline_usable_expanded, df_public_intervention_usable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => filtering_usable_data()\n",
      "  => using the following criteria: 40 items and 12 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 9\n",
      " => filtering_usable_data()\n",
      "  => using the following criteria: 20 items and 8 days logged in two weeks.\n",
      "  => # of public users pass the criteria: 5\n",
      "baseline_expanded data is saved at data/output/public_basline_usable_expanded.pickle\n",
      "intervention data is saved at data/output/public_intervention_usable.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[      Unnamed: 0       ID      unique_code  research_info_id  \\\n",
       " 0        1340147  7572733  alqt14018795225               150   \n",
       " 1        1340148   411111  alqt14018795225               150   \n",
       " 2        1340149  8409118  alqt14018795225               150   \n",
       " 488      2034996   879074  alqt78896444285               150   \n",
       " 489      2034997  4061186  alqt78896444285               150   \n",
       " ...          ...      ...              ...               ...   \n",
       " 6771     5374395  2037137  alqt62359040167               150   \n",
       " 6772     5374396  9702732  alqt62359040167               150   \n",
       " 6773     5374397  7896169  alqt62359040167               150   \n",
       " 6774     5374398  5248381  alqt62359040167               150   \n",
       " 6775     5374399  1985501  alqt62359040167               150   \n",
       " \n",
       "                                      desc_text food_type  \\\n",
       " 0                                        Water         w   \n",
       " 1                                 Coffee White         b   \n",
       " 2                                        Salad         f   \n",
       " 488                                      Water         w   \n",
       " 489             Water With Apple Cider Vinegar         w   \n",
       " ...                                        ...       ...   \n",
       " 6771    Eggs, Cheese, Chocolate, Corn Tortilla         f   \n",
       " 6772                                 Tangerine         f   \n",
       " 6773  Rice, Cucumber, Kiwee, Chickpeas, Quinoa         f   \n",
       " 6774                             Milk, Cookies         f   \n",
       " 6775    Eggs, Cheese, Chocolate, Corn Tortilla         f   \n",
       " \n",
       "                original_logtime     original_logtime_notz        date  \\\n",
       " 0     2017-12-08 17:30:00+00:00 2017-12-08 17:30:00+00:00  2017-12-08   \n",
       " 1     2017-12-09 00:01:00+00:00 2017-12-09 00:01:00+00:00  2017-12-08   \n",
       " 2     2017-12-09 00:58:00+00:00 2017-12-09 00:58:00+00:00  2017-12-08   \n",
       " 488   2018-06-06 09:06:00+00:00 2018-06-06 09:06:00+00:00  2018-06-06   \n",
       " 489   2018-06-06 09:39:00+00:00 2018-06-06 09:39:00+00:00  2018-06-06   \n",
       " ...                         ...                       ...         ...   \n",
       " 6771  2020-07-04 02:09:00+00:00 2020-07-04 02:09:00+00:00  2020-07-03   \n",
       " 6772  2020-07-04 02:09:00+00:00 2020-07-04 02:09:00+00:00  2020-07-03   \n",
       " 6773  2020-07-04 06:00:00+00:00 2020-07-04 06:00:00+00:00  2020-07-04   \n",
       " 6774  2020-07-04 09:30:00+00:00 2020-07-04 09:30:00+00:00  2020-07-04   \n",
       " 6775  2020-07-05 01:32:00+00:00 2020-07-05 01:32:00+00:00  2020-07-04   \n",
       " \n",
       "       local_time      time  week_from_start  year  \n",
       " 0      17.500000  17:30:00                1  2017  \n",
       " 1      24.016667  00:01:00                1  2017  \n",
       " 2      24.966667  00:58:00                1  2017  \n",
       " 488     9.100000  09:06:00                1  2018  \n",
       " 489     9.650000  09:39:00                1  2018  \n",
       " ...          ...       ...              ...   ...  \n",
       " 6771   26.150000  02:09:00                2  2020  \n",
       " 6772   26.150000  02:09:00                2  2020  \n",
       " 6773    6.000000  06:00:00                2  2020  \n",
       " 6774    9.500000  09:30:00                2  2020  \n",
       " 6775   25.533333  01:32:00                2  2020  \n",
       " \n",
       " [1222 rows x 13 columns],\n",
       "      Unnamed: 0       ID      unique_code  research_info_id  \\\n",
       " 0       1340217  3806061  alqt14018795225               150   \n",
       " 1       1340218  8677718  alqt14018795225               150   \n",
       " 2       1340219  2361039  alqt14018795225               150   \n",
       " 3       1340220  7124815  alqt14018795225               150   \n",
       " 4       1340221  9651830  alqt14018795225               150   \n",
       " ..          ...      ...              ...               ...   \n",
       " 553     4735510  1374706  alqt16675467779               150   \n",
       " 554     4735511  3182264  alqt16675467779               150   \n",
       " 555     4735512   574940  alqt16675467779               150   \n",
       " 556     4735513  8811536  alqt16675467779               150   \n",
       " 557     4735514   107741  alqt16675467779               150   \n",
       " \n",
       "                       desc_text food_type           original_logtime  \\\n",
       " 0              Vegetarian Chili         f  2018-03-02 04:21:00+00:00   \n",
       " 1                      Icecream         f  2018-03-02 04:41:00+00:00   \n",
       " 2                    Wine White         b  2018-03-02 04:45:00+00:00   \n",
       " 3                     Croissant         f  2018-03-02 17:57:00+00:00   \n",
       " 4                        Muffin         f  2018-03-02 18:37:00+00:00   \n",
       " ..                          ...       ...                        ...   \n",
       " 553               Corn In A Cup         f  2020-03-26 05:45:00+00:00   \n",
       " 554  Philly Cheese Steak Dinner         f  2020-03-26 08:30:00+00:00   \n",
       " 555                      Cereal         f  2020-03-27 00:15:00+00:00   \n",
       " 556       Eggs N Potatoes Tacos         f  2020-03-27 02:00:00+00:00   \n",
       " 557                       Water         w  2020-03-27 01:00:00+00:00   \n",
       " \n",
       "         original_logtime_notz        date  local_time      time  \\\n",
       " 0   2018-03-02 04:21:00+00:00  2018-03-02    4.350000  04:21:00   \n",
       " 1   2018-03-02 04:41:00+00:00  2018-03-02    4.683333  04:41:00   \n",
       " 2   2018-03-02 04:45:00+00:00  2018-03-02    4.750000  04:45:00   \n",
       " 3   2018-03-02 17:57:00+00:00  2018-03-02   17.950000  17:57:00   \n",
       " 4   2018-03-02 18:37:00+00:00  2018-03-02   18.616667  18:37:00   \n",
       " ..                        ...         ...         ...       ...   \n",
       " 553 2020-03-26 05:45:00+00:00  2020-03-26    5.750000  05:45:00   \n",
       " 554 2020-03-26 08:30:00+00:00  2020-03-26    8.500000  08:30:00   \n",
       " 555 2020-03-27 00:15:00+00:00  2020-03-26   24.250000  00:15:00   \n",
       " 556 2020-03-27 02:00:00+00:00  2020-03-26   26.000000  02:00:00   \n",
       " 557 2020-03-27 01:00:00+00:00  2020-03-26   25.000000  01:00:00   \n",
       " \n",
       "      week_from_start  year  \n",
       " 0                 13  2018  \n",
       " 1                 13  2018  \n",
       " 2                 13  2018  \n",
       " 3                 13  2018  \n",
       " 4                 13  2018  \n",
       " ..               ...   ...  \n",
       " 553               14  2020  \n",
       " 554               14  2020  \n",
       " 555               14  2020  \n",
       " 556               14  2020  \n",
       " 557               14  2020  \n",
       " \n",
       " [558 rows x 13 columns]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_baseline_and_intervention_usable_data('data/output/public.pickle',export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adherent(s):\n",
    "    \"\"\"\n",
    "    return True if the there are more than 2 loggings in one day w/ more than 4hrs apart from each other.\n",
    "    \"\"\"\n",
    "    if len(s.values) >= 2 and (max(s.values) - min(s.values)) >= 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_active_user(in_path, n, export = False, user_day_counts_path = 'data/output/public_top_users_day_counts.csv'):\n",
    "    \"\"\"\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param n: number of users with most number of logging days\\n\n",
    "    @param user_data_counts_path: output path for top_users_day_counts in csv format\\n\n",
    "    @param export : whether to save the processed dataframe locally\\n\n",
    "    @return: top_users_day_counts dataframe\\n\n",
    "    \n",
    "    This function returns a dataframe in csv format that finds top n users with the most number of days that they logged. \n",
    "    \"\"\"\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # filter the dataframe so it only contains food and beverage food type, then get the top n users who input the\n",
    "    # most loggings in descending order\n",
    "    top_users = public_all.query('food_type in [\"f\", \"b\"]')[['ID', 'unique_code']]\\\n",
    "    .groupby('unique_code').agg('count').sort_values(by = 'ID', ascending = False).index[:n]\n",
    "    top_users = set(list(top_users))\n",
    "    public_top_users = public_all.query('food_type in [\"f\", \"b\"]')\\\n",
    "    .loc[public_all.unique_code.apply(lambda x: x in top_users)].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    adherent_dict = dict(public_top_users.groupby(['unique_code', 'date'])['local_time'].agg(adherent))\n",
    "    public_top_users['adherence'] = public_top_users\\\n",
    "    .apply(lambda x: adherent_dict[(x.unique_code, x.date)], axis = 1)\n",
    "    \n",
    "    public_top_users_day_counts = pd.DataFrame(public_top_users.query('adherence == True')\\\n",
    "                            [['date', 'unique_code']].groupby('unique_code')['date'].nunique())\\\n",
    "                            .sort_values(by = 'date', ascending = False).rename(columns = {'date': 'day_count'})\n",
    "    \n",
    "    if export == True:\n",
    "        public_top_users_day_counts.to_csv(user_day_counts_path)\n",
    "        print('user day counts data is saved at {}'.format(user_day_counts_path))\n",
    "    \n",
    "    return public_top_users_day_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alqt45631586569</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt8668165687</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt21525720972</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt78896444285</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alqt14018795225</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 day_count\n",
       "unique_code               \n",
       "alqt45631586569        145\n",
       "alqt8668165687         135\n",
       "alqt21525720972         89\n",
       "alqt78896444285         81\n",
       "alqt14018795225         60"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_active_user('data/output/public.pickle',5,'data/public_top_users_day_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def convert_loggings(in_path, export = False, parsed_food_path='data/output/public_all_parsed.csv', ascending = False):\n",
    "    \"\"\"\n",
    "    @param in_path : input path, file in pickle format\\n\n",
    "    @param parsed_food_path : output path for cleaned food loggings in csv format \\n\n",
    "    @param ascending: True to sort the item from lowest to highest frequency and vice versa\\n\n",
    "    @param export: whether to save the converted dataframe locally\\n\n",
    "    @return: frequency count of food loggings in the descending order by default\\n\n",
    "    \n",
    "    This function takes in a pickle file, convert all the loggings into individual items of food and count the frequency of all food loggings. \n",
    "    \"\"\"\n",
    "    \n",
    "    # load data\n",
    "    public_all_pickle_file = open(in_path, 'rb')\n",
    "    public_all = pickle.load(public_all_pickle_file)\n",
    "    \n",
    "    # initialize food parser instance\n",
    "    fp = FoodParser()\n",
    "    fp.initialization()\n",
    "    \n",
    "    # parse food\n",
    "    parsed = [fp.parse_food(i, return_sentence_tag = True) for i in public_all.desc_text.values]\n",
    "    \n",
    "    print('All food is parsed!')\n",
    "    \n",
    "    public_all_parsed = pd.DataFrame({\n",
    "    'ID': public_all.ID,\n",
    "    'food_type': public_all.food_type,\n",
    "    'desc_text': public_all.desc_text,\n",
    "    'cleaned': parsed\n",
    "    })\n",
    "    \n",
    "    public_all_parsed['cleaned'] = public_all_parsed['cleaned'].apply(lambda x: x[0])\n",
    "    \n",
    "    if export == True:\n",
    "        public_all_parsed.to_csv(parsed_food_path)\n",
    "        print('cleaned food loggings is saved at {}'.format(parsed_food_path))\n",
    "    \n",
    "    # count food\n",
    "    all_count_dict = defaultdict(lambda : 0) \n",
    "    for food in public_all_parsed.cleaned.values:\n",
    "        if len(food) > 0:\n",
    "            for f in food:\n",
    "                all_count_dict[f] += 1\n",
    "                \n",
    "    print('All food is counted!')\n",
    "    \n",
    "    all_count_dict = dict(all_count_dict)\n",
    "    public_freq_count = pd.DataFrame({\n",
    "        'food': list(all_count_dict.keys()),\n",
    "        'count': [all_count_dict[k] for k in list(all_count_dict.keys())]\n",
    "    }).sort_values(by = 'count', ascending = ascending).reset_index(drop = True)\n",
    "    \n",
    "    return public_freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All food is parsed!\n",
      "cleaned food loggings is saved at data/public_all_parsed.csv\n",
      "All food is counted!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>water</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glass</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coffee</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pint water</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>large</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>tomato soup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>florentine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>french fry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>cheeseburger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>ramen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>856 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             food  count\n",
       "0           water   1804\n",
       "1           glass    666\n",
       "2          coffee    273\n",
       "3      pint water    248\n",
       "4           large    228\n",
       "..            ...    ...\n",
       "851   tomato soup      1\n",
       "852    florentine      1\n",
       "853    french fry      1\n",
       "854  cheeseburger      1\n",
       "855         ramen      1\n",
       "\n",
       "[856 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_loggings('data/public.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
